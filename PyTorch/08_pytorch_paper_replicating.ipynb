{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaeebf60-1b69-499f-af84-01c1a3d86115",
   "metadata": {},
   "source": [
    "### Vision Transformer\n",
    "Vision Transformer（ViT）是一种基于注意力机制的图像分类模型，它是由Alexey Dosovitskiy等人在论文\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"中提出的。ViT的核心思想是将图像分割成固定数量的图块，然后将这些图块转换为序列，再通过Transformer架构进行处理。\n",
    "\n",
    "以下是Vision Transformer的主要构架要点：\n",
    "\n",
    "1. **图像分块（Image Patching）**：\n",
    "   - 将输入图像分成若干个固定大小的非重叠图块。每个图块通常是一个正方形区域，可以通过将图像划分为网格来实现。这些图块被视为模型的输入。\n",
    "\n",
    "2. **嵌入层（Embedding Layer）**：\n",
    "   - 将每个图块转换为一个向量，这个向量被视为图块的表示。通常，一个简单的线性变换（包括一个可学习的权重矩阵）被用来实现这个转换。这一步的目的是将图像中的空间信息转化为模型可以理解的向量表示。\n",
    "\n",
    "3. **位置嵌入（Positional Embedding）**：\n",
    "   - 为了引入图块的位置信息，ViT引入了位置嵌入，将每个位置信息嵌入到对应的图块表示中。这是因为Transformer模型本身并不具备处理序列中元素的位置信息的能力。\n",
    "\n",
    "4. **Transformer Encoder**：\n",
    "   - 将嵌入的图块序列输入到Transformer编码器中。Transformer编码器由多个相同的层组成，每个层包含自注意力机制（Self-Attention）和前馈神经网络（Feedforward Neural Network）。这些层允许模型捕捉输入序列中的长程依赖关系。\n",
    "\n",
    "5. **全局平均池化（Global Average Pooling）**：\n",
    "   - 将Transformer编码器的输出序列的每个位置上的向量进行平均，得到一个全局的图像表示。这个全局表示被送入一个用于图像分类的线性层。\n",
    "\n",
    "整体上，Vision Transformer的关键思想是利用Transformer的强大的序列建模能力来处理图像信息。通过将图像转换为序列，并使用自注意力机制来捕获图块之间的关系，ViT在图像分类任务上取得了令人印象深刻的性能。\n",
    "\n",
    "（正如卷积神经网络将卷积作为前置的层，这次将注意力机制作为神经网络的前置layer）\n",
    "\n",
    "https://arxiv.org/abs/2010.11929\n",
    "\n",
    "最原始的研究论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ff32d-73d0-4b71-b7fd-2b5dbc9d68b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d5bd8-45f0-424e-b5c5-03640c88e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
    "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd45cae-fd34-424e-98bc-ae37c0a56174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4922b1-f88e-40b6-aa97-7d4a12343c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a44ea-72ec-49d1-83c1-7c14173d8f4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8897912-156b-4edd-9a99-62d65500f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pizza, steak, sushi images from GitHub\n",
    "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination=\"pizza_steak_sushi\")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da1be2-bbc5-4932-83e7-1fff895b9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory paths to train and test images\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825d7aa-9aa1-4360-baa4-c44097378cf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Create datasets and dataloaders （check batch size and image size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915801a-dff0-4c6e-bdb5-7d69935ff26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image size (from Table 3 in the ViT paper)\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "print(f\"Manually created transforms: {manual_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bed56-a543-41c2-9448-e046abff82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原本的batch size是4096我们的32的128倍，但是我们还是用32吧。穷。\n",
    "# Set the batch size\n",
    "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, # use manually created transforms\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f71f48-2f8c-4d03-a831-256ebada18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Get a single image from the batch\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# View the batch shapes\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681ca08-74a6-4f68-8fde-745cb5c27c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image with matplotlib\n",
    "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b319347-18d7-423e-a559-aaf9d0ef4123",
   "metadata": {},
   "source": [
    "### 3. Replication the paper on a overview\n",
    "\n",
    "讨论模型结构：block组成模型，layer组成block，在layer中对于输入的tensor，作用很多function在上面，比如forward函数：（很像拼乐高）\n",
    "\n",
    "1. Layer - takes an input, performs a function on it, returns an output.\n",
    "2. Block - a collection of layers, takes an input, performs a series of functions on it, returns an output.\n",
    "3. Architecture (or model) - a collection of blocks, takes an input, performs a series of functions on it, returns an output.\n",
    "\n",
    "框架自我概述：\n",
    "\n",
    "Embedded Patches 对图像进行初步处理: 图像扁平化处理为向量（Patch Embedding）和位置编码（Position Embedding）的组合。\n",
    "\n",
    "x_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\n",
    "\n",
    "Transformer Encoder: LNorm -> MHA(多头注意力机制) -> LNorm -> MLP(多层感知机)\n",
    "\n",
    "MHA block: 多头注意力机制是Transformer模型中的一个关键组件，用于处理序列数据。在多头注意力机制中，输入序列的不同部分可以同时受到注意力的关注。每个头都学习不同的注意力模式，从而使模型能够捕捉序列中的不同信息。**每个头的输出通过残差连接与原始输入相加，形成最终的输出。**\n",
    "\n",
    "x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n",
    "Notice the skip/residual connection on the end (adding the input of the layers to the output of the layers).\n",
    "\n",
    "MLP block: 一般来说多层感知机的结构式线性层-非线性层-线性层-非线性层这样。\n",
    "\n",
    "这里是：**layer norm -> linear layer -> non-linear layer -> dropout -> linear layer -> dropout**\n",
    "\n",
    "在Transformer模型中，多层感知机（MLP）主要作用于两个地方：每个注意力头的输出和每个位置的输出：\n",
    "\n",
    "1. **每个注意力头的输出：** 在多头自注意力机制中，每个注意力头产生的输出被串联（concatenate）在一起，然后经过一个线性变换和激活函数，最终得到每个位置的综合表示。在这个串联后的向量上应用多层感知机有助于对不同注意力头学到的信息进行更复杂的组合和转换。这个多层感知机的作用是引入非线性变换，使得模型能够更好地学习复杂的关系和特征。\n",
    "\n",
    "2. **每个位置的输出：** 在Transformer的每个编码器层中，对每个位置的表示（即每个位置的注意力头输出的串联）进行多层感知机的处理。这个多层感知机通常由两个线性变换和激活函数组成。这种处理有助于捕捉位置特定的信息和关系，使得模型能够更好地理解输入序列的结构。\n",
    "\n",
    "综合起来，多层感知机在Transformer中的作用是引入非线性变换，增强模型对输入数据的表达能力。这有助于模型更好地学习序列中的复杂关系，提高其表示能力，从而使得Transformer在处理自然语言处理等序列任务时能够取得更好的性能。在实践中，多层感知机通常包括一个线性变换、一个激活函数（比如ReLU），再接一个线性变换，这样的结构使得模型能够学到更复杂的映射。\n",
    "\n",
    "x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n",
    "Notice the skip/redidual connection on the end (adding the input of the layers to the output of the layers).\n",
    "\n",
    "LNorm block: 正规化处理，有助于减少训练时间，和提高模型泛化能力。（因为可以减少过拟合风险）\n",
    "\n",
    "0 index of x_output_MLP_block: 线性变换\n",
    "y = Linear_layer(LN_layer(x_output_MLP_block[0]))\n",
    "\n",
    "---\n",
    "\n",
    "Patch embedding（补丁嵌入）通常是指在计算机视觉领域中，将图像分成小块（通常是正方形的小区域），然后对每个小块进行嵌入（embedding）操作，将其映射到一个低维向量空间中。在传统的CNNs中，输入图像经过一系列卷积和池化操作，逐渐减小空间维度，最终得到一个全局的表示。而在使用补丁嵌入的情况下，图像被分割成小块，每个小块经过嵌入操作后得到一个向量表示，这些向量被组合成整个图像的表示。这种方法的优势在于能够更好地捕捉局部特征，因为每个小块都有自己的嵌入表示。这在处理大型图像时可以更高效地使用计算资源，同时也有助于模型更好地理解图像中的局部结构。补丁嵌入常常与自注意力机制（self-attention）结合使用，这种结合通常出现在一些图像处理的深度学习模型。在这些模型中，补丁嵌入被引入以处理变尺寸的输入图像，使得模型更加灵活。\n",
    "\n",
    "Position embedding（位置嵌入）是一种在深度学习中处理序列数据的技术。它主要用于为模型提供关于输入序列中每个元素在序列中的位置信息。在自然语言处理（NLP）中，位置嵌入通常用于处理文本序列，如句子或段落。在深度学习模型中，特别是那些使用自注意力机制（self-attention）的模型，如Transformer，序列中的元素通常被同时处理，而不考虑它们在序列中的位置。为了使模型能够区分不同位置的元素，引入了位置嵌入。位置嵌入的一种常见方式是通过在输入嵌入向量中添加一个表示位置的向量。这个向量包含了关于元素在序列中位置的信息。在Transformer模型中，通常使用正弦和余弦函数的组合来生成位置嵌入。这样的组合能够捕捉到相对位置之间的关系。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b935f0-065e-4432-a468-d088db4859d2",
   "metadata": {},
   "source": [
    "### It's all about the embedding. \n",
    "将高维数据映射到低维数据的表示形式，是最重要的一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c517a2-c82b-4b2f-9c54-1b1ac80c68fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Split data into patches and creating the class, position and patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ac567a-2eb1-4fb2-9982-e2c0f469e0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196.\n"
     ]
    }
   ],
   "source": [
    "# calculating patch embedding input and output shapes by hand\n",
    "# create example values\n",
    "height = 224\n",
    "width = 224\n",
    "color_channels = 3\n",
    "patch_size = 16\n",
    "\n",
    "# calculate N (number of patches)\n",
    "number_of_patches = int((height * width) / patch_size ** 2)\n",
    "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e76e44-a8fa-4aa7-b433-ec387e6d1fdb",
   "metadata": {},
   "source": [
    "- input shape is image with 2D size: H * W * C\n",
    "- output shape is sequence of flattened 2D patches with size: N * (P ** 2 * C)\n",
    "\n",
    "是一种高纬数据降维的过程，三维降为二维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d15449e-f15d-4f29-84e0-9afacdb1b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (single 2D image): (224, 224, 3)\n",
      "Output shape (single 2D image flattened into patches): (196, 768)\n"
     ]
    }
   ],
   "source": [
    "# input shape\n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "\n",
    "# output shape\n",
    "embedding_layer_output_shape = (number_of_patches, patch_size ** 2 * color_channels)\n",
    "\n",
    "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
    "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29581f54-44e8-4460-91d6-aadc9902121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150528, 50176)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224 * 224 * 3, 224 * 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa27fd3-7b1c-4e81-850b-2f526484a97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150528"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "196 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912feeff-1958-47e1-a89c-850d905bdf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view single image\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab6f82-d024-494e-a4ac-7a6a61fe0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change image shape to be compatible with matplotlib (color_channels, height, width)\n",
    "image_permuted = image.permute(1, 2, 0)\n",
    "\n",
    "# index to plot the top row of patched pixels\n",
    "patch_size = 16\n",
    "plt.figure(figsize=(patch_size, patch_size))\n",
    "plt.imshow(image_permuted[:patch_size, :, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2c690-31ae-46cc-a4ef-8b9f66d8d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn image into patches (only on patch_size : 224 x 16)\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size/patch_size\n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# create a series of subplots\n",
    "fig, axs = plt.subplots(\n",
    "    nrows = 1,\n",
    "    ncols = img_size // patch_size,\n",
    "    figsize = (num_patches, num_patches),\n",
    "    shares = True,\n",
    "    sharey = True\n",
    ")\n",
    "\n",
    "# iterate through number of patches in the top row\n",
    "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
    "    axs[i].imshow(image_permuted[:patch_size, patch:patch + patch_size, :]);\n",
    "    axs[i].set_xlabel(i + 1)\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3767e7-e425-4382-aa64-4d355a467660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn image into patches (full image 224 x 224)\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size / patch_size\n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotoal patches: {num_patches * num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# create a series of subplots\n",
    "fig, axs = plt.subplots(\n",
    "    nrows = img_size // patch_size,\n",
    "    ncols = img_size // patch_size,\n",
    "    figsize = (num_patches, num_patches),\n",
    "    shares = True,\n",
    "    sharey = True\n",
    ")\n",
    "\n",
    "# Loop through height and width of image\n",
    "for i, patch_height in enumerate(range(0, img_size, patch_size)):\n",
    "    for i, patch_width in enumerate(range(0, img_size, patch_size)):\n",
    "    # plot the permuted image patch\n",
    "        axs[i, j].imshow(image_permuted[patch_height:patch_height + patch_size,\n",
    "                                        patch_width:patch_width + patch_size, :]);\n",
    "        axs[i, j].set_ylabel(i + 1,\n",
    "                             rotation = \"horizontal\",\n",
    "                             horizontalalignment = \"right\",\n",
    "                             verticalalignment = \"center\")\n",
    "        axs[i, j].set_xlabel(j + 1)\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "        axs[i, j].label_outer()\n",
    "\n",
    "# set a super title\n",
    "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a38c1a-7c05-4b43-9edd-2a50eb557761",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create image patches with torch.nn.Conv2d()\n",
    "turn these patches into an embedding with torch.nn.Conv2d() with params kernel_size and stride tobe patch_size.（也就是说上面的被切割的图形小块，可以用2d卷积的方式得到，只需要设置核的边长和步长都为patch_size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc358b9a-b778-4d8e-b88c-8b1a2222aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Set the patch size\n",
    "patch_size=16\n",
    "\n",
    "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
    "conv2d = nn.Conv2d(in_channels=3, # number of color channels\n",
    "                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n",
    "                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n",
    "                   stride=patch_size,\n",
    "                   padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2d8bc-021a-4d95-95a9-1002b6f98af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the image through the convolutional layer\n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n",
    "print(image_out_of_conv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460d2be-b946-4923-b1c8-2f3921b14c61",
   "metadata": {},
   "source": [
    "its output shape can be read as:\n",
    "\n",
    "torch.Size([1, 768, 14, 14]) -> [batch_size, embedding_dim, feature_map_height, feature_map_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3012e5-3523-4dce-aad3-e876ceba32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random 5 convolutional feature maps\n",
    "import random\n",
    "random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\n",
    "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
    "\n",
    "# Create plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n",
    "\n",
    "# Plot random image feature maps\n",
    "for i, idx in enumerate(random_indexes):\n",
    "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
    "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
    "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8be56-e5ca-47fb-a7a0-6ae4dc9561e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single feature map in tensor form\n",
    "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
    "single_feature_map, single_feature_map.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456df464-755a-4460-afae-81b95e90bd57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Flattening the patch embedding with torch.nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242935f-a1ea-4e50-8af4-a6d953a20ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current tensor shape\n",
    "print(f\"Current tensor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce140d2-ba26-4589-beae-e449ec48fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flatten layer\n",
    "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
    "                     end_dim=3) # flatten feature_map_width (dimension 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26457f-21e6-439f-99b9-1cf879faaca2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Put patch parts all togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0780c4-d7ae-4bc5-908e-28ba828cb5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. View single image\n",
    "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "# 2. Turn image into feature maps\n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n",
    "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
    "\n",
    "# 3. Flatten the feature maps\n",
    "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
    "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8dcea-052b-417e-b0c9-e6f69cf271d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get flattened image patch embeddings in right shape\n",
    "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
    "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641b343-f12c-4885-8efe-36c858ca8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single flattened feature map\n",
    "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
    "\n",
    "# Plot the flattened feature map visually\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
    "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397b5c7-524a-498e-9975-37fbc3c7ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the flattened feature map as a tensor\n",
    "single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d11b8-5bd3-4a2b-b66c-f7f3dbc3f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class which subclasses nn.Module\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of color channels for the input image. (Defaults to 3)\n",
    "        patch_size (int): Size of patches to convert input image into.(Defaults to 16)\n",
    "        embedding_dim (int): Size of embedding to turn image into. (Defaults to 768)\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with appropriate variable\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int=3,\n",
    "        patch_size: int=16,\n",
    "        embedding_dim: int=768\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create a layer to turn an image into patches\n",
    "        self.patcher = nn.Conv2d(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = embedding_dim,\n",
    "            kernel_size = patch_size,\n",
    "            stride = patch_size,\n",
    "            padding = 0\n",
    "        )\n",
    "\n",
    "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
    "        self.flatten = nn.Flatten(\n",
    "            start_dim = 2, \n",
    "            end_dim = 3\n",
    "        )\n",
    "\n",
    "    # 5. Define the forward method\n",
    "    def forward(self, x):\n",
    "        # Create assertion to check that inputs are the correct shape\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch_size: {patch_size}\"\n",
    "\n",
    "        # Perform the forward pass\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched)\n",
    "        # 6. Make sure the output shape has the right order\n",
    "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2*C, N] -> [batch_size, N, P^2*C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906108ad-0b37-4c32-872e-7a2ca418c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "# Create an instance of patch embedding layer\n",
    "patchify = PatchEmbedding(\n",
    "    in_channels = 3, \n",
    "    patch_size = 16,\n",
    "    embedding_dim = 768\n",
    ")\n",
    "\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add a extra batch dimension on the 0th index, otherwise will error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b2a1e9-8cb3-4af7-a874-46ae85d95f7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Class token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff36467-104c-4b27-98ca-6b1dd16d0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the patch embedding and patch embedding shape\n",
    "print(patch_embedded_image)\n",
    "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589863a3-3534-49d6-bd79-d021c80001f2",
   "metadata": {},
   "source": [
    "patch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "\n",
    "这里使用torch.ones()填充初始的class，理想的是设置随机的class使用torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aec6aaf-d300-45ec-a880-e95daf1fee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量，并设置 requires_grad=True\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# 进行一些操作\n",
    "y = x ** 2\n",
    "z = 2 * y + 3\n",
    "\n",
    "# 对 z 进行求导\n",
    "z.backward()\n",
    "\n",
    "# 打印梯度\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0414100-015f-4bee-a9a5-509546dd9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the batch size and embedding dimension\n",
    "batch_size = patch_embedded_image.shape[0]\n",
    "embedding_dimension = patch_embedded_image.shape[-1]\n",
    "\n",
    "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
    "                           requires_grad=True) # make sure the embedding is learnable 意味着可以进行反向传播更新梯度\n",
    "\n",
    "# Show the first 10 examples of the class_token\n",
    "print(class_token[:, :, :10])\n",
    "\n",
    "# Print the class_token shape\n",
    "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0718595-9ead-4177-b709-559703a0e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the class token embedding to the front of the patch embedding\n",
    "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n",
    "                                                      dim=1) # concat on first dimension\n",
    "\n",
    "# Print the sequence of patch embeddings with the prepended class token embedding\n",
    "print(patch_embedded_image_with_class_embedding)\n",
    "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7074ada-ef8a-487a-82c0-a9aa5c09d418",
   "metadata": {},
   "source": [
    "关注到number_of_patches的数量增加了一个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ccdc6-f44a-49b8-9745-9ec8bcfa6980",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Position Embedding\n",
    "很重要，因为他显示了序列中各个patch的前后关系，不然他们将失去关系性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c2fcec-346a-44c6-94fa-f521f38f148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the sequence of patch embeddings with the prepended class embedding\n",
    "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403f6a9-e942-4295-876a-d3f03a391159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate N (number of patches)\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
    "\n",
    "# Create the learnable 1D position embedding\n",
    "position_embedding = nn.Parameter(torch.ones(1,\n",
    "                                             number_of_patches+1,\n",
    "                                             embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n",
    "print(position_embedding[:, :10, :10])\n",
    "print(f\"Position embeddding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b0a1e-c9df-4104-bb0b-e632868385ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the position embedding to the patch and class token embedding\n",
    "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
    "print(patch_and_position_embedding)\n",
    "print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470745c6-d100-4647-beb3-3ccd4b781a6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Put them all together\n",
    "步骤分解；：\n",
    "1. 一开始图像上是`[3, 224, 224]`形状。\n",
    "2. 加上一个batch大小变成（**unsqueeze**）`[1, 3, 224, 224]`形状。(加一个维度是为了后面放进torch的layer的时候形状适配)\n",
    "3. 通过一层**Conv2D**的特征提取后变成`[1, 768, 14, 14]`（batch_size, embedding_dim, feature_map_height, feature_map_width）形状。（将输出channels设置为768了，然后核大小和步长是patch的大小224/16=14）\n",
    "4. 通过一层**Flatten**的扁平化，将2，3维度的向量进行处理，得到`[1, 768, 196]`形状。（batch_size, embedding_dim, num_patches）\n",
    "5. 经过**permute**维度变化交换2，3维度位置变为`[1, 196, 768]`形状。\n",
    "6. 创建一个`[1, 1, 768]`形状的ClassToken（使用**nn.Parameter**(ones，或者randn, required_grad=True)，随机化最好）\n",
    "7. 使用**torch.cat**将ClassToken和patch_embedded_image在维度1上进行组合得到`[1, 197, 768]`形状。\n",
    "8. 创建一个`[1, 197, 768]`形状的PositionEmbedding（使用**nn.Parameter**(ones，或者randn, required_grad=True)，随机化最好）\n",
    "9. 将PositionEmbedding加在上上一步的结果中，得到同样形状`[1, 197, 768]`的最终结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6751d-d990-4110-9270-7a4ee9b0e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "# 1. Set patch size\n",
    "patch_size = 16\n",
    "\n",
    "# 2. Print shape of original image tensor and get the image dimensions\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# 3. Get image tensor and add batch dimension\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "# 4. Create patch embedding layer\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=768)\n",
    "\n",
    "# 5. Pass image through patch embedding layer\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "# 6. Create class token embedding\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True) # make sure it's learnable\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. Prepend class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "# 8. Create position embedding\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "# 9. Add position embedding to patch embedding with class token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77060783-dd4a-44ac-a763-53b4a9135357",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ae8a5-2987-4836-81e5-df461c8fea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"Create a multi-head self-attention block.\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim:int = 768,\n",
    "        num_heads:int = 12,\n",
    "        attn_dropout:float = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "\n",
    "        # 4. Create the MHA layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim = embedding_dim,\n",
    "            num_heads = num_heads,\n",
    "            dropout = attn_dropout,\n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "    # 5. Create a forward() method to pass the data through the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(\n",
    "            query = x,\n",
    "            key = x,\n",
    "            value = x, \n",
    "            need_weights = False\n",
    "        )\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b925ae-17a2-4a3f-9b03-c6c7db879d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MHABlock\n",
    "multihead_self_attention_block = MultiheadSelfAttentionBlock(\n",
    "    embedding_dim = 768,\n",
    "    num_heads = 12\n",
    ")\n",
    "\n",
    "# Pass patch and position image embedding through MHABlock\n",
    "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
    "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
    "print(f\"Output shape MSA block: {patched_image_through_mha_block.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ed87e-e319-41ec-9dbc-e90703bd6d53",
   "metadata": {},
   "source": [
    "### Multilayer Percaptron(MLP)\n",
    "layer norm -> linear layer -> non-linear layer -> dropout -> linear layer -> dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78f3ce-3dd0-42b1-a0ca-c33ef1f8bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MLPBlock(nn.Module):\n",
    "    \"Create a layer normalized multilayer perceptron block.\"\n",
    "    # 2. Initialize the class with hyperparameters\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim:int = 768,\n",
    "        mlp_size:int = 3072, # MLP size from Table 1 for ViT-Base\n",
    "        dropout:float = 0.1  # Dropout from Table 3 for ViT-Base\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "\n",
    "        # 4. Create the Multilayer perceptron layers\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features = embedding_dim,\n",
    "                      out_features = mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p = dropout),\n",
    "            nn.Linear(in_features = mlp_size,\n",
    "                      out_features = embedding_dim),\n",
    "            nn.Dropout(p = dropout)\n",
    "        )\n",
    "\n",
    "    # 5. Create a forward() method to pass the data through the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5e411-5ebb-4ec6-8600-b4084e90fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of MLPBlock\n",
    "mlp_block = MLPBlock(\n",
    "    embedding_dim = 768,\n",
    "    mlp_size = 3072,\n",
    "    dropout = 0.1\n",
    ")\n",
    "\n",
    "# Pass output of MHABlock through MLPBlock\n",
    "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
    "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
    "print(f\"Output shape of MLP block: {patched_image_through_mlp_block.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14428b55-ac7e-4081-8a2a-f318f43ef7e4",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "Add residual connection: x_input -> MSA_block -> MSA_block_output + **x_input** -> MLP_block -> MLP_block_output + MSA_block_output + **x_input** -> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e22104-b140-415f-8432-45a41bca215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"Create a Transformer Encoder block.\"\n",
    "    # 2. Initialize the class with hyperarameters from Table 1 and 3\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim:int = 768,\n",
    "        num_heads:int = 12,\n",
    "        mlp_size:int = 3072,\n",
    "        mlp_dropout:float = 0.1,\n",
    "        attn_dropout:float = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(\n",
    "            embedding_dim = embedding_dim,\n",
    "            num_heads = num_heads,\n",
    "            attn_dropout = attn_dropout\n",
    "        )\n",
    "\n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block = MLPBlock(\n",
    "            embedding_dim = embedding_dim,\n",
    "            mlp_size = mlp_size,\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "    # 5. Create a forward() method\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        x = self.msa_block(x) + x\n",
    "\n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f896c-b25b-4cd6-ae74-ea5e3250dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of TransformerEncoderBlock\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "# summary(model=transformer_encoder_block,\n",
    "#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1b4f5-228e-410b-a155-d2902ec153b8",
   "metadata": {},
   "source": [
    "Pytorch Transformer layers from nn.torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63a47a-90a4-45af-91cb-52c3affe8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same as above with torch.nn.TransformerEncoderLayer()\n",
    "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model = 768,\n",
    "    nhead = 12,\n",
    "    dim_feedforward = 3072,\n",
    "    dropout = 0.1,\n",
    "    activation = \"gelu\",\n",
    "    batch_first = True,\n",
    "    norm_first = True\n",
    ")\n",
    "\n",
    "torch_transformer_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ca0c6-94e6-4c36-9af5-922bb86de620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n",
    "# summary(model=torch_transformer_encoder_layer,\n",
    "#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f930bc6-ea7a-41c6-a359-7eab663d4ab9",
   "metadata": {},
   "source": [
    "由于 ViT 架构使用多个 Transformer 层，每个层堆叠在整个架构的顶部（表 1 显示了 ViT-Base 的情况下有 12 层），因此您可以使用以下命令来执行此操作torch.nn.TransformerEncoder(encoder_layer, num_layers)：\n",
    "\n",
    "- encoder_layer- 使用创建的目标 Transformer Encoder 层torch.nn.TransformerEncoderLayer()。\n",
    "- num_layers- 堆叠在一起的 Transformer Encoder 层的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8c4f4-bd6e-4a2e-90c7-934d28fb0808",
   "metadata": {},
   "source": [
    "### Create ViT (put all together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ff0a6-0982-4a15-a4f1-6d2c155a19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "\n",
    "        # 3. Make the image size is divisble by the patch size\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "\n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "\n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "\n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "\n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 12. Get batch size\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # 16. Add position embedding to patch embedding (equation 1)\n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24cb66-7f9a-4f71-95c8-2b4dcd960f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating the class embedding and expanding over a batch dimension\n",
    "batch_size = 32\n",
    "class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\n",
    "class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n",
    "\n",
    "# Print out the change in shapes\n",
    "print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\n",
    "print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbdc21c-4763-408f-b7d8-5fe0c9015b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "# Create a random tensor with same shape as a single image\n",
    "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
    "\n",
    "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
    "vit = ViT(num_classes=len(class_names))\n",
    "\n",
    "# Pass the random image tensor to our ViT instance\n",
    "vit(random_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffca49-29dd-430d-a86e-b682b0a16ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
    "# summary(model=vit,\n",
    "#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ed517-1e26-44b4-b440-f23dae9217eb",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395a1c1-f16b-4d3f-a978-9b1984aaa292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n",
    "                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n",
    "                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
    "\n",
    "# Setup the loss function for multi-class classification\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the seeds\n",
    "set_seeds()\n",
    "\n",
    "# Train the model and save the training results to a dictionary\n",
    "results = engine.train(model=vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=10,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f45b2-1683-4b09-a0c9-9d96518e921c",
   "metadata": {},
   "source": [
    "**结果不尽人意的原因**：\n",
    "\n",
    "原本的模型是在一个很大的数据集上进行，大的batch，以及很长的训练时间。正因为如此，原本的模型用到了很多过拟合的技术防止其过拟合。毕竟有这么多的数据和参数。\n",
    "\n",
    "所以我们的小批量数据并不能有很好的结果。\n",
    "\n",
    "这也说明了，现在的科学实验能有很好的结果，很多时候也得益于大数据和高性能计算机。**算力，数据，算法**，缺一不可。随着时代的发展，模型也越变越大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c451d9-30f6-4be1-b651-ae9cf3b9d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "# Plot our ViT model's loss curves\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8f28e-fc5b-4376-ac7c-ee7cdf39abf5",
   "metadata": {},
   "source": [
    "### Using a pretrained ViT from torchvision.models on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7581d7e-8529-4ac7-9ddd-78fa3f5f6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following requires torch v0.12+ and torchvision v0.13+\n",
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4586d-a270-4454-9a04-ee72f5bbcc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe950a06-b0eb-4ff0-bb37-d07540818523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get pretrained weights for ViT-Base\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision >= 0.13, \"DEFAULT\" means best available\n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n",
    "set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "# pretrained_vit # uncomment for model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08942ca8-3039-4429-822d-c95cf674cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print a summary using torchinfo (uncomment for actual output)\n",
    "# summary(model=pretrained_vit,\n",
    "#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa6a9f-3be1-493c-a568-62d9605845f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import download_data\n",
    "\n",
    "# Download pizza, steak, sushi images from GitHub\n",
    "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination=\"pizza_steak_sushi\")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84921b32-c745-47a9-8c0b-cf05aebc1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train and test directory paths\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea4811-321a-4e91-92d1-0de122a6fc96",
   "metadata": {},
   "source": [
    "ensure your own custom data is transformed/formatted in the same way the data the original model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35769816-efb3-4533-b4a6-d4eb0cc445cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698bc2f-6c7b-4ca3-8b5b-431b8bda109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                     test_dir=test_dir,\n",
    "                                                                                                     transform=pretrained_vit_transforms,\n",
    "                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9353bdc-7e35-4e87-ad34-81659435eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the classifier head of the pretrained ViT feature extractor model\n",
    "set_seeds()\n",
    "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
    "                                      train_dataloader=train_dataloader_pretrained,\n",
    "                                      test_dataloader=test_dataloader_pretrained,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=10,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2d99e-078b-4de4-a8b3-0f81d67add2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(pretrained_vit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e246be-d9bf-4a41-bc7b-a84919fd61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "from going_modular import utils\n",
    "\n",
    "utils.save_model(model=pretrained_vit,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951159b-009a-402d-ab21-37e371249823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
    "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241c10b-fb10-48a5-a0ff-7efd55624c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on custom data\n",
    "import requests\n",
    "\n",
    "# Import function to make predictions on images and plot them\n",
    "from helper_functions import pred_and_plot_image\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = image_path / \"04-pizza-dad.jpeg\"\n",
    "\n",
    "# Download the image if it doesn't already exist\n",
    "if not custom_image_path.is_file():\n",
    "    with open(custom_image_path, \"wb\") as f:\n",
    "        # When downloading from GitHub, need to use the \"raw\" file link\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "        print(f\"Downloading {custom_image_path}...\")\n",
    "        f.write(request.content)\n",
    "else:\n",
    "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
    "\n",
    "# Predict on custom image\n",
    "pred_and_plot_image(model=pretrained_vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
