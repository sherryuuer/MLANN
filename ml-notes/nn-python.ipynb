{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbee0e4b-7581-4343-b9ec-a4f77396e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.62461377  1.2144206   0.2659123  -0.13177798]\n",
      " [-0.9952449  -0.61325293  0.0383907  -0.3697716 ]]\n",
      "[-0.95479794  0.7789426  -0.58961902 -0.90648772]\n",
      "[[-2.16198273 -2.59835266 -0.35925473 -0.24790277]\n",
      " [-1.0472482  -1.54779084 -0.27511269  0.00674971]\n",
      " [-0.5320618  -0.05160915  0.1170959  -0.31884204]\n",
      " [-1.75791633 -1.50856527 -0.08089703 -0.46656318]\n",
      " [-2.91805913 -2.33877767 -0.07647233 -0.8470061 ]\n",
      " [-0.91303675 -0.95934712 -0.10348284 -0.16521008]\n",
      " [ 2.11740588  1.69120199  0.05343959  0.61717809]\n",
      " [-1.71927246 -1.61392622 -0.12754606 -0.39554917]\n",
      " [-0.99518113 -0.21125412  0.17891211 -0.54605114]\n",
      " [ 2.73745885  3.14234061  0.4032662   0.37864622]]\n",
      "[[-3.11678067 -1.81941006 -0.94887374 -1.15439048]\n",
      " [-2.00204614 -0.76884824 -0.8647317  -0.89973801]\n",
      " [-1.48685974  0.72733345 -0.47252312 -1.22532976]\n",
      " [-2.71271427 -0.72962267 -0.67051605 -1.3730509 ]\n",
      " [-3.87285707 -1.55983507 -0.66609134 -1.75349382]\n",
      " [-1.86783469 -0.18040452 -0.69310186 -1.0716978 ]\n",
      " [ 1.16260794  2.47014459 -0.53617943 -0.28930963]\n",
      " [-2.6740704  -0.83498362 -0.71716508 -1.30203688]\n",
      " [-1.94997907  0.56768848 -0.41070691 -1.45253885]\n",
      " [ 1.78266091  3.92128321 -0.18635281 -0.52784149]]\n"
     ]
    }
   ],
   "source": [
    "# inference of nn\n",
    "# h = xW + b\n",
    "# mini-batch version nn\n",
    "import numpy as np\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "x = np.random.randn(10, 2)\n",
    "h = np.dot(x, W1) + b1\n",
    "print(W1)\n",
    "print(b1)\n",
    "print(np.dot(x, W1))\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a679fed-5b5f-41ac-866a-dffd38b69839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c12dedf-923a-4c74-899a-20fa2f3a3361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04242035, 0.13950468, 0.27911138, 0.23968806],\n",
       "       [0.11898826, 0.31672831, 0.29635171, 0.28910434],\n",
       "       [0.18439353, 0.67421984, 0.38401923, 0.22699987],\n",
       "       [0.06222727, 0.32527754, 0.3383813 , 0.20212738],\n",
       "       [0.02037508, 0.17367031, 0.33937261, 0.14760707],\n",
       "       [0.13379247, 0.45502079, 0.3333434 , 0.25508034],\n",
       "       [0.76180627, 0.92202216, 0.36907679, 0.42817289],\n",
       "       [0.06452085, 0.30259234, 0.32801756, 0.21382241],\n",
       "       [0.12455564, 0.63822963, 0.39874263, 0.18961114],\n",
       "       [0.85602512, 0.98056938, 0.45354615, 0.37102047]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sigmoid(h)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c82ae45-e27e-4b13-aa14-bf00eaf895b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (2, 4) (10, 4) (4, 3) (10, 3)\n"
     ]
    }
   ],
   "source": [
    "# add the sigmoid layer\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "x = np.random.randn(10, 2) # 2 features\n",
    "h = np.dot(x, W1) + b1\n",
    "a = sigmoid(h)\n",
    "s = np.dot(a, W2) + b2  # 3 classes (10, 3) \n",
    "print(x.shape, W1.shape, h.shape, W2.shape, s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7a1ea-05da-4b56-85b9-6040686cb5b4",
   "metadata": {},
   "source": [
    "Create the nn with class:\n",
    "\n",
    "Affine -> Sigmoid -> Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef820aca-4d8e-4101-9c07-3882c73fb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for layer sigmoid\n",
    "import numpy as np\n",
    "\n",
    "class Sigmoid():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = []  # have no params for learning\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86c6919-58b2-470c-a7cf-5dcfa5426101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine layer (全联接层)\n",
    "class Affine:\n",
    "\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e830ae15-39b3-4673-8486-be38b7337af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two layer net\n",
    "# Affine -> Sigmoid -> Affine\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        # init weight and bias\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "\n",
    "        # create layer\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "\n",
    "        # add all the weights to the params list\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bc4ea26-1c4c-40b4-ac23-0ca480c02d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.68844636,  0.39330731,  3.05041955],\n",
       "       [ 1.84456739,  0.17384794,  1.71543507],\n",
       "       [ 1.85803044,  0.38345414,  1.82811933],\n",
       "       [ 1.6901803 ,  0.62057506,  2.80134375],\n",
       "       [ 1.72725251,  0.04285055,  2.69147908],\n",
       "       [ 1.8998701 ,  0.15892358,  1.5272931 ],\n",
       "       [ 1.64426747,  0.92152412,  2.68959026],\n",
       "       [ 1.73940639, -0.27167269,  0.9539565 ],\n",
       "       [ 1.77428358,  0.11882549,  2.29472944],\n",
       "       [ 1.8055458 ,  0.39715877,  2.08358442]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference with the nn\n",
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd58c4a-28f9-4937-a48b-2a21384bbfa6",
   "metadata": {},
   "source": [
    "beautiful! Next, how dose nn train?\n",
    "\n",
    "net:\n",
    "\n",
    "X -> Affine -> Sigmoid -> Affine -> **Softmax with Loss layer**（Softmax -> CrossEntropyError -> Loss Function）\n",
    "\n",
    "* 损失函数：Softmax本质是概率，和为1，Softmax将计算结果和正确标签的one-hot形式一起传递给交叉熵函数，计算出该batch的损失\n",
    "* 导数和梯度：梯度向量，其实就是对W矩阵的各个元素求偏导后的矩阵\n",
    "* 链式法则，复合函数不断求导，进行反向传播\n",
    "* sigmoid求偏导：y(1 - y)*上一层传过来的梯度dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f6235c2-fdf2-4307-a56a-50dbb23848b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add backward to sigmoid class\n",
    "class Sigmoid():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d4f30e-f1f6-480c-b303-159c90cebe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the backward of matmul point class (矩阵乘积节点的类)\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zero_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grad[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0036240-7ea3-4bca-831c-4b9ca1e63492",
   "metadata": {},
   "source": [
    "a=b和a[...]=b的区别是使用省略号时数据被覆盖，变量指向的内存地址不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ed6763-241f-425d-831a-3758d340330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6de92-63b7-4646-812e-4ec76523d065",
   "metadata": {},
   "source": [
    "### 关于神经网络的思考\n",
    "\n",
    "对于神经网络的经典的那张图。就是类似于神经元连接的图。虽然通过代码敲了几次但是其实还是没有完全理解，直到再次学习矩阵点积运算，想到其实是特征向量和权重向量进行运算的不断迭代，每次的新的一排输出的原点就是新的特征提取向量。最终进行一个扁平运算然后输出最终的结果，其实这个最终的结果也是一种特征提取，给每一个最终特征一个概率，就可以通过argmax得到最后的分类结果。\n",
    "\n",
    "通过卷积神经网络的过程也可以理解到，整个深度学习的过程，其实就是不断提取数据特征的过程。\n",
    "\n",
    "正因为如此，我们在一开始的数据处理就显得特别重要，因为初始化的特征向量就是一开始的重要基准，对特征进行标准化，有助于后期更好的进行特征提取。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
