{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce868a5c-31cc-4a44-b34d-db4920a84ba0",
   "metadata": {},
   "source": [
    "### what is SkimLit\n",
    "\n",
    "实质上是一种Seq2seq的模型，将医学论文的摘要进行总结的模型。将每一个段落或者句子，进行总结描述，是哪一种内容，也是一种多分类问题。\n",
    "\n",
    "SkimLit 使用迁移学习的方法，通过微调预训练的语言模型（如BERT或RoBERTa）来进行科学文献的摘要生成。它的训练数据通常包括科学期刊中的摘要-全文对，以及额外的摘要摘要对，以便进行监督学习。\n",
    "\n",
    "SkimLit 的应用领域主要集中在科学研究和学术出版领域，帮助研究人员更有效地处理和理解大量的学术文献，以便快速获取信息和洞见。\n",
    "\n",
    "该项目使用的[数据集](https://github.com/Franck-Dernoncourt/pubmed-rct.git)是已经被分割标注好了的数据，很干净，但是实际上如果自己做的话需要很多的前置工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5dbb5a-da9d-4251-a4fa-a49433bf9803",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e885c8-66f8-453f-95fa-3e286f2cf602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8d7888e-2179-400f-8b76-b2040ccf55c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tensorflow-macos\n",
    "# !pip install tensorflow-metal\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3a885a-9298-49d7-8206-9cb7eaf20fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pubmed-rct'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 39 (delta 8), reused 5 (delta 5), pack-reused 25\u001b[K\n",
      "Receiving objects: 100% (39/39), 177.08 MiB | 5.52 MiB/s, done.\n",
      "Resolving deltas: 100% (15/15), done.\n",
      "\u001b[34mPubMed_200k_RCT\u001b[m\u001b[m\n",
      "\u001b[34mPubMed_200k_RCT_numbers_replaced_with_at_sign\u001b[m\u001b[m\n",
      "\u001b[34mPubMed_20k_RCT\u001b[m\u001b[m\n",
      "\u001b[34mPubMed_20k_RCT_numbers_replaced_with_at_sign\u001b[m\u001b[m\n",
      "README.md\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
    "!ls pubmed-rct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0129137f-2c8f-4358-9ad2-c87a74553538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.txt   test.txt  train.txt\n"
     ]
    }
   ],
   "source": [
    "# Check what files are in the PubMed_20K dataset \n",
    "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "665987c0-6a85-4cc8-9185-cd40b6a19c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by using the 20k dataset\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df42774e-0637-40a1-907c-35126472f3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all of the filenames in the target directory\n",
    "import os\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2533f6a-ce56-42ee-8ca5-feaed052dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "  \"\"\"\n",
    "  Reads filename (a text file) and returns the lines of text as a list.\n",
    "  \n",
    "  Args:\n",
    "      filename: a string containing the target filepath to read.\n",
    "  \n",
    "  Returns:\n",
    "      A list of strings with one string per line from the target filename.\n",
    "      For example:\n",
    "      [\"this is the first line of filename\",\n",
    "       \"this is the second line of filename\",\n",
    "       \"...\"]\n",
    "  \"\"\"\n",
    "  with open(filename, \"r\") as f:\n",
    "    return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23e7b9e2-adaa-41ad-af64-f3b53bf6a7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
       " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
       " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
       " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
       " '\\n',\n",
       " '###24854809\\n',\n",
       " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
       " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
       " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
       " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
       " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines = get_lines(data_dir + \"train.txt\")\n",
    "train_lines[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb06aab-05e9-492c-9929-5d44d11b4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "  \"\"\"Returns a list of dictionaries of abstract line data.\n",
    "\n",
    "  Takes in filename, reads its contents and sorts through each line,\n",
    "  extracting things like the target label, the text of the sentence,\n",
    "  how many sentences are in the current abstract and what sentence number\n",
    "  the target line is.\n",
    "\n",
    "  Args:\n",
    "      filename: a string of the target text file to read and extract line data\n",
    "      from.\n",
    "\n",
    "  Returns:\n",
    "      A list of dictionaries each containing a line from an abstract,\n",
    "      the lines label, the lines position in the abstract and the total number\n",
    "      of lines in the abstract where the line is from. For example:\n",
    "\n",
    "      [{\"target\": 'CONCLUSION',\n",
    "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
    "        \"line_number\": 8,\n",
    "        \"total_lines\": 8}]\n",
    "  \"\"\"\n",
    "  input_lines = get_lines(filename) # get all lines from filename\n",
    "  abstract_lines = \"\" # create an empty abstract\n",
    "  abstract_samples = [] # create an empty list of abstracts\n",
    "  \n",
    "  # Loop through each line in target file\n",
    "  for line in input_lines:\n",
    "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
    "      abstract_id = line\n",
    "      abstract_lines = \"\" # reset abstract string\n",
    "    elif line.isspace(): # check to see if line is a new line\n",
    "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "\n",
    "      # Iterate through each line in abstract and count them at the same time\n",
    "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "        line_data = {} # create empty dict to store data from line\n",
    "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
    "        line_data[\"target\"] = target_text_split[0] # get target label\n",
    "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
    "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
    "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
    "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
    "    \n",
    "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
    "      abstract_lines += line\n",
    "  \n",
    "  return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f19f90e-559a-43e5-b568-db342eea0143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from file and preprocess it\n",
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
    "len(train_samples), len(val_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db3eb40d-dae9-4a26-9970-13be2625291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'these differences remained significant at @ weeks .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 11},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 11},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'emotional eating is associated with overeating and the development of obesity .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 10},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 10}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first abstract of our training data\n",
    "train_samples[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce936a44-f300-4a07-91a8-cd430173265f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
       "2    METHODS  outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  secondary outcome measures included the wester...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           11  \n",
       "1           11  \n",
       "2           11  \n",
       "3           11  \n",
       "4           11  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebdaffb5-cb90-4a19-b6fd-5c7c1f890fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b23bf9c6-69c7-4651-a7a8-346030134cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.total_lines.plot.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "883f40a9-f940-42cb-b071-c74e8379797b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert abstract text lines into lists \n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36d2c1e6-6a7e-4485-b8e1-0602613f0acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       " 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       " 'these differences remained significant at @ weeks .']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2380b9f-ff98-4883-8998-88ef4ba07530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallyw/machine-learning-lab/mlztm/env/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Check what training labels look like\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "222fc8fa-c2f1-4c32-9aab-c7f7d3096079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels (\"target\" columns) and encode them into integers \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "# Check what training labels look like\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67491ec9-6923-448e-88df-b491c2aa9884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names and number of classes from LabelEncoder instance \n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70c633-fefb-4520-98fc-fb0a7b1e72f6",
   "metadata": {},
   "source": [
    "### Model0: baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b62a6bc9-c910-49dd-bf40-b268d633b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "  (\"tf-idf\", TfidfVectorizer()),\n",
    "  (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_sentences, \n",
    "            y=train_labels_encoded);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37b57722-93ee-437f-b844-663b748f6c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218323844829869"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate baseline on validation dataset\n",
    "model_0.score(X=val_sentences,\n",
    "              y=val_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3137ac33-04bf-48fd-8ffe-7108380c4624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 3, ..., 4, 4, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e59192a-710e-46ed-bc58-9561321cce0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 72.1832384482987,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1': 0.6989250353450294}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import helper function\n",
    "from helper_functions import calculate_results\n",
    "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed0ea0-8bf7-4140-840a-70e67b4e6852",
   "metadata": {},
   "source": [
    "### Preparing for data vectorization and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c01ab524-f75a-4b84-a117-719e7b5d6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81c290d5-c236-4905-9835-e9655398da29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how long is each lines in avarage\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b67e29e-3b9e-4cf4-b060-419d7b2650cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.5999e+05, 1.8760e+04, 1.1510e+03, 9.9000e+01, 2.8000e+01,\n",
       "        1.0000e+01, 2.0000e+00]),\n",
       " array([  1.        ,  43.14285714,  85.28571429, 127.42857143,\n",
       "        169.57142857, 211.71428571, 253.85714286, 296.        ]),\n",
       " <BarContainer object of 7 artists>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sent_lens, bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "164407b1-fb07-4fd9-bf58-7487b08a86af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long of a sentence covers 95% of the lengths?\n",
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "710dc3a1-8aea-4031-9c9a-68e2e2247901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sent_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e611714-5806-49fa-ba84-9ad53f881b53",
   "metadata": {},
   "source": [
    "### Create text vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9d8e7fe-e359-446e-b196-b0939b7e3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words are in the vocabulary\n",
    "max_tokens = 68000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c17c5b29-3527-4f39-9862-1abc65ee9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens, output_sequence_length=output_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06cb96af-0bca-4c88-acc7-30b07d54fedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 17:12:52.154376: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# adapt vectorizer to train_sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8774f1ef-8f38-45d9-9596-29afafc6bb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sentence is : we conducted a randomized control trial in which physicians-in-training were randomized to an auc-based educational intervention or a control group at an academic medical center in boston , massachusetts .\n",
      "Length of the sentence is : 30\n",
      "Text vectorization: [[   43   198     8    29    35    32     5   126 25183     9    29     6\n",
      "     26 27392   904    38    16     8    35    13    15    26  1562   274\n",
      "    768     5  7402  8035     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# test out\n",
    "import random\n",
    "target_sentence = random.choice(train_sentences)\n",
    "print(f\"Target sentence is : {target_sentence}\")\n",
    "print(f\"Length of the sentence is : {len(target_sentence.split())}\")\n",
    "print(f\"Text vectorization: {text_vectorizer([target_sentence])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2049d2c2-d126-462b-a401-f9829d06df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 64841\n",
      "Top 5 common words: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least 5 common words: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "# how many words in our training vocabulary\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\")\n",
    "print(f\"Top 5 common words: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least 5 common words: {rct_20k_text_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62540e63-c266-4956-bb22-33a333259d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization_1',\n",
       " 'trainable': True,\n",
       " 'dtype': 'string',\n",
       " 'batch_input_shape': (None,),\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None,\n",
       " 'encoding': 'utf-8',\n",
       " 'vocabulary_size': 64841}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the config of the text_vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9867e-d9aa-497b-9fff-18ba77aa68b8",
   "metadata": {},
   "source": [
    "- Vectorization only make the sentences into numbers but can not show the relationship of the words.\n",
    "- so we need Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47dd392-8622-4839-935b-d12190be6b56",
   "metadata": {},
   "source": [
    "### Create Embedding layer\n",
    "input_dim define the size of vocabulary, and output_dim defines the dimension of the embedding output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abde5c9a-a407-4054-8a47-162affddb906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding layer\n",
    "token_embed = layers.Embedding(\n",
    "    input_dim=len(rct_20k_text_vocab),\n",
    "    output_dim=128,\n",
    "    mask_zero=True,\n",
    "    name=\"token_embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "472e7340-a8f6-4a79-a144-d4d0e911e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target_sentence: we conducted a randomized control trial in which physicians-in-training were randomized to an auc-based educational intervention or a control group at an academic medical center in boston , massachusetts .\n",
      "Vectorized target sentence: [[   43   198     8    29    35    32     5   126 25183     9    29     6\n",
      "     26 27392   904    38    16     8    35    13    15    26  1562   274\n",
      "    768     5  7402  8035     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n",
      "Embedded sentence: [[[ 0.01562114 -0.02947158 -0.04362962 ...  0.00153196  0.02208545\n",
      "    0.0051954 ]\n",
      "  [-0.03097613  0.02665827 -0.01545527 ...  0.01333607 -0.04281775\n",
      "   -0.00745289]\n",
      "  [ 0.02958253 -0.0265415  -0.029838   ... -0.03157397 -0.03163259\n",
      "    0.00769433]\n",
      "  ...\n",
      "  [-0.04596529 -0.0396254  -0.0310029  ...  0.04357486  0.02015528\n",
      "   -0.012834  ]\n",
      "  [-0.04596529 -0.0396254  -0.0310029  ...  0.04357486  0.02015528\n",
      "   -0.012834  ]\n",
      "  [-0.04596529 -0.0396254  -0.0310029  ...  0.04357486  0.02015528\n",
      "   -0.012834  ]]], with shape of (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "# Show example embedding\n",
    "print(f\"Target_sentence: {target_sentence}\")\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Vectorized target sentence: {vectorized_sentence}\")\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"Embedded sentence: {embedded_sentence}, with shape of {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08e209bc-d9d1-4e8d-884b-5501486234a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 55]), TensorShape([1, 55, 128]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_sentence.shape, embedded_sentence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b299e5-6c96-4d08-99ac-efd2e2ca0f53",
   "metadata": {},
   "source": [
    "- 55 is the output sentence length setted by text_vectorizer\n",
    "- 128 is the output_dim setted by token_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3603be3-5460-49d0-a8ef-afb8fb7af0b7",
   "metadata": {},
   "source": [
    "### Create datasets (check the doc!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0422da56-4be4-4d21-9443-ea208fcd7999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "826c8060-40ed-428b-8587-0a7373995127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefetched batches\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba244ac-3d66-4b6f-a2c9-a3c9b8dba58e",
   "metadata": {},
   "source": [
    "The models will like :\n",
    "\n",
    "Input(text) --> Tokenize --> Embedding --> Layers --> Output(label probalibity)\n",
    "\n",
    "build model --> train model --> evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e42d18-1bbf-48b7-b600-10792bcd1751",
   "metadata": {},
   "source": [
    "### Model1: Conv1D with token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fdbe9617-9e13-4d8e-b555-32fb5c907a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 1D convolutional model to process sequences\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs)\n",
    "token_embeddings = token_embed(text_vectors)\n",
    "x = layers.Conv1D(\n",
    "    64,\n",
    "    kernel_size=5,\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\"\n",
    ")(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# compile\n",
    "model_1.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d823a8e-4d48-45f5-9481-8f96f351dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 55)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " token_embedding (Embedding  (None, 55, 128)           8299648   \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d_3  (None, 64)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8340997 (31.82 MB)\n",
      "Trainable params: 8340997 (31.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get summary of Conv1D model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff94ec-47fd-4cda-ab4e-bc197f9da5ba",
   "metadata": {},
   "source": [
    "- About use the subset:\n",
    "- training data contains nearly 200000 sentences, fitting will take a long time. So to keep out experiments swift, run them on a subset of the training dataset.\n",
    "- only use the first 10% of batches(about 18000 samples) of the training set to train on and the first 10% of batches from the validation set to validate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86934d02-f99f-4273-8383-7db0ebef8010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 27s 39ms/step - loss: 0.9144 - accuracy: 0.6392 - val_loss: 0.6871 - val_accuracy: 0.7380\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 20s 36ms/step - loss: 0.6584 - accuracy: 0.7552 - val_loss: 0.6316 - val_accuracy: 0.7713\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 20s 36ms/step - loss: 0.6184 - accuracy: 0.7745 - val_loss: 0.5979 - val_accuracy: 0.7839\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model_1_history = model_1.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "    epochs=3,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=int(0.1 * len(valid_dataset))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "078244b0-6165-4bcb-ac3c-128225e6c68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 9s 10ms/step - loss: 0.5986 - accuracy: 0.7849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5985892415046692, 0.7848867774009705]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on whole validation dataset\n",
    "model_1.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e339e4df-9f38-42d7-89a3-732755c6b9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.37065840e-01, 1.55582607e-01, 8.47149715e-02, 2.99972326e-01,\n",
       "        2.26642415e-02],\n",
       "       [4.91023898e-01, 2.28664279e-01, 1.27205839e-02, 2.60172248e-01,\n",
       "        7.41901621e-03],\n",
       "       [1.30823761e-01, 5.82501013e-03, 1.32411532e-03, 8.62003446e-01,\n",
       "        2.36797605e-05],\n",
       "       ...,\n",
       "       [4.30040291e-06, 7.55501795e-04, 6.77421573e-04, 3.50593223e-06,\n",
       "        9.98559296e-01],\n",
       "       [5.96649647e-02, 4.38765347e-01, 1.04137465e-01, 6.98668063e-02,\n",
       "        3.27565432e-01],\n",
       "       [1.74039513e-01, 6.73732340e-01, 4.93405238e-02, 3.93536873e-02,\n",
       "        6.35339543e-02]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions\n",
    "model_1_pred_probs = model_1.predict(valid_dataset)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbdc5ff8-2146-4e3e-a5a6-88e72d27bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1])>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert pred probs to classes\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c47212d-3136-4587-b17a-0508db968282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.48867999470409,\n",
       " 'precision': 0.78126643061663,\n",
       " 'recall': 0.7848867999470409,\n",
       " 'f1': 0.7822280729042087}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate model_1 results\n",
    "model_1_results = calculate_results(\n",
    "    y_true=val_labels_encoded,\n",
    "    y_pred=model_1_preds\n",
    ")\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59b5d5-d1b9-44bb-9171-bc4a33b6101f",
   "metadata": {},
   "source": [
    "### Model2: Feature extraction with pretrained token embeddings\n",
    "\n",
    "inputs(string) --> pretrained embeddings from tensorflow hub(Universe sentence encoder)\n",
    "\n",
    "transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7daacdae-f596-45a7-b03c-eee4d0adafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained TensorFlow Hub USE\n",
    "import tensorflow_hub as hub\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad278e0b-751e-4c24-8ae3-b602cbbd7d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/sallyw/machine-learning-lab/mlztm/env/lib/python3.11/site-packages (from tensorflow_hub) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/sallyw/machine-learning-lab/mlztm/env/lib/python3.11/site-packages (from tensorflow_hub) (4.23.4)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow_hub)\n",
      "  Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m609.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras, tensorflow_hub\n",
      "Successfully installed tensorflow_hub-0.16.1 tf-keras-2.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71dbd06c-91c0-48e1-995e-d25dd0c2c73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random training sentence :\n",
      "after an @-week baseline period , patients were randomized to once-daily placebo ( n = @ ) , esl @ mg ( n = @ ) , or esl @,@ mg ( n = @ ) .\n",
      "\n",
      "Sentence after embedding:\n",
      "[-0.01201206 -0.03525863  0.04698653 -0.02744422 -0.06384209  0.03896685\n",
      "  0.03069838 -0.00859197 -0.05859613 -0.01284317  0.06522817  0.00561847\n",
      "  0.06010677  0.03959933  0.04121524  0.0124637  -0.07487276 -0.02542698\n",
      " -0.02706639 -0.06882976  0.073079   -0.01362267 -0.05067691  0.02234137\n",
      " -0.03860472 -0.03533686 -0.00754866 -0.0507905  -0.0065466  -0.06792147](truncated output)\n",
      "\n",
      "Length of sentence embedding:\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# test out the embedding on a random sentence\n",
    "random_training_sentence = random.choice(train_sentences)\n",
    "print(f\"Random training sentence :\\n{random_training_sentence}\\n\")\n",
    "use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence])\n",
    "print(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:30]}(truncated output)\\n\")\n",
    "print(f\"Length of sentence embedding:\\n{len(use_embedded_sentence[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c3ffd0a3-1bd1-4a82-9dbf-7cf75e803048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and fit the model from USE by transfer learning\n",
    "# define feature extractor model using TF Hub layer\n",
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "# tokenize text and create embedding\n",
    "pretrained_embedding = tf_hub_embedding_layer(inputs)\n",
    "x = layers.Dense(128, activation=\"relu\")(pretrained_embedding)\n",
    "# here can be add more layers if need to \n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "model_2 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# compile the model\n",
    "model_2.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84bc149a-3cd0-40db-99f5-f4f8ab6779bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " universal_sentence_encoder  (None, 512)               256797824 \n",
      "  (KerasLayer)                                                   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256864133 (979.86 MB)\n",
      "Trainable params: 66309 (259.02 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ce18a-2b03-4f88-a355-ad3780714aab",
   "metadata": {},
   "source": [
    "there are a hole banch of parameters, but most of them will not be trained because we set trainable as False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "144f80e7-ded3-4364-a5b9-38c3ae5dce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 52s 90ms/step - loss: 0.9131 - accuracy: 0.6537 - val_loss: 0.8105 - val_accuracy: 0.6882\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 43s 76ms/step - loss: 0.7852 - accuracy: 0.6948 - val_loss: 0.7829 - val_accuracy: 0.6958\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 52s 93ms/step - loss: 0.7855 - accuracy: 0.7000 - val_loss: 0.7850 - val_accuracy: 0.6981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29a7252d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit feature extractor model for 3 epochs\n",
    "model_2.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "    epochs=3,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=int(0.1 * len(valid_dataset))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89acb32b-76d4-411d-80f9-b03f472fba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 84s 89ms/step - loss: 0.7805 - accuracy: 0.6982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7805309891700745, 0.6981993913650513]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on whole validation dataset\n",
    "model_2.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff410855-4402-4935-93e1-445233eaee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 84s 88ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.42271605, 0.37420774, 0.00191271, 0.19273399, 0.00842952],\n",
       "       [0.3587523 , 0.5066348 , 0.00143363, 0.1312234 , 0.00195592],\n",
       "       [0.20703864, 0.20470196, 0.01401198, 0.5201336 , 0.05411379],\n",
       "       ...,\n",
       "       [0.00374495, 0.00718034, 0.0531804 , 0.00165737, 0.93423694],\n",
       "       [0.0062439 , 0.04793263, 0.20905368, 0.00232448, 0.7344454 ],\n",
       "       [0.12665541, 0.138025  , 0.6488827 , 0.0046369 , 0.08180003]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions with model 2\n",
    "model_2_pred_probs = model_2.predict(valid_dataset)\n",
    "model_2_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "309ffec7-40d3-4e74-9e09-b66500239107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 4, 2])>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the predictions to classes\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
    "model_2_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a76e5245-8c75-450b-93e6-a4e16f9159c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 69.81993909704754,\n",
       " 'precision': 0.6974501497895977,\n",
       " 'recall': 0.6981993909704753,\n",
       " 'f1': 0.6948814977781582}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate results\n",
    "model_2_results = calculate_results(\n",
    "    y_true=val_labels_encoded,\n",
    "    y_pred=model_2_preds\n",
    ")\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b8ff5-ff8d-4ae7-89a9-05aea7ac3f10",
   "metadata": {},
   "source": [
    "### Model3: Conv1D with character embeddings\n",
    "\n",
    "字符级别的分词，更加细化了，但是一共只有26个字母而已，这样其实是不是也是一种简化？还是内部更加复杂？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1281236e-b09a-4e1f-96be-a9f356fad019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a f t e r   a n   @ - w e e k   b a s e l i n e   p e r i o d   ,   p a t i e n t s   w e r e   r a n d o m i z e d   t o   o n c e - d a i l y   p l a c e b o   (   n   =   @   )   ,   e s l   @   m g   (   n   =   @   )   ,   o r   e s l   @ , @   m g   (   n   =   @   )   .'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "    return \" \".join(list(text))\n",
    "\n",
    "# test splitting non-character-level sequence into characters\n",
    "split_chars(random_training_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "549d8036-b403-468a-94cd-4021c5d578bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .\n",
      "i g e   s e n s i t i z a t i o n   t o   a s p e r g i l l u s   f u m i g a t u s   a n d   a   p o s i t i v e   s p u t u m   f u n g a l   c u l t u r e   r e s u l t   a r e   c o m m o n   i n   p a t i e n t s   w i t h   r e f r a c t o r y   a s t h m a   .\n",
      "t h i s   s t u d y   a n a l y z e d   l i v e r   f u n c t i o n   a b n o r m a l i t i e s   i n   h e a r t   f a i l u r e   p a t i e n t s   a d m i t t e d   w i t h   s e v e r e   a c u t e   d e c o m p e n s a t e d   h e a r t   f a i l u r e   (   a d h f   )   .\n"
     ]
    }
   ],
   "source": [
    "# split sequence-level data splits into character-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "print(train_chars[0])\n",
    "print(val_chars[0])\n",
    "print(test_chars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7162095d-8572-4cb5-a135-d6840cfb568a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.3662574983337"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the average character length\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "75f40c0c-23fe-47e4-b156-42752617f31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.41175e+05, 3.71110e+04, 1.60000e+03, 1.27000e+02, 2.10000e+01,\n",
       "        5.00000e+00, 1.00000e+00]),\n",
       " array([1.00000000e+00, 1.98857143e+02, 3.96714286e+02, 5.94571429e+02,\n",
       "        7.92428571e+02, 9.90285714e+02, 1.18814286e+03, 1.38600000e+03]),\n",
       " <BarContainer object of 7 artists>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the distribution of sequences at character-level\n",
    "plt.hist(char_lens, bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b2b2a33-84f7-4e00-8e0d-96151c2c5ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find what character length covers 95% of sequences\n",
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bcc921ac-0bd2-412d-9ce7-3aa9711b361c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all keyboard characters for char-level embedding\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69d594e4-d9ab-4ae1-a94a-28325b0fd8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # space and OOV token\n",
    "NUM_CHAR_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d2e827ab-f344-4016-a2f6-a27c4a875d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create char-level token vectorizer instance\n",
    "char_vectorizer = TextVectorization(\n",
    "    max_tokens=NUM_CHAR_TOKENS,\n",
    "    output_sequence_length=output_seq_char_len,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    name=\"char_vectorizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3168e19-3d71-4f67-873c-fef4bae201e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt character vectorizer to training characters\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c5e1343-7824-425d-8bd6-39271395edfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters in character vocab: 28\n",
      "5 most common characters: ['', '[UNK]', 'e', 't', 'i']\n",
      "5 least common characters: ['k', 'x', 'z', 'q', 'j']\n"
     ]
    }
   ],
   "source": [
    "# check character vocabulary characteristics\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
    "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "582bc878-8b85-406d-8293-8b28b9b550b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text: \n",
      "p o s t o p e r a t i v e l y   ,   t h e   r a t e   o f   c h a n g e   i n   c e n t r a l   c o r n e a l   t h i c k n e s s   a n d   a n t e r i o r   c h a m b e r   i n f l a m m a t i o n   a t   d a y s   @   a n d   @   ,   e n d o t h e l i a l   c e l l   d e n s i t y   a t   @   m o n t h s   ,   a n d   c o r n e a l   c l a r i t y   o n   d a y   @   w e r e   c o m p a r e d   .\n",
      "\n",
      "Length of chars: 166\n",
      "\n",
      "Vectorized chars :\n",
      "[[14  7  9  3  7 14  2  8  5  3  4 21  2 12 19  3 13  2  8  5  3  2  7 17\n",
      "  11 13  5  6 18  2  4  6 11  2  6  3  8  5 12 11  7  8  6  2  5 12  3 13\n",
      "   4 11 23  6  2  9  9  5  6 10  5  6  3  2  8  4  7  8 11 13  5 15 22  2\n",
      "   8  4  6 17 12  5 15 15  5  3  4  7  6  5  3 10  5 19  9  5  6 10  2  6\n",
      "  10  7  3 13  2 12  4  5 12 11  2 12 12 10  2  6  9  4  3 19  5  3 15  7\n",
      "   6  3 13  9  5  6 10 11  7  8  6  2  5 12 11 12  5  8  4  3 19  7  6 10\n",
      "   5 19 20  2  8  2 11  7 15 14  5  8  2 10  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]]\n",
      "\n",
      "Length of vectorized chars: 290\n"
     ]
    }
   ],
   "source": [
    "# test on random sentence\n",
    "random_train_chars = random.choice(train_chars)\n",
    "print(f\"Charified text: \\n{random_train_chars}\\n\")\n",
    "print(f\"Length of chars: {len(random_train_chars.split())}\\n\")\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "print(f\"Vectorized chars :\\n{vectorized_chars}\\n\")\n",
    "print(f\"Length of vectorized chars: {len(vectorized_chars[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cb3adc45-7b38-4abd-826b-1c8ddfbc290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding layer\n",
    "char_embed = layers.Embedding(\n",
    "    input_dim=NUM_CHAR_TOKENS,\n",
    "    output_dim=25,\n",
    "    mask_zero=False,\n",
    "    name=\"char_embed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bca49b46-9479-4a1c-abb9-8a42a13af2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text (before vectorization and embedding):\n",
      "p o s t o p e r a t i v e l y   ,   t h e   r a t e   o f   c h a n g e   i n   c e n t r a l   c o r n e a l   t h i c k n e s s   a n d   a n t e r i o r   c h a m b e r   i n f l a m m a t i o n   a t   d a y s   @   a n d   @   ,   e n d o t h e l i a l   c e l l   d e n s i t y   a t   @   m o n t h s   ,   a n d   c o r n e a l   c l a r i t y   o n   d a y   @   w e r e   c o m p a r e d   .\n",
      "\n",
      "Embedded chars (after vectorization and embedding):\n",
      "[[[ 0.00364358  0.04845798 -0.00674721 ... -0.02329942 -0.00302293\n",
      "   -0.01582553]\n",
      "  [ 0.04322224 -0.03558048  0.02255918 ... -0.0383814  -0.03017029\n",
      "   -0.04810159]\n",
      "  [-0.0372312  -0.00130381  0.02208276 ...  0.02525742  0.03226322\n",
      "   -0.01807858]\n",
      "  ...\n",
      "  [-0.03350841  0.03933946  0.03929074 ...  0.03764829  0.03092643\n",
      "   -0.00914077]\n",
      "  [-0.03350841  0.03933946  0.03929074 ...  0.03764829  0.03092643\n",
      "   -0.00914077]\n",
      "  [-0.03350841  0.03933946  0.03929074 ...  0.03764829  0.03092643\n",
      "   -0.00914077]]]\n",
      "\n",
      "Character embedding shape: (1, 290, 25)\n"
     ]
    }
   ],
   "source": [
    "# test out character embedding layer\n",
    "print(f\"Charified text (before vectorization and embedding):\\n{random_train_chars}\\n\")\n",
    "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
    "print(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\")\n",
    "print(f\"Character embedding shape: {char_embed_example.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d095ba4-915d-4169-bda6-16a03f2d8418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "# input (character-level text) --> tokenize --> embedding --> layers\n",
    "# make conv1D on chars only\n",
    "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
    "char_vectors = char_vectorizer(inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "x = layers.Conv1D(\n",
    "    64,\n",
    "    kernel_size=5,\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\"\n",
    ")(char_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_3 = tf.keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    name=\"model_3_conv1D_char_embedding\"\n",
    ")\n",
    "\n",
    "# compile model \n",
    "model_3.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aa39e2e0-68cd-4b22-ae55-e5a2a231623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_conv1D_char_embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " char_vectorizer (TextVecto  (None, 290)               0         \n",
      " rization)                                                       \n",
      "                                                                 \n",
      " char_embed (Embedding)      (None, 290, 25)           1750      \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 290, 64)           8064      \n",
      "                                                                 \n",
      " global_average_pooling1d_4  (None, 64)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10139 (39.61 KB)\n",
      "Trainable params: 10139 (39.61 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check the summary of model\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2132fe38-53e5-4c94-8138-3c37ecd65969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char level batch need to be create\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_chars, train_labels_one_hot)\n",
    ").batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_chars, val_labels_one_hot)\n",
    ").batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "train_char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "79f0d4d2-c2c2-4a20-9796-1b5c91d09a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 24s 40ms/step - loss: 1.4617 - accuracy: 0.3376 - val_loss: 1.4150 - val_accuracy: 0.3983\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 21s 38ms/step - loss: 1.3735 - accuracy: 0.4122 - val_loss: 1.3389 - val_accuracy: 0.4302\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 22s 38ms/step - loss: 1.3283 - accuracy: 0.4488 - val_loss: 1.3135 - val_accuracy: 0.4408\n"
     ]
    }
   ],
   "source": [
    "# fit the model on chars only with 10% data\n",
    "model_3_history = model_3.fit(\n",
    "    train_char_dataset,\n",
    "    steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
    "    epochs=3,\n",
    "    validation_data=val_char_dataset,\n",
    "    validation_steps=int(0.1 * len(val_char_dataset))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b3c4959d-8b12-4ed4-8f6b-d623c73deacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 16s 17ms/step - loss: 1.3118 - accuracy: 0.4480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3117729425430298, 0.44800078868865967]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model 3 on whole validation char dataset\n",
    "model_3.evaluate(val_char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e800f756-e8c1-4965-b557-4595e1674e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 4s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.18311791, 0.24895434, 0.24399595, 0.1863451 , 0.13758667],\n",
       "       [0.1103473 , 0.16169944, 0.185353  , 0.06094738, 0.48165292],\n",
       "       [0.10022524, 0.15889555, 0.4032759 , 0.1105636 , 0.2270397 ],\n",
       "       ...,\n",
       "       [0.06701434, 0.11186036, 0.21072549, 0.04220208, 0.5681977 ],\n",
       "       [0.04883656, 0.10094733, 0.16332415, 0.04168338, 0.64520854],\n",
       "       [0.14379421, 0.18027633, 0.18862236, 0.0676598 , 0.4196473 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with character model only\n",
    "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
    "model_3_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e20cb4ac-6456-48ea-ac0d-acb166e870dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 4, 2, ..., 4, 4, 4])>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert predictions to classes\n",
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "model_3_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "89e941e4-414f-436d-ab54-465317866a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sallyw/machine-learning-lab/mlztm/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 44.80007943863365,\n",
       " 'precision': 0.37772119906005436,\n",
       " 'recall': 0.44800079438633655,\n",
       " 'f1': 0.39367231056863616}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Conv1D char only model results\n",
    "model_3_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                        y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee6df5-4d8a-4826-97a1-b55f8e08a9e4",
   "metadata": {},
   "source": [
    "### Model4: combining pretrained token embeddings + character embeddings(hybrid embedding layer)\n",
    "\n",
    "- create a stacked embedding to represent sequences before passing them to the sequence label prediction layer\n",
    "\n",
    "- steps:\n",
    "  1. create a token-level model like model 1\n",
    "  2. create a char-level model like model 3\n",
    "  3. combine (use layers.Concatenate) the outputs of 1 and 2\n",
    "  4. build a series of output layers on top of 3\n",
    "  5. construct a model which takes token and char-level sequences as input and produces sequence label probabilities as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f533c3f0-d62a-40aa-aca6-f672e54af5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup token inputs/model\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_output)\n",
    "\n",
    "# 2. Setup char inputs/model\n",
    "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Concatenate token and char inputs (create hybrid token embedding)\n",
    "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, \n",
    "                                                                  char_model.output])\n",
    "\n",
    "# 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf\n",
    "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
    "combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers\n",
    "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
    "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
    "\n",
    "# 5. Construct model with char and token inputs\n",
    "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
    "                         outputs=output_layer,\n",
    "                         name=\"model_4_token_and_char_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e8d65df-125d-41c0-86b7-fddc83dbe7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_token_and_char_embeddings\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " char_input (InputLayer)     [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " token_input (InputLayer)    [(None,)]                    0         []                            \n",
      "                                                                                                  \n",
      " char_vectorizer (TextVecto  (None, 290)                  0         ['char_input[0][0]']          \n",
      " rization)                                                                                        \n",
      "                                                                                                  \n",
      " universal_sentence_encoder  (None, 512)                  2567978   ['token_input[0][0]']         \n",
      "  (KerasLayer)                                            24                                      \n",
      "                                                                                                  \n",
      " char_embed (Embedding)      (None, 290, 25)              1750      ['char_vectorizer[1][0]']     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 128)                  65664     ['universal_sentence_encoder[1\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 50)                   10200     ['char_embed[1][0]']          \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " token_char_hybrid (Concate  (None, 178)                  0         ['dense_7[0][0]',             \n",
      " nate)                                                               'bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 178)                  0         ['token_char_hybrid[0][0]']   \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 200)                  35800     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 200)                  0         ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 5)                    1005      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256912243 (980.04 MB)\n",
      "Trainable params: 114419 (446.95 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get a summary\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4c38004-097e-41a4-b0c0-e9be4ba305cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# plot hybrid token and character model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2d9ae5f4-f254-49f1-a2c5-38bb70cab6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile token char model\n",
    "model_4.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.legacy.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we'll stick with Adam\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "63440c54-322f-4dad-af5d-13bcfff99e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine chars and tokens into a dataset\n",
    "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
    "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
    "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n",
    "\n",
    "# Prefetch and batch train data\n",
    "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "# Repeat same steps validation data\n",
    "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
    "val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n",
    "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e316e69a-2cd1-4312-91ff-75396ee2298b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>,\n",
       " <_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out training char and token embedding dataset\n",
    "train_char_token_dataset, val_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "72969ca3-aeb1-4ff3-84c5-cc9141ba79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 161s 276ms/step - loss: 0.9392 - accuracy: 0.6279 - val_loss: 0.8119 - val_accuracy: 0.6902\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 119s 211ms/step - loss: 0.8113 - accuracy: 0.6847 - val_loss: 0.7666 - val_accuracy: 0.7118\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 116s 207ms/step - loss: 0.8175 - accuracy: 0.6832 - val_loss: 0.7697 - val_accuracy: 0.7074\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on tokens and chars\n",
    "model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_token_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_token_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "81decc66-698f-4704-867b-b8f1e846d02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 132s 139ms/step - loss: 0.7720 - accuracy: 0.7038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7720068693161011, 0.7037931680679321]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on the whole validation dataset\n",
    "model_4.evaluate(val_char_token_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6ab0925e-25fc-4a4b-a0c5-f0e3fcadf2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 142s 148ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.42880782, 0.37506148, 0.00303074, 0.18447258, 0.00862736],\n",
       "       [0.4346552 , 0.28224072, 0.00126704, 0.2807588 , 0.00107825],\n",
       "       [0.27890745, 0.1417344 , 0.01373107, 0.5397887 , 0.02583842],\n",
       "       ...,\n",
       "       [0.00565291, 0.00919025, 0.07577018, 0.00191877, 0.9074679 ],\n",
       "       [0.01116359, 0.05466448, 0.32920718, 0.00726238, 0.5977023 ],\n",
       "       [0.18917415, 0.23968682, 0.44625884, 0.00845534, 0.11642484]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using the token-character model hybrid\n",
    "model_4_pred_probs = model_4.predict(val_char_token_dataset)\n",
    "model_4_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b7434e46-baf2-4fd9-80b7-f281db554ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 4, 2])>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn prediction probabilities into prediction classes\n",
    "model_4_preds = tf.argmax(model_4_pred_probs, axis=1)\n",
    "model_4_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8b494a69-ba03-4b9d-884a-aed9d977ee78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 70.37931947570502,\n",
       " 'precision': 0.7019032610621948,\n",
       " 'recall': 0.7037931947570502,\n",
       " 'f1': 0.7007302355142682}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get results of token-char-hybrid model\n",
    "model_4_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faad8bc-e9ce-43b0-b76a-9ab2705a8100",
   "metadata": {},
   "source": [
    "### Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings\n",
    "\n",
    "- `line_number` col and `total_lines` col can be passed to model as a positional embedding\n",
    "- turn the cols into one hot format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0008a135-dde9-41b1-b884-5e6623892221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
       "2    METHODS  outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  secondary outcome measures included the wester...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           11  \n",
       "1           11  \n",
       "2           11  \n",
       "3           11  \n",
       "4           11  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7cb42ee0-2c83-45f3-983a-3d250ee036f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "line_number\n",
       "0     15000\n",
       "1     15000\n",
       "2     15000\n",
       "3     15000\n",
       "4     14992\n",
       "5     14949\n",
       "6     14758\n",
       "7     14279\n",
       "8     13346\n",
       "9     11981\n",
       "10    10041\n",
       "11     7892\n",
       "12     5853\n",
       "13     4152\n",
       "14     2835\n",
       "15     1861\n",
       "16     1188\n",
       "17      751\n",
       "18      462\n",
       "19      286\n",
       "20      162\n",
       "21      101\n",
       "22       66\n",
       "23       33\n",
       "24       22\n",
       "25       14\n",
       "26        7\n",
       "27        4\n",
       "28        3\n",
       "29        1\n",
       "30        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"line_number\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fd1ba931-0374-411b-a58a-55e5e96ae788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGdCAYAAAAPLEfqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApUklEQVR4nO3df1TUdb7H8dfEr5CFWRRhnCsZd0VWwjyFu4r2w1JRE638I7u0pOlaXUtlhWO6/ZHdu0f8kVj3cDO3OtoPd2l31b2dq7Kwq+FyzV8kKa653TLBBLHEAUkHwu/9w/V7GzH7OGEz0PNxzpzjfL5vvvOez/l4eJ3PfOeLw7IsSwAAALii6wLdAAAAQFdAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADAQGugGupPz58/r+PHjio6OlsPhCHQ7AADAgGVZam5ultvt1nXXff1+EqGpEx0/flyJiYmBbgMAAPihtrZWffv2/drjhKZOFB0dLenCpMfExAS4GwAAYKKpqUmJiYn27/GvQ2jqRBc/kouJiSE0AQDQxXzTpTVcCA4AAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGAgNNANwMyNCzYFuoWr9smSCYFuAQCATkNowjVD0AMAdCd8PAcAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGAgNNANAMHkxgWbAt3CVftkyYRAtwAA3wvsNAEAABggNAEAABggNAEAABggNAEAABggNAEAABggNAEAABggNAEAABggNAEAABggNAEAABggNAEAABggNAEAABggNAEAABggNAEAABgImtBUUFAgh8Oh3Nxce8yyLC1atEhut1uRkZEaOXKkDh486PNzXq9Xs2fPVlxcnKKiojRp0iQdO3bMp6axsVE5OTlyOp1yOp3KycnR6dOnfWpqamo0ceJERUVFKS4uTnPmzFFra+u1ersAAKCLCYrQtGfPHv3617/WzTff7DO+bNkyFRYWqqioSHv27JHL5dKYMWPU3Nxs1+Tm5mrjxo0qLi5WRUWFzpw5o6ysLLW3t9s12dnZqqqqUklJiUpKSlRVVaWcnBz7eHt7uyZMmKCWlhZVVFSouLhY69evV15e3rV/8wAAoEsIeGg6c+aMHnroIb388suKjY21xy3L0vPPP6+nn35akydPVlpaml577TV98cUX+s1vfiNJ8ng8evXVV7VixQqNHj1at9xyi958800dOHBAf/7znyVJhw4dUklJiV555RVlZGQoIyNDL7/8sv77v/9bhw8fliSVlpbqb3/7m958803dcsstGj16tFasWKGXX35ZTU1N3/2kAACAoBPw0PTEE09owoQJGj16tM/4kSNHVF9fr8zMTHssIiJCd955p3bs2CFJqqysVFtbm0+N2+1WWlqaXfPuu+/K6XRq6NChds2wYcPkdDp9atLS0uR2u+2asWPHyuv1qrKy8mt793q9ampq8nkAAIDuKTSQL15cXKz33ntPe/bs6XCsvr5ekpSQkOAznpCQoKNHj9o14eHhPjtUF2su/nx9fb3i4+M7nD8+Pt6n5tLXiY2NVXh4uF1zOQUFBXr22We/6W0CAIBuIGA7TbW1tZo7d67efPNNXX/99V9b53A4fJ5bltVh7FKX1lyu3p+aSy1cuFAej8d+1NbWXrEvAADQdQUsNFVWVqqhoUHp6ekKDQ1VaGioysvL9R//8R8KDQ21d34u3elpaGiwj7lcLrW2tqqxsfGKNSdOnOjw+idPnvSpufR1Ghsb1dbW1mEH6qsiIiIUExPj8wAAAN1TwELTqFGjdODAAVVVVdmPIUOG6KGHHlJVVZX++Z//WS6XS2VlZfbPtLa2qry8XMOHD5ckpaenKywszKemrq5O1dXVdk1GRoY8Ho92795t1+zatUsej8enprq6WnV1dXZNaWmpIiIilJ6efk3nAQAAdA0Bu6YpOjpaaWlpPmNRUVHq1auXPZ6bm6vFixcrOTlZycnJWrx4sXr06KHs7GxJktPp1IwZM5SXl6devXqpZ8+eys/P16BBg+wLywcOHKhx48Zp5syZWr16tSTp0UcfVVZWllJSUiRJmZmZSk1NVU5OjpYvX65Tp04pPz9fM2fOZPcIAABICvCF4N9k/vz5Onv2rGbNmqXGxkYNHTpUpaWlio6OtmtWrlyp0NBQPfDAAzp79qxGjRqltWvXKiQkxK5Zt26d5syZY3/LbtKkSSoqKrKPh4SEaNOmTZo1a5ZGjBihyMhIZWdn67nnnvvu3iwAAAhqDsuyrEA30V00NTXJ6XTK4/F0+g7VjQs2der50H18smRCoFsAgC7N9Pd3wO/TBAAA0BUQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwENDStWrVKN998s2JiYhQTE6OMjAxt2bLFPm5ZlhYtWiS3263IyEiNHDlSBw8e9DmH1+vV7NmzFRcXp6ioKE2aNEnHjh3zqWlsbFROTo6cTqecTqdycnJ0+vRpn5qamhpNnDhRUVFRiouL05w5c9Ta2nrN3jsAAOhaAhqa+vbtqyVLlmjv3r3au3ev7r77bt177712MFq2bJkKCwtVVFSkPXv2yOVyacyYMWpubrbPkZubq40bN6q4uFgVFRU6c+aMsrKy1N7ebtdkZ2erqqpKJSUlKikpUVVVlXJycuzj7e3tmjBhglpaWlRRUaHi4mKtX79eeXl5391kAACAoOawLMsKdBNf1bNnTy1fvlzTp0+X2+1Wbm6unnrqKUkXdpUSEhK0dOlSPfbYY/J4POrdu7feeOMNTZkyRZJ0/PhxJSYmavPmzRo7dqwOHTqk1NRU7dy5U0OHDpUk7dy5UxkZGfrggw+UkpKiLVu2KCsrS7W1tXK73ZKk4uJiTZs2TQ0NDYqJiTHqvampSU6nUx6Px/hnTN24YFOnng/dxydLJgS6BQDo0kx/fwfNNU3t7e0qLi5WS0uLMjIydOTIEdXX1yszM9OuiYiI0J133qkdO3ZIkiorK9XW1uZT43a7lZaWZte8++67cjqddmCSpGHDhsnpdPrUpKWl2YFJksaOHSuv16vKyspr+r4BAEDXEBroBg4cOKCMjAydO3dOP/jBD7Rx40alpqbagSYhIcGnPiEhQUePHpUk1dfXKzw8XLGxsR1q6uvr7Zr4+PgOrxsfH+9Tc+nrxMbGKjw83K65HK/XK6/Xaz9vamoyfdsAAKCLCXhoSklJUVVVlU6fPq3169dr6tSpKi8vt487HA6fesuyOoxd6tKay9X7U3OpgoICPfvss1fsBbjWuupHt3ysCKCrCfjHc+Hh4erfv7+GDBmigoICDR48WC+88IJcLpckddjpaWhosHeFXC6XWltb1djYeMWaEydOdHjdkydP+tRc+jqNjY1qa2vrsAP1VQsXLpTH47EftbW1V/nuAQBAVxHw0HQpy7Lk9XqVlJQkl8ulsrIy+1hra6vKy8s1fPhwSVJ6errCwsJ8aurq6lRdXW3XZGRkyOPxaPfu3XbNrl275PF4fGqqq6tVV1dn15SWlioiIkLp6elf22tERIR9u4SLDwAA0D0F9OO5X/7ylxo/frwSExPV3Nys4uJivfPOOyopKZHD4VBubq4WL16s5ORkJScna/HixerRo4eys7MlSU6nUzNmzFBeXp569eqlnj17Kj8/X4MGDdLo0aMlSQMHDtS4ceM0c+ZMrV69WpL06KOPKisrSykpKZKkzMxMpaamKicnR8uXL9epU6eUn5+vmTNnEoQAAICkAIemEydOKCcnR3V1dXI6nbr55ptVUlKiMWPGSJLmz5+vs2fPatasWWpsbNTQoUNVWlqq6Oho+xwrV65UaGioHnjgAZ09e1ajRo3S2rVrFRISYtesW7dOc+bMsb9lN2nSJBUVFdnHQ0JCtGnTJs2aNUsjRoxQZGSksrOz9dxzz31HMwEAAIJd0N2nqSvjPk2AOS4EBxAsutx9mgAAAIIZoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMAAoQkAAMCAX6HpyJEjnd0HAABAUPMrNPXv31933XWX3nzzTZ07d66zewIAAAg6foWm999/X7fccovy8vLkcrn02GOPaffu3Z3dGwAAQNDwKzSlpaWpsLBQn376qdasWaP6+nrddtttuummm1RYWKiTJ092dp8AAAAB9a0uBA8NDdX999+v3/3ud1q6dKk++ugj5efnq2/fvnr44YdVV1fXWX0CAAAE1LcKTXv37tWsWbPUp08fFRYWKj8/Xx999JG2bt2qTz/9VPfee29n9QkAABBQof78UGFhodasWaPDhw/rnnvu0euvv6577rlH1113IYMlJSVp9erV+vGPf9ypzQIAAASKX6Fp1apVmj59uh555BG5XK7L1txwww169dVXv1VzAAAAwcKv0PThhx9+Y014eLimTp3qz+kBAACCjl/XNK1Zs0a///3vO4z//ve/12uvvfatmwIAAAg2foWmJUuWKC4ursN4fHy8Fi9e/K2bAgAACDZ+haajR48qKSmpw3i/fv1UU1PzrZsCAAAINn6Fpvj4eO3fv7/D+Pvvv69evXp966YAAACCjV+h6cEHH9ScOXO0bds2tbe3q729XVu3btXcuXP14IMPdnaPAAAAAefXt+d+9atf6ejRoxo1apRCQy+c4vz583r44Ye5pgkAAHRLfoWm8PBwvfXWW/r3f/93vf/++4qMjNSgQYPUr1+/zu4PAAAgKPgVmi4aMGCABgwY0Fm9AAAABC2/QlN7e7vWrl2rv/zlL2poaND58+d9jm/durVTmgMAAAgWfoWmuXPnau3atZowYYLS0tLkcDg6uy8AAICg4ldoKi4u1u9+9zvdc889nd0PAABAUPLrlgPh4eHq379/Z/cCAAAQtPwKTXl5eXrhhRdkWVZn9wMAABCU/Pp4rqKiQtu2bdOWLVt00003KSwszOf4hg0bOqU5AACAYOFXaPrhD3+o+++/v7N7AQAACFp+haY1a9Z0dh8AAABBza9rmiTpyy+/1J///GetXr1azc3NkqTjx4/rzJkzndYcAABAsPBrp+no0aMaN26campq5PV6NWbMGEVHR2vZsmU6d+6cXnrppc7uEwAAIKD82mmaO3euhgwZosbGRkVGRtrj999/v/7yl790WnMAAADBwu9vz/3P//yPwsPDfcb79eunTz/9tFMaAwAACCZ+7TSdP39e7e3tHcaPHTum6Ojob90UAABAsPErNI0ZM0bPP/+8/dzhcOjMmTN65pln+NMqAACgW/Lr47mVK1fqrrvuUmpqqs6dO6fs7Gx9+OGHiouL029/+9vO7hEAACDg/ApNbrdbVVVV+u1vf6v33ntP58+f14wZM/TQQw/5XBgOAADQXfgVmiQpMjJS06dP1/Tp0zuzHwAAgKDkV2h6/fXXr3j84Ycf9qsZAACAYOVXaJo7d67P87a2Nn3xxRcKDw9Xjx49CE0AAKDb8evbc42NjT6PM2fO6PDhw7rtttu4EBwAAHRLfv/tuUslJydryZIlHXahAAAAuoNOC02SFBISouPHj3fmKQEAAIKCX9c0vf322z7PLctSXV2dioqKNGLEiE5pDAAAIJj4FZruu+8+n+cOh0O9e/fW3XffrRUrVnRGXwAAAEHFr9B0/vz5zu4DAAAgqHXqNU0AAADdlV87TfPmzTOuLSws9OclAAAAgopfoWnfvn1677339OWXXyolJUWS9Pe//10hISG69dZb7TqHw9E5XQIAAASYX6Fp4sSJio6O1muvvabY2FhJF254+cgjj+j2229XXl5epzYJAAAQaH5d07RixQoVFBTYgUmSYmNj9atf/YpvzwEAgG7Jr9DU1NSkEydOdBhvaGhQc3Pzt24KAAAg2PgVmu6//3498sgj+sMf/qBjx47p2LFj+sMf/qAZM2Zo8uTJnd0jAABAwPl1TdNLL72k/Px8/exnP1NbW9uFE4WGasaMGVq+fHmnNggAABAM/ApNPXr00Isvvqjly5fro48+kmVZ6t+/v6Kiojq7PwAAgKDwrW5uWVdXp7q6Og0YMEBRUVGyLKuz+gIAAAgqfoWmzz//XKNGjdKAAQN0zz33qK6uTpL085//nNsNAACAbsmv0PSLX/xCYWFhqqmpUY8ePezxKVOmqKSkpNOaAwAACBZ+XdNUWlqqP/3pT+rbt6/PeHJyso4ePdopjQEAAAQTv3aaWlpafHaYLvrss88UERHxrZsCAAAINn6FpjvuuEOvv/66/dzhcOj8+fNavny57rrrrk5rDgAAIFj4FZqWL1+u1atXa/z48WptbdX8+fOVlpam7du3a+nSpcbnKSgo0E9+8hNFR0crPj5e9913nw4fPuxTY1mWFi1aJLfbrcjISI0cOVIHDx70qfF6vZo9e7bi4uIUFRWlSZMm6dixYz41jY2NysnJkdPplNPpVE5Ojk6fPu1TU1NTo4kTJyoqKkpxcXGaM2eOWltbr25yAABAt+RXaEpNTdX+/fv105/+VGPGjFFLS4smT56sffv26Uc/+pHxecrLy/XEE09o586dKisr05dffqnMzEy1tLTYNcuWLVNhYaGKioq0Z88euVwujRkzxufPteTm5mrjxo0qLi5WRUWFzpw5o6ysLLW3t9s12dnZqqqqUklJiUpKSlRVVaWcnBz7eHt7uyZMmKCWlhZVVFSouLhY69ev59uAAABAkuSwrvLmSm1tbcrMzNTq1as1YMCATm3m5MmTio+PV3l5ue644w5ZliW3263c3Fw99dRTki7sKiUkJGjp0qV67LHH5PF41Lt3b73xxhuaMmWKJOn48eNKTEzU5s2bNXbsWB06dEipqanauXOnhg4dKknauXOnMjIy9MEHHyglJUVbtmxRVlaWamtr5Xa7JUnFxcWaNm2aGhoaFBMT8439NzU1yel0yuPxGNVfjRsXbOrU8wGB9smSCYFuAQAkmf/+vuqdprCwMFVXV8vhcHyrBi/H4/FIknr27ClJOnLkiOrr65WZmWnXRERE6M4779SOHTskSZWVlXaQu8jtdistLc2ueffdd+V0Ou3AJEnDhg2T0+n0qUlLS7MDkySNHTtWXq9XlZWVl+3X6/WqqanJ5wEAALonvz6ee/jhh/Xqq692aiOWZWnevHm67bbblJaWJkmqr6+XJCUkJPjUJiQk2Mfq6+sVHh6u2NjYK9bEx8d3eM34+HifmktfJzY2VuHh4XbNpQoKCuxrpJxOpxITE6/2bQMAgC7Cr/s0tba26pVXXlFZWZmGDBnS4W/OFRYWXvU5n3zySe3fv18VFRUdjl26q2VZ1jfudF1ac7l6f2q+auHChZo3b579vKmpieAEAEA3dVWh6eOPP9aNN96o6upq3XrrrZKkv//97z41/nxsN3v2bL399tvavn27zw0zXS6XpAu7QH369LHHGxoa7F0hl8ul1tZWNTY2+uw2NTQ0aPjw4XbNiRMnOrzuyZMnfc6za9cun+ONjY1qa2vrsAN1UUREBPelAgDge+KqPp5LTk7WZ599pm3btmnbtm2Kj49XcXGx/Xzbtm3aunWr8fksy9KTTz6pDRs2aOvWrUpKSvI5npSUJJfLpbKyMnustbVV5eXldiBKT09XWFiYT01dXZ2qq6vtmoyMDHk8Hu3evduu2bVrlzwej09NdXW1/Xf0pAt3Po+IiFB6evpVzBIAAOiOrmqn6dIv2m3ZssXn9gBX64knntBvfvMb/dd//Zeio6Pta4ecTqciIyPlcDiUm5urxYsXKzk5WcnJyVq8eLF69Oih7Oxsu3bGjBnKy8tTr1691LNnT+Xn52vQoEEaPXq0JGngwIEaN26cZs6cqdWrV0uSHn30UWVlZSklJUWSlJmZqdTUVOXk5Gj58uU6deqU8vPzNXPmzE7/JhwAAOh6/Lqm6aKrvFtBB6tWrZIkjRw50md8zZo1mjZtmiRp/vz5Onv2rGbNmqXGxkYNHTpUpaWlio6OtutXrlyp0NBQPfDAAzp79qxGjRqltWvXKiQkxK5Zt26d5syZY3/LbtKkSSoqKrKPh4SEaNOmTZo1a5ZGjBihyMhIZWdn67nnnvtW7xEAAHQPV3WfppCQENXX16t3796SpOjoaO3fv7/Dx2rfV9ynCTDHfZoABAvT399X/fHctGnT7Iufz507p8cff7zDt+c2bNjgR8sAAADB66pC09SpU32e/+xnP+vUZgAAAILVVYWmNWvWXKs+AAAAgppfdwQHAAD4viE0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGAgNdAMAvp9uXLAp0C1ctU+WTAh0CwACiJ0mAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAA4QmAAAAAwENTdu3b9fEiRPldrvlcDj0xz/+0ee4ZVlatGiR3G63IiMjNXLkSB08eNCnxuv1avbs2YqLi1NUVJQmTZqkY8eO+dQ0NjYqJydHTqdTTqdTOTk5On36tE9NTU2NJk6cqKioKMXFxWnOnDlqbW29Fm8bAAB0QQENTS0tLRo8eLCKiooue3zZsmUqLCxUUVGR9uzZI5fLpTFjxqi5udmuyc3N1caNG1VcXKyKigqdOXNGWVlZam9vt2uys7NVVVWlkpISlZSUqKqqSjk5Ofbx9vZ2TZgwQS0tLaqoqFBxcbHWr1+vvLy8a/fmAQBAl+KwLMsKdBOS5HA4tHHjRt13332SLuwyud1u5ebm6qmnnpJ0YVcpISFBS5cu1WOPPSaPx6PevXvrjTfe0JQpUyRJx48fV2JiojZv3qyxY8fq0KFDSk1N1c6dOzV06FBJ0s6dO5WRkaEPPvhAKSkp2rJli7KyslRbWyu32y1JKi4u1rRp09TQ0KCYmBij99DU1CSn0ymPx2P8M6ZuXLCpU88H4Op9smRCoFsAcA2Y/v4O2muajhw5ovr6emVmZtpjERERuvPOO7Vjxw5JUmVlpdra2nxq3G630tLS7Jp3331XTqfTDkySNGzYMDmdTp+atLQ0OzBJ0tixY+X1elVZWfm1PXq9XjU1Nfk8AABA9xS0oam+vl6SlJCQ4DOekJBgH6uvr1d4eLhiY2OvWBMfH9/h/PHx8T41l75ObGyswsPD7ZrLKSgosK+TcjqdSkxMvMp3CQAAuoqgDU0XORwOn+eWZXUYu9SlNZer96fmUgsXLpTH47EftbW1V+wLAAB0XUEbmlwulyR12OlpaGiwd4VcLpdaW1vV2Nh4xZoTJ050OP/Jkyd9ai59ncbGRrW1tXXYgfqqiIgIxcTE+DwAAED3FLShKSkpSS6XS2VlZfZYa2urysvLNXz4cElSenq6wsLCfGrq6upUXV1t12RkZMjj8Wj37t12za5du+TxeHxqqqurVVdXZ9eUlpYqIiJC6enp1/R9AgCAriE0kC9+5swZ/e///q/9/MiRI6qqqlLPnj11ww03KDc3V4sXL1ZycrKSk5O1ePFi9ejRQ9nZ2ZIkp9OpGTNmKC8vT7169VLPnj2Vn5+vQYMGafTo0ZKkgQMHaty4cZo5c6ZWr14tSXr00UeVlZWllJQUSVJmZqZSU1OVk5Oj5cuX69SpU8rPz9fMmTPZPQIAAJICHJr27t2ru+66y34+b948SdLUqVO1du1azZ8/X2fPntWsWbPU2NiooUOHqrS0VNHR0fbPrFy5UqGhoXrggQd09uxZjRo1SmvXrlVISIhds27dOs2ZM8f+lt2kSZN87g0VEhKiTZs2adasWRoxYoQiIyOVnZ2t55577lpPAQAA6CKC5j5N3QH3aQK6N+7TBHRPXf4+TQAAAMGE0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGCA0AQAAGAgNNANAEBXceOCTYFu4ap9smRCoFsAug12mgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAwQmgAAAAyEBroBAMC1c+OCTYFu4ap9smRCoFsALoudJgAAAAOEJgAAAAOEpku8+OKLSkpK0vXXX6/09HT99a9/DXRLAAAgCBCavuKtt95Sbm6unn76ae3bt0+33367xo8fr5qamkC3BgAAAozQ9BWFhYWaMWOGfv7zn2vgwIF6/vnnlZiYqFWrVgW6NQAAEGB8e+4fWltbVVlZqQULFviMZ2ZmaseOHZf9Ga/XK6/Xaz/3eDySpKampk7v77z3i04/JwAEoxt+8ftAt3DVqp8dG+gW8C1c/L1tWdYV6whN//DZZ5+pvb1dCQkJPuMJCQmqr6+/7M8UFBTo2Wef7TCemJh4TXoEAAQn5/OB7gCdobm5WU6n82uPE5ou4XA4fJ5bltVh7KKFCxdq3rx59vPz58/r1KlT6tWr19f+jD+ampqUmJio2tpaxcTEdNp5uyPm6uowX+aYK3PMlTnmyty1nCvLstTc3Cy3233FOkLTP8TFxSkkJKTDrlJDQ0OH3aeLIiIiFBER4TP2wx/+8Fq1qJiYGP5TGWKurg7zZY65MsdcmWOuzF2rubrSDtNFXAj+D+Hh4UpPT1dZWZnPeFlZmYYPHx6grgAAQLBgp+kr5s2bp5ycHA0ZMkQZGRn69a9/rZqaGj3++OOBbg0AAAQYoekrpkyZos8//1z/9m//prq6OqWlpWnz5s3q169fQPuKiIjQM8880+GjQHTEXF0d5sscc2WOuTLHXJkLhrlyWN/0/ToAAABwTRMAAIAJQhMAAIABQhMAAIABQhMAAIABQlMX8OKLLyopKUnXX3+90tPT9de//jXQLQWdRYsWyeFw+DxcLleg2woK27dv18SJE+V2u+VwOPTHP/7R57hlWVq0aJHcbrciIyM1cuRIHTx4MDDNBtg3zdW0adM6rLNhw4YFptkAKygo0E9+8hNFR0crPj5e9913nw4fPuxTw9q6wGSuWFsXrFq1SjfffLN9A8uMjAxt2bLFPh7oNUVoCnJvvfWWcnNz9fTTT2vfvn26/fbbNX78eNXU1AS6taBz0003qa6uzn4cOHAg0C0FhZaWFg0ePFhFRUWXPb5s2TIVFhaqqKhIe/bskcvl0pgxY9Tc3Pwddxp43zRXkjRu3DifdbZ58+bvsMPgUV5erieeeEI7d+5UWVmZvvzyS2VmZqqlpcWuYW1dYDJXEmtLkvr27aslS5Zo79692rt3r+6++27de++9djAK+JqyENR++tOfWo8//rjP2I9//GNrwYIFAeooOD3zzDPW4MGDA91G0JNkbdy40X5+/vx5y+VyWUuWLLHHzp07ZzmdTuull14KQIfB49K5sizLmjp1qnXvvfcGpJ9g19DQYEmyysvLLctibV3JpXNlWaytK4mNjbVeeeWVoFhT7DQFsdbWVlVWViozM9NnPDMzUzt27AhQV8Hrww8/lNvtVlJSkh588EF9/PHHgW4p6B05ckT19fU+aywiIkJ33nkna+xrvPPOO4qPj9eAAQM0c+ZMNTQ0BLqloODxeCRJPXv2lMTaupJL5+oi1pav9vZ2FRcXq6WlRRkZGUGxpghNQeyzzz5Te3t7hz8YnJCQ0OEPC3/fDR06VK+//rr+9Kc/6eWXX1Z9fb2GDx+uzz//PNCtBbWL64g1Zmb8+PFat26dtm7dqhUrVmjPnj26++675fV6A91aQFmWpXnz5um2225TWlqaJNbW17ncXEmsra86cOCAfvCDHygiIkKPP/64Nm7cqNTU1KBYU/wZlS7A4XD4PLcsq8PY99348ePtfw8aNEgZGRn60Y9+pNdee03z5s0LYGddA2vMzJQpU+x/p6WlaciQIerXr582bdqkyZMnB7CzwHryySe1f/9+VVRUdDjG2vL1dXPF2vp/KSkpqqqq0unTp7V+/XpNnTpV5eXl9vFAril2moJYXFycQkJCOiTohoaGDkkbvqKiojRo0CB9+OGHgW4lqF38hiFrzD99+vRRv379vtfrbPbs2Xr77be1bds29e3b1x5nbXX0dXN1Od/ntRUeHq7+/ftryJAhKigo0ODBg/XCCy8ExZoiNAWx8PBwpaenq6yszGe8rKxMw4cPD1BXXYPX69WhQ4fUp0+fQLcS1JKSkuRyuXzWWGtrq8rLy1ljBj7//HPV1tZ+L9eZZVl68skntWHDBm3dulVJSUk+x1lb/++b5upyvs9r61KWZcnr9QbHmvpOLjeH34qLi62wsDDr1Vdftf72t79Zubm5VlRUlPXJJ58EurWgkpeXZ73zzjvWxx9/bO3cudPKysqyoqOjmSfLspqbm619+/ZZ+/btsyRZhYWF1r59+6yjR49almVZS5YssZxOp7VhwwbrwIED1r/8y79Yffr0sZqamgLc+XfvSnPV3Nxs5eXlWTt27LCOHDlibdu2zcrIyLD+6Z/+6Xs5V//6r/9qOZ1O65133rHq6ursxxdffGHXsLYu+Ka5Ym39v4ULF1rbt2+3jhw5Yu3fv9/65S9/aV133XVWaWmpZVmBX1OEpi7gP//zP61+/fpZ4eHh1q233urzNVVcMGXKFKtPnz5WWFiY5Xa7rcmTJ1sHDx4MdFtBYdu2bZakDo+pU6dalnXhq+HPPPOM5XK5rIiICOuOO+6wDhw4ENimA+RKc/XFF19YmZmZVu/eva2wsDDrhhtusKZOnWrV1NQEuu2AuNw8SbLWrFlj17C2LvimuWJt/b/p06fbv+969+5tjRo1yg5MlhX4NeWwLMv6bva0AAAAui6uaQIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADBAaAIAADDwf8/PcwimAqrIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the distribution of line_number column\n",
    "train_df[\"line_number\"].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84479882-6ebe-4527-8e0d-2ce9357eaa77",
   "metadata": {},
   "source": [
    "looking at the distribution of the line number, it looks like the majority of lines have a position of 15 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7b70c-8848-4dcf-b79d-387bbf6c153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Tensorflow to create one-hot-encoded tensors of line_number col\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e3921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check one hot encoded line_number features shape and sample\n",
    "train_line_numbers_one_hot.shape, train_line_numbers_one_hot[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different numbers of lines are there for \"total_lines\"?\n",
    "train_df[\"total_lines\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee30a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of total_lines\n",
    "train_df[\"total_lines\"].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d618477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the coverage of total_lines of value 20\n",
    "np.percentile(train_df.total_lines, 98)\n",
    "# which value covers 98% of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ea189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Tenserflow to create one-hot encoded tensers of total_lines column\n",
    "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
    "\n",
    "# check the sample and sample of the one hot encoded total_lines col\n",
    "train_total_lines_one_hot.shape, train_total_lines_one_hot[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d84db",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Create a token-level model (similar to model_1)\n",
    "2. Create a character-level model (similar to model_3 with a slight modification to reflect the paper)\n",
    "3. Create a \"line_number\" model (takes in one-hot-encoded \"line_number\" tensor and passes it through a non-linear layer)\n",
    "4. Create a \"total_lines\" model (takes in one-hot-encoded \"total_lines\" tensor and passes it through a non-linear layer)\n",
    "5. Combine (using layers.Concatenate) the outputs of 1 and 2 into a token-character-hybrid embedding and pass it series of output to Figure 1 and section 4.2 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts\n",
    "6. Combine (using layers.Concatenate) the outputs of 3, 4 and 5 into a token-character-positional tribrid embedding\n",
    "7. Create an output layer to accept the tribrid embedding and output predicted label probabilities\n",
    "8. Combine the inputs of 1, 2, 3, 4 and outputs of 7 into a tf.keras.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. token inputs\n",
    "token_input = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs, outputs=token_outputs)\n",
    "\n",
    "# 2. char inputs\n",
    "char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
    "char_model = tf.keras.Model(inputs=char_inputs, outputs=char_bi_lstm)\n",
    "\n",
    "# 3. line numbers inputs\n",
    "line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\")\n",
    "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(inputs=line_number_inputs, outputs=x)\n",
    "\n",
    "# 4. total lines inputs\n",
    "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\")\n",
    "y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(inputs=total_lines_inputs, outputs=y)\n",
    "\n",
    "# 5. combine token and char embeddings into a hybrid embedding\n",
    "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, char_model.output])\n",
    "\n",
    "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "\n",
    "# 6. combine positional embeddings with combined token and char embeddings into a tribrid embedding\n",
    "z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output, total_lines_model.output, z])\n",
    "\n",
    "# 7. create output layer\n",
    "output_layer = layers.Dense(\n",
    "    5,\n",
    "    activation=\"softmax\",\n",
    "    name=\"output_layer\"\n",
    ")(z)\n",
    "\n",
    "# 8. put together model\n",
    "model_5 = tf.keras.Model(\n",
    "    inputs=[\n",
    "        line_number_model.input,\n",
    "        total_line_model.input,\n",
    "        token_model.input,\n",
    "        char_model.input\n",
    "    ],\n",
    "    outputs=output_layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2833c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a summary of token, char, and positional embedding model\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the token, char, positional embedding model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which layers of our model are trainable or not\n",
    "for layer in model_5.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c032d",
   "metadata": {},
   "source": [
    "`label_smoothing` helps to regularize model (prevent overfitting) by making sure it doesn't get too focused on applying one particular label to a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile token, char, positional embedding model\n",
    "model_5.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
    "    optimizer=tf.keras.optimizer.legacy.Adam(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87062088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation datasets (4 inputs)\n",
    "train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((\n",
    "    train_line_numbers_one_hot,\n",
    "    train_total_lines_one_hot,\n",
    "    train_sentences,\n",
    "    train_chars))\n",
    "train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_pos_char_token_dataset = tf.data.Dataset.zip((\n",
    "    train_pos_char_token_data,\n",
    "    train_pos_char_token_labels))\n",
    "train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# validation dataset\n",
    "val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((\n",
    "    val_line_numbers_one_hot,\n",
    "    val_total_lines_one_hot,\n",
    "    val_sentences,\n",
    "    val_chars\n",
    "))\n",
    "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_pos_char_token_dataset = tf.data.Dataset.zip((\n",
    "    val_pos_char_token_data,\n",
    "    val_pos_char_token_labels\n",
    "))\n",
    "val_pos_char_token_dataset = val_pos_char_token_dataset.batcha(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# check input shapes\n",
    "train_pos_char_token_dataset, val_pos_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f761b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the token, char, and positional embedding model\n",
    "history_model_5 = model_5.fit(\n",
    "    train_pos_char_token_dataset,\n",
    "    steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)),\n",
    "    epochs=3,\n",
    "    validation_data=val_pos_char_token_dataset,\n",
    "    validation_steps=int(0.1 * len(val_pos_char_token_dataset))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd01cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with token-char-positional hybrid model\n",
    "model_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1)\n",
    "model_5_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30178344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn prediction probabilities into prediction classes\n",
    "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
    "model_5_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate results of token-char-positional hybrid model\n",
    "model_5_results = calculate_results(\n",
    "    y_true=val_labels_encoded,\n",
    "    y_pred=model_5_preds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3c280",
   "metadata": {},
   "source": [
    "### Compare model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb015c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine model results into Dataframe\n",
    "all_model_results = pd.DataFrame({\n",
    "    \"baseline\": baseline_results,\n",
    "    \"custom_token_embed_conv1d\": model_1_results,\n",
    "    \"pretrained_token_embed\": model_2_results,\n",
    "    \"custom_char_embed_conv1d\": model_3_results,\n",
    "    \"hybrid_char_token_embed\": model_4_results,\n",
    "    \"tribrid_pos_char_token_embed\": model_5_results\n",
    "})\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the accuracy to same scale as other metrics\n",
    "all_model_result[\"accuracy\"] = all_model_results[\"accuracy\"]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and compare all of the model results\n",
    "all_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb83d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort model results by f1 score\n",
    "all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de745f",
   "metadata": {},
   "source": [
    "### Save and load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326fee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_5.save(\"skimlit_tribrid_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c2093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "model_path = \"skimlit_tribrid_model/\"\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with the loaded model\n",
    "loaded_pred_probs = loaded_model_predict(val_pos_char_token_dataset, verbose=1)\n",
    "loaded_preds = tf.argmax(loaded_pred_probs, axis=1)\n",
    "loaded_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d249de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model's predictions\n",
    "loaded_model_results = calculate_results(\n",
    "    val_labels_encoded,\n",
    "    loaded_preds\n",
    ")\n",
    "loaded_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a48f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loaded model results with original trained model results (should be quite close)\n",
    "np.isclose(list(model_5_results.values()), list(loaded_model_results.values()), rtol=1e-02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b23218",
   "metadata": {},
   "source": [
    "### Evaluate model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ac920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset batch and prefetched\n",
    "test_pos_char_token_data = tf.data.Dataset.from_tensor_slice((\n",
    "    test_line_numbers_one_hot,\n",
    "    test_total_lines_one_hot,\n",
    "    test_sentences,\n",
    "    test_chars\n",
    "))\n",
    "test_pos_char_token_labels = tf.data.Dataset.from_tensor_slice(test_labels_one_hot)\n",
    "test_pos_char_token_dataset = tf.data.Dataset.zip((test_pos_char_token_data, test_pos_char_token_labels))\n",
    "test_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# check shapes\n",
    "test_pos_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fe521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test dataset\n",
    "test_pred_probs = loaded_model.predict(\n",
    "    test_pos_char_token_dataset,\n",
    "    verbose=1\n",
    ")\n",
    "test_preds = tf.argmax(test_pred_probs, axis=1)\n",
    "test_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aefc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model test predictions\n",
    "loaded_model_test_results = calculate_results(\n",
    "    y_true=test_labels_encoded,\n",
    "    y_pred=test_preds\n",
    ")\n",
    "loaded_model_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053df5f5",
   "metadata": {},
   "source": [
    "### Find most wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of class names of test predictions\n",
    "test_pred_classes = [label_encoder.classes_[pred] for pred in test_preds]\n",
    "test_pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d461d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediction-enriched test dataframe\n",
    "test_df[\"prediction\"] = test_pred_classes\n",
    "test_df[\"pred_prob\"] = tf.refuce_max(test_pred_probs, axis=1).numpy()\n",
    "test_df[\"correct\"] = test_df[\"prediction\"] == test_df[\"target\"]\n",
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 100 most wrong samples\n",
    "top_100_wrong = test_df[test_df[\"correct\"] == False].sort_values(\"pred_prob\", ascending=False)[:100]\n",
    "top_100_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf06e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate top wrong preds\n",
    "for row in top_100_wrong[0:10].itertuples(): # adjust indexes to view different samples\n",
    "  _, target, text, line_number, total_lines, prediction, pred_prob, _ = row\n",
    "  print(f\"Target: {target}, Pred: {prediction}, Prob: {pred_prob}, Line number: {line_number}, Total lines: {total_lines}\\n\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"-----\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
