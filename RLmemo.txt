### 基础理论部分1/5
强化学习特点：
只有奖励信号，信号可能延迟
时间序列的因素很重要
当前行为影响后续接收的数据

概念分解
·奖励Reward
最大化R(t)
t是时间
R是在该时间的表现
然后最大化奖励就是目标

·序列决策Sequential Decision Making
行为系列决策
有时候宁愿牺牲短期奖励来获得长期奖励

·个体和环境Agent&Environment
在t时刻
个体对环境有一个观察评估O(t)，个体做出一个行为A(t)
环境接收A(t)，并且更新环境信息O(t＋1)，并且给个体一个奖励信号R(t＋1)
个体从环境得到这个R(t＋1)

·历史和状态History&State
历史是观测，行为，奖励的序列
H=ORA
状态是关于历史的一个函数
S=f(H)

·个体状态
个体可以使用的，决定未来动作的所有信息。
Sat=f(H)

·信息状态
历史上所有的有用信息，又称为Markov状态。（马尔可夫状态）

·马尔可夫属性（Markov Property）
未来状态只依赖于当前状态，和历史状态无关。

·完全可观测的环境（Fully Observable Environment）
个体能直接观测到的环境状态
个体对环境的观测＝个体状态＝环境状态
正确地说，这种问题是一个马尔可夫决定过程。

·部分可观测环境（Partially Observable Environment）
个体不能观测到自己的绝对位置。只能去预估。所以个体状态≠环境状态
预估方法有几个：
一个幼稚的方法比如，记住完整历史。
其他还有：
1，Beliefs of Environment
虽然不知道环境状态，但是个体可以用已有经验，也就是数据，利用概率分布作为当前状态的呈现。
2，Recurrent Neural Network
不需要知道概率，只根据当前的个体状态以及当前时刻个体的观测，送入循环神经网络来得到一个状态的呈现。

个体的组成部分
·策略Policy
决定行为的机制，从状态到行为的映射，可以确定也可以不确定。
·价值函数Value Function
未来奖励的预测，当前状态的好坏。
用一个Value值来评估。
一个策略对应一个价值函数。
·模型Model
个体对环境的思考的一个建模。模型。
--这个模型的泛用性是不是越大越好呢，但是越大对具体的指导又太笼统，还是要落到具体的点，比如自动驾驶的话，你不具体就会有很多的危险产生。
模型要解决的问题：一个是状态转化率，下一个可能状态的概率，另一个是预测即时奖励。
非必须。

环境实际运行机制：环境动力学。
--这是什么？应该就是指环境，但是动力学的话应该包含的东西更多。

个体的分类：
根据工具分类三种value based,policy based,actor-critic
根据环境依存分基于模型和不基于模型
--这个和莫烦python讲解的一样，莫烦的更好理解。

学习和规划的常用解决思路：learning and planning
先学习和了解环境的工作方式，得到一个模型，然后用这个模型进行规划。

探索和利用：exploration and exploitation
相当于你去探索新的餐馆，或者去已知的餐馆。
两个决策需要平衡，发现好的策略，同时又不至于在试错的过程中丢失太多奖励。
关键词，试错，平衡。

预测和控制：prediction and control
预测：给定一个策略，评价未来，未来就是要求的价值函数。
控制：直接找到一个好的策略来最大化奖励。在所有可能的策略下找最优。





### 基础理论部分2/5--马尔可夫决策过程MDP（Markov decision process）
对完全可观测的环境进行描述，用观测到的内容决定决策需要的特征。
几乎所有的强化学习问题都可以转化为MDP。
马尔可夫性质：未来只和现在有关，与过去无关。只用St决定当前决策。
某一状态信息包含了相关的历史信息。只需要用当前状态进行决策。

马尔可夫过程：Markov process是一个二元组<S,P>
无记忆的随机过程。memoryless random process
S是一个有限的状态集合。
P是一个状态转移矩阵。P定义了所有状态s转移到所有后续状态s'的概率。

马尔可夫奖励过程：Markov reword process是一个四元组<S,P,R,λ>
R是一个奖励函数，是一个<期望>，也就是到下一个状态的概率的，<均值E>。
λ是一个折扣因子，或者衰减系数，在0到1之间。
奖励是状态的一个反馈。
回报Return,Gt是从时间t开始的总折扣奖励。
价值函数value Function
Bellman Equation for MRPs贝尔曼方程其实就是价值函数里的Gt写成递归的形式。
--好的解释：
R(t＋1)是一个即时奖励！是状态转移。
Gt是一个长期奖励！是状态转移链。
Vs价值函数，是一个状态和很多状态转移链相连，然后求所有链子的期望来衡量St的价值。

马尔可夫决策过程Markov decision process是一个五元组<S,A,P,R,λ>
加入一个A是一个动作集。状态包括了动作集合。某个动作又会导致不确定的状态转移。
（如果某个动作产生确定的状态转移就是一个动态规划问题了。）
策略policy，是给定状态的动作分布。A/S，策略决定智能体行为。MDP策略依赖于当前状态。

贝尔曼期望方程：
--每一条链子的每个点都有各自的链子，是一个递归的状态节点生链子
状态值函数state-value Function策略分布的概率v
--某一个动作的价值可以用该动作所处的状态的后继状态的价值来表述
动作值函数action-value Function加入了动作的要素，从策略坍缩成了一个具体的动作q
--某一个状态的价值可以用该状态下所有动作的价值来表述
最优Optimal策略取得最优状态值函数，和最优动作值函数。可以通过最大化最优行为价值函数来找到最优策略。






### 基础理论部分3/5
### 基础理论部分4/5
### 基础理论部分5/5
### 应用实践部分1/5
### 应用实践部分2/5
### 应用实践部分3/5
### 应用实践部分4/5
### 应用实践部分5/5




