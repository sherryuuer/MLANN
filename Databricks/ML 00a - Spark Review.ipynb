{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af3ee6fa-fb60-444d-bad8-e558934c9f87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d11444ae-502f-4be9-9ece-4b42e17b33b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"1108b110-983d-4034-9156-6b95c04dc62c\"/>\n",
    "\n",
    "# Spark Review\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)このレッスンでは次を行います:<br>\n",
    " - Spark DataFrameを作成する\n",
    " - Spark UIで分析する\n",
    " - データをキャッシュする\n",
    " - Pandas DataFrame と Spark DataFrame を行き来する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad42bb68-c929-4976-9625-3140998bf4b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"890d085b-9058-49a7-aa15-bff3649b9e05\"/>\n",
    "\n",
    "![](https://files.training.databricks.com/images/sparkcluster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e4a6e20-87b2-43c7-8b6f-e6bd61827a20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18b0a5e7-f7df-4fc6-ab88-48b18bab84d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"df081f79-6894-4174-a554-fa0943599408\"/>\n",
    "\n",
    "## Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0922e409-7b2d-470f-8518-5aa2d42a6576",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "df = (spark.range(1, 1000000)\n",
    "      .withColumn(\"id\", (col(\"id\") / 1000).cast(\"integer\"))\n",
    "      .withColumn(\"v\", rand(seed=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51699384-abc0-4910-b417-191910e62c6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"a0c6912d-a8d6-449b-a3ab-5ca91c7f9805\"/>\n",
    "\n",
    "なぜ、上のコマンドではSparkジョブが実行されなかったのでしょうか？データに「触れなかった」ので、Sparkはクラスタ全体で何も実行する必要がありませんでした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1039d1c5-79fa-4d73-a3ed-a63e3e8affc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.sample(.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "188723ad-decf-4871-99d8-606e0aa2f7f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"6eadef21-d75c-45ba-8d77-419d1ce0c06c\"/>\n",
    "\n",
    "## Views\n",
    "\n",
    "この作成したDataFrameにSQLでアクセスするにはどうすればよいでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f934be90-24ff-4f93-9258-6ea06cabb88e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f485f2-3af5-49bd-ba42-94651dbe298b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM df_temp LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f245d50d-28d6-4445-b72e-ddae1465dbc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"2593e6b0-d34b-4086-9fed-c4956575a623\"/>\n",
    "\n",
    "## Count\n",
    "\n",
    "レコードの数を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e3fc39-fbb4-42b7-a4a2-a7ca4ffb6c49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e8b8356-51b5-4af9-aef0-29be29cc1fde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"5d00511e-15da-48e7-bd26-e89fbe56632c\"/>\n",
    "\n",
    "## Spark UI\n",
    "\n",
    "Spark UIを使ってみましょう - shuffle readとshuffle writeのフィールドはどうなっていますか？次のコマンドがヒントになるはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8f51d5-52c8-47f2-ae53-7207f2f79e58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd2510d3-30a6-475c-87ad-678148fd53b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"50330454-0168-4f50-8355-0204632b20ec\"/>\n",
    "\n",
    "## キャッシュ (Cache)\n",
    "\n",
    "データに繰り返しアクセスする場合は、データをキャッシュすることでより高速になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e81ce91-d2f3-4c88-9f10-d7510597a8c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f66802-4573-4df2-9555-a4f8848e1137",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"7dd81880-1575-410c-a168-8ac081a97e9d\"/>\n",
    "\n",
    "## Count を再実行 (Re-run Count)\n",
    "\n",
    "すごい！こんなに速くなりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd034a52-ff8c-4ad8-b341-64e467a35c41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a53e3a9-7f28-425e-9494-3ac17f063577",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"ce238b9e-fee4-4644-9469-b7d9910f6243\"/>\n",
    "\n",
    "## データの収集 (Collect Data)\n",
    "\n",
    "データをドライバに引き戻すとき（例えば、 **`.collect()`** , **` .toPandas()`** などを呼び出す場合）、ドライバに戻すデータの量に注意する必要があります。量が多すぎる場合、out of memory (OOM) 例外が発生する可能性があります。\n",
    "\n",
    "ベストプラクティスは、データセットが小さいことが分かっている場合を除き、レコードの数を明示的に制限してから **`.collect()`** や **`.toPandas()`** を呼び出すことです."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4027ac8b-c3a1-44ef-bf81-59c57d0718d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "523e50d1-e22a-4952-919f-b5bcceea690f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"279e3325-b121-402b-a2d0-486e1cc26fc0\"/>\n",
    "\n",
    "## <a href=\"https://www.youtube.com/watch?v=l6SuXvhorDY&feature=emb_logo\" target=\"_blank\">Spark 3.0</a> の新機能 (What's new in Spark 3.0)\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=jzrEc4r90N8&feature=emb_logo\" target=\"_blank\">Adaptive Query Execution (AQE)</a>\n",
    "  * クエリーの実行中に統計情報を収集し、動的にクエリーの実行方法を最適化します\n",
    "    * サイズの小さなシャッフルパーティションを動的に結合(coalesce)します\n",
    "    * ジョイン戦略を動的に切り替えます\n",
    "    * Skew ジョイン(一部のパーティションに偏って多くのデータがある場合(Skew)のジョイン)を動的に最適化します\n",
    "  * 次のプロパティ設定を行います: **`spark.sql.adaptive.enabled=true`**\n",
    "* Dynamic Partition Pruning (DPP)\n",
    "  * 他のクエリの実行結果に基づいて、クエリに関係するデータを持たないパーティションのスキャンを避けることができます\n",
    "* Joinのヒント\n",
    "* <a href=\"https://www.youtube.com/watch?v=UZl0pHG-2HA&feature=emb_logo\" target=\"_blank\">Pandas UDFの改良</a>\n",
    "  * 型のヒント\n",
    "  * イテレータ\n",
    "  * Pandas Function API (mapInPandas, applyInPandas など)\n",
    "* その他多数上記リンク先の <a href=\"https://spark.apache.org/docs/latest/api/python/migration_guide/pyspark_2.4_to_3.0.html\" target=\"_blank\">移行ガイド</a> とリソースをご覧ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f05345-1d52-45aa-945c-e3523aa0c840",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1158389040245668,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 00a - Spark Review",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
