{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38c7340f-3665-4e74-9c9b-1c7cbb0b031d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ff1a5f7-0f56-41e4-862d-bc8149fe4985",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## About the Exam\n",
    "- Number of items: 45 multiple-choice questions\n",
    "- Time limit: 90 minutes\n",
    "- Passing score: At least 70% on the overall exam\n",
    "- Registration fee: USD 200, plus applicable taxes as required per local law\n",
    "- Delivery method: Online Proctored\n",
    "- Test aides: None allowed.\n",
    "- Prerequisite: None required; course attendance and six months of hands-on experience in Databricks is highly recommended\n",
    "- Validity: 2 years\n",
    "- Recertification: Recertification is required every two years to maintain your certified status. To recertify, you must take the full exam that is currently live. Please review the “Getting Ready for the Exam” section on the exam webpage to prepare for taking the exam again.\n",
    "- Unscored Content: Exams may include unscored items to gather statistical information for future use. These items are not identified on the form and do not impact your score. Additional time is factored into account for this content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c854422-8437-44ac-9797-e60ae93abb23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Recommended Training\n",
    "- Instructor-led: Scalable Machine Learning with Apache Spark\n",
    "- Self-paced (available in Databricks Academy): Scalable Machine Learning with\n",
    "Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae1d4713-be1c-47fa-b2c1-b814847dd175",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Out-of-scope\n",
    "- Advanced ML Operations: Webhooks, Automation, Deployment, Monitoring, CLI/REST APIs\n",
    "- Advanced ML Workflows: Target Encoding, Embeddings, Deep Learning, NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acb2cc1c-0cd8-47fe-9faf-4adec8addab2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exam Platform\n",
    "see the link below<br>\n",
    "<a href=\"https://www.webassessor.com/databricks\">webassessor</a>.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5179a6f-e6c0-4737-90c8-ba09186d7aaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "- install secure browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88d00257-2717-4732-b5d7-110f5b3f6c37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Exam Outline\n",
    "## Section 1: Databricks Machine Learning\n",
    "### Databricks ML\n",
    "- Identify when a standard cluster is preferred over a single-node cluster and\n",
    "vice versa\n",
    "- Connect a repo from an external Git provider to Databricks repos.\n",
    "- Commit changes from a Databricks Repo to an external Git provider.\n",
    "- Create a new branch and commit changes to an external Git provider.\n",
    "- Pull changes from an external Git provider back to a Databricks workspace.\n",
    "- Orchestrate multi-task ML workflows using Databricks jobs.\n",
    "### Databricks Runtime for Machine Learning\n",
    "- Create a cluster with the Databricks Runtime for Machine Learning.\n",
    "- Install a Python library to be available to all notebooks that run on a cluster.\n",
    "### AutoML\n",
    "- Identify the steps of the machine learning workflow completed by AutoML.\n",
    "- Identify how to locate the source code for the best model produced by\n",
    "AutoML.\n",
    "- Identify which evaluation metrics AutoML can use for regression problems.\n",
    "- Identify the key attributes of the data set using the AutoML data exploration\n",
    "notebook.\n",
    "### Feature Store\n",
    "- Describe the benefits of using Feature Store to store and access features for\n",
    "machine learning pipelines.\n",
    "- Create a feature store table.\n",
    "- Write data to a feature store table.\n",
    "- Train a model with features from a feature store table.\n",
    "- Score a model using features from a feature store table.\n",
    "### Managed MLflow\n",
    "- Identify the best run using the MLflow Client API.\n",
    "- Manually log metrics, artifacts, and models in an MLflow Run.\n",
    "- Create a nested Run for deeper Tracking organization.\n",
    "- Locate the time a run was executed in the MLflow UI.\n",
    "- Locate the code that was executed with a run in the MLflow UI\n",
    "- Register a model using the MLflow Client API.\n",
    "- Transition a model’s stage using the Model Registry UI page.\n",
    "- Transition a model’s stage using the MLflow Client API.\n",
    "- Request to transition a model’s stage using the ML Registry UI page.\n",
    "\n",
    "## Section 2: ML Workflows\n",
    "### Exploratory Data Analysis\n",
    "- Compute summary statistics on a Spark DataFrame using .summary()\n",
    "- Compute summary statistics on a Spark DataFrame using dbutils data\n",
    "summaries.\n",
    "- Remove outliers from a Spark DataFrame that are beyond or less than a\n",
    "designated threshold.\n",
    "### Feature Engineering\n",
    "- Identify why it is important to add indicator variables for missing values that\n",
    "have been imputed or replaced.\n",
    "- Describe when replacing missing values with the mode value is an\n",
    "appropriate way to handle missing values.\n",
    "- Compare and contrast imputing missing values with the mean value or\n",
    "median value.\n",
    "- Impute missing values with the mean or median value.\n",
    "- Describe the process of one-hot encoding categorical features.\n",
    "- Describe why one-hot encoding categorical features can be inefficient for\n",
    "tree-based models.\n",
    "### Training\n",
    "- Perform random search as a method for tuning hyperparameters.\n",
    "- Describe the basics of Bayesian methods for tuning hyperparameters.\n",
    "- Describe why parallelizing sequential/iterative models can be difficult.\n",
    "- Understand the balance between compute resources and parallelization.\n",
    "- Parallelize the tuning of hyperparameters using Hyperopt and SparkTrials.\n",
    "- Identify the usage of SparkTrials as the tool that enables parallelization for\n",
    "tuning single-node models.\n",
    "### Evaluation and Selection\n",
    "- Describe cross-validation and the benefits of downsides of using\n",
    "cross-validation over a train-validation split.\n",
    "- Perform cross-validation as a part of model fitting.\n",
    "- Identify the number of models being trained in conjunction with a\n",
    "grid-search and cross-validation process.\n",
    "- Describe Recall and F1 as evaluation metrics.\n",
    "- Identify the need to exponentiate the RMSE when the log of the label variable\n",
    "is used.\n",
    "- Identify that the RMSE has not been exponentiated when the log of the label\n",
    "variable is used\n",
    "## Section 3: Spark ML\n",
    "### Distributed ML Concepts\n",
    "- Describe some of the difficulties associated with distributing machine\n",
    "learning models.\n",
    "- Identify Spark ML as a key library for distributing traditional machine learning\n",
    "work.\n",
    "- Identify scikit-learn as a single-node solution relative to Spark ML.\n",
    "### Spark ML Modeling APIs\n",
    "- Split data using Spark ML.\n",
    "- Identify key gotchas when splitting distributed data using Spark ML.\n",
    "- Train / evaluate a machine learning model using Spark ML.\n",
    "- Describe Spark ML estimator and Spark ML transformer.\n",
    "- Develop a Pipeline using Spark ML.\n",
    "- Identify key gotchas when developing a Spark ML Pipeline.\n",
    "### Hyperopt\n",
    "- Identify Hyperopt as a solution for parallelizing the tuning of single-node\n",
    "models.\n",
    "- Identify Hyperopt as a solution for Bayesian hyperparameter inference for\n",
    "distributed models.\n",
    "- Parallelize the tuning of hyperparameters for Spark ML models using\n",
    "Hyperopt and Trials.\n",
    "- Identify the relationship between the number of trials and model accuracy.\n",
    "### Pandas API on Spark\n",
    "- Describe key differences between Spark DataFrames and Pandas on Spark\n",
    "DataFrames.\n",
    "- Identify the usage of an InternalFrame making Pandas API on Spark not quite\n",
    "as fast as native Spark.\n",
    "- Identify Pandas API on Spark as a solution for scaling data pipelines without\n",
    "much refactoring.\n",
    "- Convert data between a PySpark DataFrame and a Pandas on Spark\n",
    "DataFrame.\n",
    "- Identify how to import and use the Pandas on Spark APIs\n",
    "### Pandas UDFs/Function APIs\n",
    "- Identify Apache Arrow as the key to Pandas <-> Spark conversions.\n",
    "- Describe why iterator UDFs are preferred for large data.\n",
    "- Apply a model in parallel using a Pandas UDF.\n",
    "- Identify that pandas code can be used inside of a UDF function.\n",
    "- Train / apply group-specific models using the Pandas Function API.\n",
    "## Section 4: Scaling ML Models\n",
    "### Model Distribution\n",
    "- Describe how Spark scales linear regression.\n",
    "- Describe how Spark scales decision trees.\n",
    "### Ensembling Distribution\n",
    "- Describe the basic concepts of ensemble learning.\n",
    "- Compare and contrast bagging, boosting, and stacking\n",
    "\n",
    "\n",
    "<a href=\"https://www.databricks.com/learn/certification/machine-learning-associate\">Databricks Certified Machine Learning Associate</a>.<br/>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00_Overview",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
