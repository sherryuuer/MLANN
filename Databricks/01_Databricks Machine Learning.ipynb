{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e89ae98-5d7e-47a9-9207-a8842f2362c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Section 1: Databricks Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a4d1e2-fc0a-477c-b1c7-0dca4e03000d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84966ba-9807-46f9-baa0-c5f97724ecd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Databricks ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d530317-5a48-4de1-89e7-aadf5d125dc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify when a standard cluster is preferred over a single-node cluster and vice versa\n",
    "\n",
    "# シングルノードのクラスタでもDriverノードを持つので、Sparkを動かすことは可能。\n",
    "# 大規模で並列分散処理が必要な場合はマルチノード、小規模・またはSparkを使わない(sklearnなど)でモデルを開発する場合はシングルノードが望ましい\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/databricks/compute/cluster-config-best-practices#--training-machine-learning-models\n",
    "# https://learn.microsoft.com/en-us/azure/databricks/compute/configure#--single-node-or-multi-node-compute\n",
    "# https://community.databricks.com/t5/data-engineering/when-should-i-use-single-node-clusters-vs-standard/td-p/24301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7696a717-bc03-4c06-9a42-db5a2a78ff62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect a repo from an external Git provider to Databricks repos.\n",
    "\n",
    "# ワークスペース>「作成」>「リポジトリ」でリポジトリURL、プロバイダーを入力してclone可能\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/databricks/repos/get-access-tokens-from-git-provider\n",
    "# https://github.com/lorenzo1285/-scalable-machine-learning-with-apache-spark-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f5c0c8f-8989-4284-87c4-3e5880b718ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Commit changes from a Databricks Repo to an external Git provider.\n",
    "\n",
    "# Commit/Pushはコンソール上から可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562984fc-5454-4124-9d76-2bb6dd4612c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new branch and commit changes to an external Git provider.\n",
    "# 集成了Github的仓库，可以在Databricks进行git各种操作\n",
    "# ブランチ作成はコンソール上から可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699e054b-42e2-4aaf-84ab-bf11fd97af8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pull changes from an external Git provider back to a Databricks workspace.\n",
    "\n",
    "# プルもコンソール上から可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "173ed59b-5be9-4b4c-9f5f-39ff83d852bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Orchestrate multi-task ML workflows using Databricks jobs.\n",
    "# workflow功能可以自定义task和他们的上下依存关系，很像airflow\n",
    "# 使用的是Job cluster，runtime包括Standard（适合ETL）和ML\n",
    "# ←の「ワークフロー」>「ジョブを作成」後、タスクに依存関係を持たせることで可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e2003e-b5d8-46ee-a74a-bc956492160b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Databricks Runtime for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c3ec6b7-2602-4b06-9805-c5fefd09892b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a cluster with the Databricks Runtime for Machine Learning.\n",
    "\n",
    "# コンソール上から確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8785f3-36bf-4794-b2f9-26ff5b05ee3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install a Python library to be available to all notebooks that run on a cluster.\n",
    "\n",
    "# !pip install or install libraries in cluster\n",
    "\n",
    "# https://docs.databricks.com/ja/libraries/index.html\n",
    "# 分为Notebook级别和Cluster级别，也就是安装的库可以使用的范围不同，Cluster的安装就可给该集群所有的笔记本使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99cdb5ee-d3cd-4a5e-b222-d5ddfa024ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c97bbfa-2cc0-4fc1-a465-7a9acf5ebd0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify the steps of the machine learning workflow completed by AutoML.\n",
    "# 是左边栏的实验功能\n",
    "\n",
    "# 1. 欠損値の補完\n",
    "# 2. チューニング\n",
    "# 3. 学習\n",
    "# 4. 評価\n",
    "# 5. EDA(自動作成だが、本来はモデル作成の前にやるべき)\n",
    "\n",
    "# モデルのデプロイはModelsのコンポーネント\n",
    "# 生成最好的模型后，需要右上角手动登录到register，然后再deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a1ae4a-22c4-4fa3-9d8e-07cc5c21640a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify how to locate the source code for the best model produced by AutoML.\n",
    "from databricks import automl\n",
    "\n",
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "summary = automl.regress(train_df, target_col=\"price\", primary_metric=\"rmse\", timeout_minutes=5, max_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd79e25-bb3d-44c4-bdd1-beba03b6d15d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify which evaluation metrics AutoML can use for regression problems.\n",
    "\n",
    "help(automl.regress)\n",
    "\n",
    "# https://docs.databricks.com/ja/machine-learning/automl/train-ml-model-automl-api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b80efad-f8d5-4858-a7ae-4982d97cbe2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify the key attributes of the data set using the AutoML data explorationnotebook.\n",
    "\n",
    "# UI参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841b32bc-4321-484a-878e-e50545944ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e966412-e426-460f-ab67-3b22c6f5d357",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe the benefits of using Feature Store to store and access features for machine learning pipelines.\n",
    "\n",
    "# It enables feature sharing and discovery across your organization and also ensures that the same feature computation code is used for model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b508b57-fac2-4eaf-8288-b82a308ad8d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a feature store table.\n",
    "import pyspark.sql.functions as F\n",
    "import uuid\n",
    "from databricks import feature_store\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from databricks.feature_store import feature_table, FeatureLookup\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path).coalesce(1).withColumn(\"index\", F.monotonically_increasing_id())\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DA.cleaned_username}\")\n",
    "table_name = f\"{DA.cleaned_username}.airbnb_\" + str(uuid.uuid4())[:6]\n",
    "print(table_name)\n",
    "\n",
    "fs = feature_store.FeatureStoreClient()\n",
    "\n",
    "## select numeric features and exclude target column \"price\"\n",
    "numeric_cols = [x.name for x in airbnb_df.schema.fields if (x.dataType == DoubleType()) and (x.name != \"price\")]\n",
    "numeric_features_df = airbnb_df.select([\"index\"] + numeric_cols)\n",
    "\n",
    "# create fs table and insert records\n",
    "fs.create_table(\n",
    "    name=table_name,\n",
    "    primary_keys=[\"index\"],\n",
    "    df=numeric_features_df,\n",
    "    schema=numeric_features_df.schema,\n",
    "    description=\"Numeric features of airbnb data\"\n",
    ")\n",
    "\n",
    "# # create fs table\n",
    "# fs.create_table(\n",
    "#     name=table_name,\n",
    "#     # 主键必须\n",
    "#     primary_keys=[\"index\"],\n",
    "#     schema=numeric_features_df.schema,\n",
    "#     description=\"Original Airbnb data\"\n",
    "# )\n",
    "\n",
    "# # insert records later\n",
    "# fs.write_table(\n",
    "#     name=table_name,\n",
    "#     df=numeric_features_df,\n",
    "#     mode=\"overwrite\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab989a4-f842-4d91-b0ae-24519ec1da5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write data to a feature store table.\n",
    "\n",
    "# overwrite\n",
    "\n",
    "df_new_feature = numeric_features_df\\\n",
    "  .filter(F.col('index')< 100)\\\n",
    "  .withColumn('new_feature', F.lit(999))\n",
    "\n",
    "# fs.write_table(\n",
    "#     name=table_name,\n",
    "#     df=df_new_feature,\n",
    "#     mode=\"overwrite\"\n",
    "# )\n",
    "\n",
    "# fs.write_table(\n",
    "#     name=table_name,\n",
    "#     df=df_new_feature,\n",
    "#     mode=\"merge\"\n",
    "# )\n",
    "\n",
    "# get_table()とread_table()の違いは押さえておく\n",
    "\n",
    "feature_table_df = fs.read_table(table_name)\n",
    "display(feature_table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dc1486-60dc-421a-b25f-19d47b378263",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train a model with features from a feature store table.\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    rf = RandomForestRegressor(max_depth=3, n_estimators=20, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    mlflow.log_metric(\"test_mse\", mean_squared_error(y_test, y_pred))\n",
    "    mlflow.log_metric(\"test_r2_score\", r2_score(y_test, y_pred))\n",
    "\n",
    "    # loggingにfsモジュールを使う\n",
    "    fs.log_model(\n",
    "        model=rf,\n",
    "        artifact_path=\"feature-store-model\",\n",
    "        flavor=mlflow.sklearn,\n",
    "        training_set=training_set,\n",
    "        registered_model_name=f\"feature_store_airbnb_{DA.cleaned_username}\",\n",
    "        input_example=X_train[:5],\n",
    "        signature=infer_signature(X_train, y_train)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1daa5aa6-a12b-4092-9861-3c747ef83ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Score a model using features from a feature store table.\n",
    "batch_input_df = inference_data_df.drop(\"price\") # Exclude true label\n",
    "predictions_df = fs.score_batch(f\"models:/feature_store_airbnb_{DA.cleaned_username}/1\", \n",
    "                                  batch_input_df, result_type=\"double\")\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97625b33-0f26-457c-8685-552c9b509c35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Managed MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad70f3ff-2b69-4b9b-a1dc-6b2ba6891483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify the best run using the MLflow Client API.\n",
    "\n",
    "# 単純にexperimentを指定してすべてのrunを取得し、メトリックを昇順(決定係数とかであれば降順)にソートして先頭のrunを取得すればOK\n",
    "# experiment包括很多run\n",
    "run_id_best = mlflow.search_runs(\n",
    "            summary.experiment.experiment_id,\n",
    "            order_by = [\"metrics.val_rmse\"]\n",
    "            )[\"run_id\"][0]\n",
    "\n",
    "model_uri = f'runs:/{run_id_best}/model'\n",
    "\n",
    "# PyFuncModelとしてモデルをロード\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54fb10c0-636c-4992-860a-8b603ef5fce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Manually log metrics, artifacts, and models in an MLflow Run.\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.sql.functions import col, log, exp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with mlflow.start_run(run_name=\"LR-Log-Price\") as run:\n",
    "    # Take log of price\n",
    "    log_train_df = train_df.withColumn(\"log_price\", log(col(\"price\")))\n",
    "    log_test_df = test_df.withColumn(\"log_price\", log(col(\"price\")))\n",
    "\n",
    "    # Log parameter\n",
    "    mlflow.log_param(\"label\", \"log_price\")\n",
    "    mlflow.log_param(\"features\", \"all_features\")\n",
    "\n",
    "    # Create pipeline\n",
    "    r_formula = RFormula(\n",
    "        formula=\"log_price ~ . - price\",\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"log_price\",\n",
    "        handleInvalid=\"skip\",\n",
    "    )\n",
    "    lr = LinearRegression(labelCol=\"log_price\", predictionCol=\"log_prediction\")\n",
    "    pipeline = Pipeline(stages=[r_formula, lr])\n",
    "    pipeline_model = pipeline.fit(log_train_df)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.spark.log_model(\n",
    "        pipeline_model, \"log-model\", input_example=log_train_df.limit(5).toPandas()\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    pred_df = pipeline_model.transform(log_test_df)\n",
    "    exp_df = pred_df.withColumn(\"prediction\", exp(col(\"log_prediction\")))\n",
    "\n",
    "    # Evaluate predictions\n",
    "    rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(exp_df)\n",
    "    r2 = regression_evaluator.setMetricName(\"r2\").evaluate(exp_df)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Log artifact\n",
    "    plt.clf()\n",
    "\n",
    "    log_train_df.toPandas().hist(column=\"log_price\", bins=100)\n",
    "    fig = plt.gcf()\n",
    "    mlflow.log_figure(fig, f\"{DA.username}_log_normal.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad33ee5-a18e-4d09-8133-649b9df46931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a nested Run for deeper Tracking organization.\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_model(df_pandas: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Trains an sklearn model on grouped instances\n",
    "    \"\"\"\n",
    "    # Pull metadata\n",
    "    device_id = df_pandas[\"device_id\"].iloc[0]\n",
    "    n_used = df_pandas.shape[0]\n",
    "    run_id = df_pandas[\"run_id\"].iloc[0] # Pulls run ID to do a nested run\n",
    "\n",
    "    # Train the model\n",
    "    X = df_pandas[[\"feature_1\", \"feature_2\", \"feature_3\"]]\n",
    "    y = df_pandas[\"label\"]\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = rf.predict(X)\n",
    "    mse = mean_squared_error(y, predictions) # Note we could add a train/test split\n",
    "\n",
    "    # Resume the top-level training\n",
    "    with mlflow.start_run(run_id=run_id) as outer_run:\n",
    "        # Small hack for running as a job\n",
    "        experiment_id = outer_run.info.experiment_id\n",
    "        print(f\"Current experiment_id = {experiment_id}\")\n",
    "\n",
    "        # Create a nested run for the specific device\n",
    "        with mlflow.start_run(run_name=str(device_id), nested=True, experiment_id=experiment_id) as run:\n",
    "            mlflow.sklearn.log_model(rf, str(device_id))\n",
    "            mlflow.log_metric(\"mse\", mse)\n",
    "            mlflow.set_tag(\"device\", str(device_id))\n",
    "\n",
    "            artifact_uri = f\"runs:/{run.info.run_id}/{device_id}\"\n",
    "            # Create a return pandas DataFrame that matches the schema above\n",
    "            return_df = pd.DataFrame([[device_id, n_used, artifact_uri, mse]], \n",
    "                                    columns=[\"device_id\", \"n_used\", \"model_path\", \"mse\"])\n",
    "\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7411ca-512f-482e-b68d-e6afaf169613",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Locate the time a run was executed in the MLflow UI.\n",
    "\n",
    "# GUI参照\n",
    "\n",
    "# Notebookの場合\n",
    "import mlflow\n",
    "\n",
    "exp_id = ''\n",
    "runs = mlflow.search_runs(exp_id)\n",
    "df_runs = spark.read.format(\"mlflow-experiment\").load(exp_id)\n",
    "display(df_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357b9c4a-66c7-4dcd-918c-df3b8d995905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Locate the code that was executed with a run in the MLflow UI\n",
    "\n",
    "# GUI参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066f0f04-1877-43d1-85f9-b8721b51f4cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register a model using the MLflow Client API.\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "model_name = f\"{DA.cleaned_username}_review\"\n",
    "model_uri = f\"runs:/{run_id_best}/model\"\n",
    "\n",
    "model_details = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "\n",
    "# optional\n",
    "client.update_registered_model(\n",
    "    name=model_details.name,\n",
    "    description=\"This model forecasts Airbnb housing list prices based on various listing inputs.\"\n",
    ")\n",
    "\n",
    "client.update_model_version(\n",
    "    name=model_details.name,\n",
    "    version=model_details.version,\n",
    "    description=\"This model version was built using OLS linear regression with sklearn.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de13553-b1e6-43dc-ba17-31f81e311160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transition a model’s stage using the Model Registry UI page.\n",
    "\n",
    "# UI参照\n",
    "client.search_model_versions(f\"name = '{model_name}'\")[0].current_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6636434-e7f6-4e0c-8f48-a2927ab97483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transition a model’s stage using the MLflow Client API.\n",
    "client.transition_model_version_stage(\n",
    "    name=model_details.name,\n",
    "    version=model_details.version,\n",
    "    stage=\"Production\"\n",
    ")\n",
    "client.search_model_versions(f\"name = '{model_name}'\")[0].current_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306f1e1c-8d33-48b5-9637-a5c1401bc7a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Request to transition a model’s stage using the ML Registry UI page.\n",
    "\n",
    "# UI参照"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_Databricks Machine Learning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
