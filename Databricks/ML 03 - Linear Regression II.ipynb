{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c645af3-378e-4e5b-bed4-264e1fd034ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e52ef25-bb24-45c4-a4b8-04d0b37a4e62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"60a5d18a-6438-4ee3-9097-5145dc31d938\"/>\n",
    "\n",
    "# 線形回帰モデルの改善 (Linear Regression: Improving our model)\n",
    "\n",
    "このノートブックでは、モデルに特徴量を追加します。また、カテゴリ型特徴量の扱い方について説明します。\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)このレッスンで次を行います: <br>\n",
    " - カテゴリカル変数のOne Hot Encode\n",
    " - Pipeline APIの使用\n",
    " - モデルの保存と読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28243df-f206-4bc4-8351-ba86b1264c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571a92ff-796a-47ae-87fb-3419445b78ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c16fc7dc-0077-46db-be00-ebf3a4f096e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"f8b3c675-f8ce-4339-865e-9c64f05291a6\"/>\n",
    "\n",
    "## トレーニングデータ  / テストデータの分割 (Train/Test Split)\n",
    "\n",
    "前のノートブックと同様に80/20分割をします。フェアな比較ができるように同じシード値を使用して行います（※クラスタの設定を変更しない限り！）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210cb944-7266-40d1-bfdf-ae91a95ca79f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35b34cb-e996-44b3-b227-39a8367632fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"09003d63-70c1-4fb7-a4b7-306101a88ae3\"/>\n",
    "\n",
    "## カテゴリカル変数 (Categorical Variables)\n",
    "\n",
    "カテゴリ型特徴量を扱うにはいくつかの方法があります。\n",
    "* 数値の値の割当 \n",
    "* ダミー変数の作成（One Hot Encodingとも呼ばれる） \n",
    "* Embeddingの作成（主にテキストデータで利用される）\n",
    "\n",
    "### One Hot Encoder\n",
    "カテゴリ型変数にOne Hot Encode（OHE）を適用します。Sparkには **`dummies`** 関数はありません。OHEを2ステップで行います。まず <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html?highlight=stringindexer#pyspark.ml.feature.StringIndexer\" target=\"_blank\">StringIndexer</a> を使って、文字列型のカラムのラベル値を、MLで使うカラムのラベルインデックス値に対応付ける必要があります。\n",
    "\n",
    "次に、StringIndexerの出力に <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html?highlight=onehotencoder#pyspark.ml.feature.OneHotEncoder\" target=\"_blank\">OneHotEncoder</a> を適用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e08d3334-013c-4898-81b5-272f827d16a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d079ec3f-9108-4e08-9767-f04260cf70e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"dedd7980-1c27-4f35-9d94-b0f1a1f92839\"/>\n",
    "\n",
    "## Vector Assembler\n",
    "\n",
    "OHEされたカテゴリ型特徴量と数値の特徴量を組み合わせることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b533df-b2c3-459c-a2fe-4ec8a0054ec0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44689fff-1b1f-4d54-aca4-b25fe7ab4dec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"fb06fb9b-5dac-46df-aff3-ddee6dc88125\"/>\n",
    "\n",
    "## 線形回帰 (Linear Regression)\n",
    "\n",
    "全ての特徴量が揃ったので、線形回帰モデルを構築してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1fcc46-89ab-4b28-8107-efae441dbeab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "939a963e-9edb-4537-b7db-4a7bbedca6b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"a7aabdd1-b384-45fc-bff2-f385cc7fe4ac\"/>\n",
    "\n",
    "## パイプライン (Pipeline)\n",
    "\n",
    "これら全てのステージをパイプラインにまとめてみましょう。 <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html?highlight=pipeline#pyspark.ml.Pipeline\" target=\"_blank\">パイプライン</a> は、変換器(tarnsformer)と推定器(estimator)をまとめて実行する方法です。\n",
    "\n",
    "処理内容をパイプラインにまとめておくことで、テストデータの処理にも同じパイプラインを再利用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63198293-8cb1-4739-b7ac-05f3d0a1ce34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = [string_indexer, ohe_encoder, vec_assembler, lr]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f03020-b82a-401d-a615-cb4d171175a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"c7420125-24be-464f-b609-1bb4e765d4ff\"/>\n",
    "\n",
    "## モデルの保存 (Saving Models)\n",
    "\n",
    "クラスタがダウンした場合に備えて、モデルを永続的なストレージ（DBFSなど）に保存しておけば、結果を再計算する必要がありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c85cf2-26f0-4751-ba89-0c9fddef6e36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save(DA.paths.working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fadd317-5dcd-4ccb-ad79-c9fffa0f648b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"15f4623d-d99a-42d6-bee8-d7c4f79fdecb\"/>\n",
    "\n",
    "## モデルのロード (Loading models)\n",
    "\n",
    "モデルをロードする際、モデルの種類（線形回帰モデルだったのか、ロジスティック回帰モデルだったのか）を知る必要があります。\n",
    "\n",
    "このため、変換器(transformer)や推定器(estimator)を常にパイプラインに配置し、汎用的なPipelineModelをロードすることをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c27c31-427a-4a14-99a5-e96ee982da1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "saved_pipeline_model = PipelineModel.load(DA.paths.working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8891b778-ee06-45e2-8c79-025605fdd172",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"1303ef7d-1a57-4573-8afe-561f7730eb33\"/>\n",
    "\n",
    "## テストデータへのモデルの適用 (Apply model to test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f94899eb-6ff7-4d89-aed1-0de28f0311b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred_df = saved_pipeline_model.transform(test_df)\n",
    "\n",
    "display(pred_df.select(\"features\", \"price\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265885a1-c34c-4382-9aff-5cb485711e48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"9497f680-1c61-4bf1-8ab4-e36af502268d\"/>\n",
    "\n",
    "## モデルの評価 (Evaluate model)\n",
    "\n",
    "![](https://files.training.databricks.com/images/r2d2.jpg) R2の結果はどうですか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29814721-110a-4b32-b9e5-3c2ed559bc92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "rmse = regression_evaluator.evaluate(pred_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d322b100-2386-4842-bc5a-3e55146bd15a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"cc0618e0-59d9-4a6d-bb90-a7945da1457e\"/>\n",
    "\n",
    "見た通り、OHEを行わないモデルと比較して、RMSEが低くなったし、R2が高くなりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da994311-69c9-4780-92f2-1661f9b27b24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 03 - Linear Regression II",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
