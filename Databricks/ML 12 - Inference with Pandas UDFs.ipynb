{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15a0431a-bc1a-477f-af7b-6c787d7cac3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55832f8b-902d-4172-a2c8-564d19512285",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"2b5dc285-0d50-4ea7-a71b-8a7aa355ad7c\"/>\n",
    "\n",
    "# Pandas UDFを使った推論 (Inference with Pandas UDFs)\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) このレッスンでは次を行います:<br>\n",
    "- scikit-learnのモデルを構築し、MLflowで追跡そしてPandas Scalar Iterator UDFsと **`mapInPandas()`** を使って大規模に適用します。\n",
    "\n",
    "Pandas UDFについて詳しく知りたい方は、こちらの<a href=\"https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html\" target=\"_blank\">ブログ記事</a>でSpark 3.0の新機能を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b5ce13-7689-4a39-aeb4-5cb17d004402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2e44d7b-6dae-49bd-bd2e-4b19fb2cd2e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"8b52bca0-45f0-4ada-be31-c2c473fb8e77\"/>\n",
    "\n",
    "sklearnのモデルを学習し、MLflowで記録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "872b5dc9-9f8e-480d-85c7-9f6e37d86bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with mlflow.start_run(run_name=\"sklearn-random-forest\") as run:\n",
    "    # Enable autologging \n",
    "    mlflow.sklearn.autolog(log_input_examples=True, log_model_signatures=True, log_models=True)\n",
    "    # Import the data\n",
    "    df = pd.read_csv(f\"{DA.paths.datasets}/airbnb/sf-listings/airbnb-cleaned-mlflow.csv\".replace(\"dbfs:/\", \"/dbfs/\")).drop([\"zipcode\"], axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n",
    "\n",
    "    # Create model\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c447669-6b48-48ff-96c1-4edc44987e58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"7ebcaaf9-c6f5-4c92-865a-c7f2c7afb555\"/>\n",
    "\n",
    "Spark DataFrameの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4f7011-0632-4d56-96f4-eca01d8fcedc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2be33287-fd9b-42e8-8147-7d418091df92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"1cdc4475-f55f-4126-9d38-dedb19577f4e\"/>\n",
    "\n",
    "### Pandas/ベクトル化されたUDF (Pandas/Vectorized UDFs)\n",
    "\n",
    "Spark 2.3からは、Pythonで利用できるPandas UDFがあり、UDFの効率を向上させることができます。PandasのUDFは、Apache Arrowを利用して計算を高速化します。それが処理時間の改善にどう役立つかを見てみましょう。\n",
    "\n",
    "* <a href=\"https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\" target=\"_blank\">ブログ記事</a>\n",
    "* <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html#pyspark-usage-guide-for-pandas-with-apache-arrow\" target=\"_blank\">ドキュメンテーション</a>\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2017/10/image1-4.png\" alt=\"Benchmark\" width =\"500\" height=\"1500\">\n",
    "\n",
    "ユーザー定義関数が実行されます。\n",
    "* <a href=\"https://arrow.apache.org/\" target=\"_blank\">Apache Arrow</a>は、Sparkで使用されてJVM と Python プロセス間のデータをほぼゼロの（デ）シリアライズコストで効率的に転送するためのインメモリ列型データ形式です。詳しくは<a href=\"https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html\" target=\"_blank\">こちら</a>をご覧ください。\n",
    "* pandasのインスタンスおよびAPIと連携するため、関数内部でpandasを使用します。\n",
    "\n",
    "**注**:Spark 3.0では、Pythonのタイプヒントを使用してPandas UDFを定義する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85fd93aa-26dd-43cd-b584-617d951bd35e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def predict(*args: pd.Series) -> pd.Series:\n",
    "    model_path = f\"runs:/{run.info.run_id}/model\" \n",
    "    model = mlflow.sklearn.load_model(model_path) # Load model\n",
    "    pdf = pd.concat(args, axis=1)\n",
    "    return pd.Series(model.predict(pdf))\n",
    "\n",
    "prediction_df = spark_df.withColumn(\"prediction\", predict(*spark_df.columns))\n",
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f0334e2-f40b-41db-87df-483c45119827",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"e97526c6-ef40-4d55-9763-ee3ebe846096\"/>\n",
    "\n",
    "### Pandas Scalar Iterator UDF\n",
    "\n",
    "モデルが非常に大きい場合、同じPythonワーカープロセスでバッチごとに同じモデルを繰り返しロードすることは、Pandas UDFにとって高いオーバーヘッドとなります。Spark 3.0では、Pandas UDFはpandas.Seriesまたはpandas.DataFrameのiteratorを受け取ることができるので、iterator内のシリーズごとにモデルを読み込むのではなく、一度だけモデルを読み込むことで済みます。\n",
    "\n",
    "そうすれば、必要なセットアップのコストが発生する回数も少なくなります。扱うレコード数が **`spark.conf.get('spark.sql.execution.arrow.maxRecordsPerBatch')`** (デフォルトは 10,000) より多い場合、pandas scalar UDFはpd.Seriesのバッチを反復処理するので、スピードアップが見られるはずです。\n",
    "\n",
    "一般的な構文：\n",
    "\n",
    "```\n",
    "@pandas_udf(...)\n",
    "def predict(iterator):\n",
    "    model = ... # load model\n",
    "    for features in iterator:\n",
    "        yield model.predict(features)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe7155d-e8cf-4cc9-b200-c3aa4b43ed24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple\n",
    "\n",
    "@pandas_udf(\"double\")\n",
    "def predict(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.Series]:\n",
    "    model_path = f\"runs:/{run.info.run_id}/model\" \n",
    "    model = mlflow.sklearn.load_model(model_path) # Load model\n",
    "    for features in iterator:\n",
    "        pdf = pd.concat(features, axis=1)\n",
    "        yield pd.Series(model.predict(pdf))\n",
    "\n",
    "prediction_df = spark_df.withColumn(\"prediction\", predict(*spark_df.columns))\n",
    "display(prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc58d26e-7bd8-472f-90a7-6a6eaccf6a2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"23b8296e-e0bc-481e-bd35-4048d532c71d\"/>\n",
    "\n",
    "### Pandas Function API\n",
    "\n",
    "Pandas UDFを使う代わりに、Pandas Function APIを使うことができます。Apache Spark 3.0のこの新しい機能では、PySpark DataFrameに対してPandasインスタンスを取得・出力するPythonネイティブ関数を直接適用することができるようになりました。Apache Spark 3.0でサポートされるPandas Functions APIは、grouped map、mapとco-grouped mapです。\n",
    "\n",
    "**`mapInPandas()`** は pandas.DataFrame のiteratorを入力とし、別の pandas.DataFrame のiteratorを出力する。モデルが入力として全てのカラムを必要とする場合、柔軟で使いやすいですが、DataFrame全体のシリアライズ/デシリアライズが必要です（入力として渡されるため）。iteratorが出力する各pandas.DataFrameのバッチサイズは、 **`spark.sql.execution.arrow.maxRecordsPerBatch`** の設定により制御できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0c6a99-c590-4438-be97-82223e0a67dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    model_path = f\"runs:/{run.info.run_id}/model\" \n",
    "    model = mlflow.sklearn.load_model(model_path) # Load model\n",
    "    for features in iterator:\n",
    "        yield pd.concat([features, pd.Series(model.predict(features), name=\"prediction\")], axis=1)\n",
    "    \n",
    "display(spark_df.mapInPandas(predict, \"\"\"`host_total_listings_count` DOUBLE,`neighbourhood_cleansed` BIGINT,`latitude` DOUBLE,`longitude` DOUBLE,`property_type` BIGINT,`room_type` BIGINT,`accommodates` DOUBLE,`bathrooms` DOUBLE,`bedrooms` DOUBLE,`beds` DOUBLE,`bed_type` BIGINT,`minimum_nights` DOUBLE,`number_of_reviews` DOUBLE,`review_scores_rating` DOUBLE,`review_scores_accuracy` DOUBLE,`review_scores_cleanliness` DOUBLE,`review_scores_checkin` DOUBLE,`review_scores_communication` DOUBLE,`review_scores_location` DOUBLE,`review_scores_value` DOUBLE, `prediction` DOUBLE\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99f28dd0-64cc-4f68-8a54-d3d638091e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"d13b87a7-0625-4acc-88dc-438cf06e18bd\"/>\n",
    "\n",
    "あるいは、以下のようなスキーマを定義することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c19aa9-e512-4865-b43b-0e635028b1a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "schema = spark_df.withColumn(\"prediction\", lit(None).cast(DoubleType())).schema\n",
    "display(spark_df.mapInPandas(predict, schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a1d4632-c797-4d79-a19e-3148c25a9294",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 12 - Inference with Pandas UDFs",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
