{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f568c9f-09bb-4d7b-a1dc-17d957c28795",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d21037-a890-4482-ad48-192920ce2ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"263caa08-bb08-4022-8d8f-bd2f51d77752\"/>\n",
    "\n",
    "# 分類：(Classification:)ロジスティック回帰\n",
    "\n",
    "ここまでは、回帰のユースケースのみを検証してきました。では、分類の扱い方を見てみましょう。\n",
    "\n",
    "このラボでは、同じAirbnbのデータセットを使用しますが、価格を予測する代わりに、ホストが<a href=\"https://www.airbnb.com/superhost\" target=\"_blank\">スーパーホスト</a>であるかどうかを予測します。\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) このレッスンでは以下を行います。<br>\n",
    " - ロジスティック回帰モデルを構築\n",
    " - モデルの性能を評価するための様々なメトリックスを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1977ec9-1500-44cb-ae96-96045adc1381",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa5f929f-2bf3-454d-b962-bcae2337c57a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eee1170-ee60-45ce-ab68-bc210dc8b8a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"3f07e772-c15d-46e4-8acd-866b661fbb9b\"/>\n",
    "\n",
    "## ベースラインモデル (Baseline Model)\n",
    "\n",
    "機械学習モデルを構築する前に、比較するためのベースラインモデルを構築します。まず、ホストが <a href=\"https://www.airbnb.com/superhost\" target=\"_blank\">superhost</a> であるかどうかを予測することから始めます。\n",
    "\n",
    "ベースラインモデルでは、誰もスーパーホストではないことを予測し、その精度を評価することにしています。他の指標については、後ほどより複雑なモデルを構築する際に検討する予定です。\n",
    "\n",
    "0. **`host_is_superhost`** カラム (t/f) を 1/0 に変換し、結果のカラムを **`label`** と呼びます。その後、 **`host_is_superhost`** を DROPします。\n",
    "0. 結果のDataFrameに、全ての値を **`0.0`** にした **`prediction`** カラムを追加します。誰もスーパーホストではないことを常に予測するようにします。\n",
    "\n",
    "この2つのステップを終えたら、次に「モデル」の精度を評価します。\n",
    "\n",
    "便利な機能がいくつかあります。\n",
    "* <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.when.html\" target=\"_blank\">when()</a>\n",
    "* <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html\" target=\"_blank\">withColumn()</a>\n",
    "* <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lit.html\" target=\"_blank\">lit()</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17d9adce-27b8-49fc-8b0a-4b504384088d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "label_df = airbnb_df.select(when(col(\"host_is_superhost\") == \"t\", 1.0).otherwise(0.0).alias(\"label\"), \"*\").drop(\"host_is_superhost\")\n",
    "\n",
    "pred_df = label_df.withColumn(\"prediction\", lit(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99fa0bf7-5fd5-4214-9f57-f8b2aa539dbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"d04eb817-2010-4021-a898-42ca8abaa00d\"/>\n",
    "\n",
    "## モデル評価\n",
    "\n",
    "とりあえず、「accuracy」を指標にしましょう。<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html?highlight=multiclassclassificationevaluator#pyspark.ml.evaluation.MulticlassClassificationEvaluator\" target=\"_blank\">MulticlassClassificationEvaluator</a> を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcb1840d-388c-4461-8b2f-6121a5a88f65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "mc_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(f\"The accuracy is {100*mc_evaluator.evaluate(pred_df):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad70f6e9-d967-44ae-91fc-141ddd5e0f81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"5fe00f31-d186-4ab8-b6bb-437f7ddc4a00\"/>\n",
    "\n",
    "## Train-Test分割 (Train-Test Split)\n",
    "\n",
    "よしっ!これでベースラインモデルができました。次に、データを学習用とテスト用に分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e38efb7-aa23-463c-a054-2fa762627433",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = label_df.randomSplit([.8, .2], seed=42)\n",
    "print(train_df.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51e66f7a-3b54-480b-83c7-4ddac2cc35b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"a7998d44-af91-4dfa-b80c-8b96ebfe5311\"/>\n",
    "\n",
    "## 可視化 (Visualize)\n",
    "\n",
    "トレーニングデータセットにおける **`review_scores_rating`** と **`label`** の関係性を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab83ae88-1744-4bd8-8de8-332016eb3dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df.select(\"review_scores_rating\", \"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e613f79-4f9d-4ff4-828c-8af9589d07ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"1ce4ba05-f558-484d-a8e8-53bde1e119fc\"/>\n",
    "\n",
    "## ロジスティック回帰 (Logistic Regression)\n",
    "\n",
    "ここで、すべての特徴量を使用して、<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html?highlight=logisticregression#pyspark.ml.classification.LogisticRegression\" target=\"_blank\">ロジスティック回帰モデル</a>を構築します（ヒント：RFormulaを使用します）。前処理ステップとロジスティック回帰モデル構築ステップをPipelineに入れます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3301c321-27c5-4318-91fd-dedf066ee5bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "r_formula = RFormula(formula=\"label ~ .\", \n",
    "                    featuresCol=\"features\", \n",
    "                    labelCol=\"label\", \n",
    "                    handleInvalid=\"skip\") # Look at handleInvalid\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[r_formula, lr])\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "pred_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4bee055-ab47-410b-8c6a-ec11a3dc196f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"3a06d71c-8551-44c8-b33e-8ae40a443713\"/>\n",
    "\n",
    "## 評価 (Evaluate)\n",
    "\n",
    "AUROCは何に役に立つでしょうか？PR曲線下の面積のような評価指標を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033fbbaa-aefd-488e-90b7-336d8bd516d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "mc_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(f\"The accuracy is {100*mc_evaluator.evaluate(pred_df):.2f}%\")\n",
    "\n",
    "bc_evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "print(f\"The area under the ROC curve: {bc_evaluator.evaluate(pred_df):.2f}\")\n",
    "\n",
    "bc_evaluator.setMetricName(\"areaUnderPR\")\n",
    "print(f\"The area under the PR curve: {bc_evaluator.evaluate(pred_df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aea3e25-0329-4943-959d-27558f146471",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"0ef0e2b9-6ce9-4377-8587-83b5260fd05a\"/>\n",
    "\n",
    "## ハイパーパラメータチューニングの追加 (Add Hyperparameter Tuning)\n",
    "\n",
    "クロスバリデーターを用いて、ロジスティック回帰モデルのハイパーパラメーターを変更します。どの程度、指標を改善できますか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae823851-916f-4986-abe2-8e37831fd1c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "            .addGrid(lr.regParam, [0.1, 0.2])\n",
    "            .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "            .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr, evaluator=mc_evaluator, estimatorParamMaps=param_grid,\n",
    "                    numFolds=3, parallelism=4, seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[r_formula, cv])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "pred_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969f7bab-24f0-454d-83bb-c45169b6b1b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"111f2dc7-5535-45b7-82f6-ad2e5f2cbf16\"/>\n",
    "\n",
    "## 再評価 (Evaluate again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af008fa0-c213-4a52-96b3-fcce67456dee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mc_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(f\"The accuracy is {100*mc_evaluator.evaluate(pred_df):.2f}%\")\n",
    "\n",
    "bc_evaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "print(f\"The area under the ROC curve: {bc_evaluator.evaluate(pred_df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eda1e2d9-2cd3-48c8-977d-b3019b4566ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"7e88e044-0a34-4815-8eab-1dc37532a082\"/>\n",
    "\n",
    "## スーパーボーナス (Super Bonus)\n",
    "\n",
    "MLflowを使って実験を記録してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "233e3b94-d3da-4f00-ad6a-60a0acd64f40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "MLE 03 - Logistic Regression Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
