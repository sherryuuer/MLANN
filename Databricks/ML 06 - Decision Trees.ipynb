{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a15042e-adeb-4b0d-b1b4-1b8c5c914eee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f3d2dd4-0e2c-4ca5-81dc-86869a645d22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"3bdc2b9e-9f58-4cb7-8c55-22bade9f79df\"/>\n",
    "\n",
    "# 決定木 (Decision Trees)\n",
    "\n",
    "前回のノートでは、パラメトリックモデルであるLinear Regression（線形回帰）を使っていました。線形回帰モデルでもっとハイパーパラメータを調整することもできますが、今回は木構造の手法を試して、パフォーマンスが向上するかどうかを確認します。\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)このレッスンでは次を行います: <br>\n",
    " - シングルノードの決定木と分散型の決定木の実装の違いを確認する\n",
    " - 特徴量の重要度(importance)を取得する\n",
    " - 決定木のよくある落とし穴を検証する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1277f2-296b-48e2-a8f8-eb511bb5cd07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb13ed6-3fbc-421d-b4dc-fe092ddae64d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceaa528f-4b4e-4dc3-bf93-f723b16ba631",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"9af16c65-168c-4078-985d-c5f8991f171f\"/>\n",
    "\n",
    "## カテゴリ型特徴量をどう扱うか？ (How to Handle Categorical Features?)\n",
    "\n",
    "前回のノートブックで、StringIndexer/OneHotEncoder/VectorAssemblerやRFormulaが使えることを確認しました。\n",
    "\n",
    "**しかし決定木、特にランダムフォレストでは、変数のOne Hot Encodingをすべきではありません。**\n",
    "\n",
    "これについては、<a href=\"https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769#:~:text=One%2Dhot%20encoding%20categorical%20variables,importance%20resulting%20in%20poorer%20performance\" target=\"_blank\">ブログ</a> に詳しく説明があります。そのエッセンスは次です : \n",
    ">>> \"基数の多い(high cardinality)カテゴリ変数にOHEを適用すると、ツリーベースの手法では非効率になることがあります。アルゴリズムにより、連続変数がダミー変数よりも重要視されるようになるため、特徴量の重要度の順序が不明瞭になり、パフォーマンスが低下する可能性があります。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea849fc-451b-4a7c-9766-f98c2cc23505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "599617f8-0ebe-4399-acf9-3f1c8aea3411",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"35e2f231-2ebb-4889-bc55-089200dd1605\"/>\n",
    "\n",
    "## VectorAssembler\n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html?highlight=vectorassembler#pyspark.ml.feature.VectorAssembler\" target=\"_blank\">VectorAssembler</a> を使って、すべてのカテゴリ型および数値型の入力を結合してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15f2aa3-5410-41e9-91dc-ac6b74d90d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Filter for just numeric columns (and exclude price, our label)\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6663c79-b224-483d-aee7-406bf004062f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"2096f7aa-7fab-4807-b45f-fcbd0424a3e8\"/>\n",
    "\n",
    "## 決定木 (Decision Tree)\n",
    "\n",
    "では、デフォルトのハイパーパラメータで <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html?highlight=decisiontreeregressor#pyspark.ml.regression.DecisionTreeRegressor\" target=\"_blank\">DecisionTreeRegressor</a> を構築してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96a23ac-6ea6-475c-b786-b9aaacaf0e0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8519db8-48e3-449c-a8b3-5e89c32f2adf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"506ab7fa-0952-4c55-ad9b-afefb6469380\"/>\n",
    "\n",
    "## PipelineをFitする (Fit Pipeline)\n",
    "\n",
    "以下のセルはエラーになるはずです。後で修正します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2597c078-c95d-4c4a-ac0f-f3c29596ca7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Combine stages into pipeline\n",
    "stages = [string_indexer, vec_assembler, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Uncomment to perform fit\n",
    "# pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a663fc0e-e59f-4eab-abbd-5b08ef31e099",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"d0791ff8-8d79-4d32-937d-9fcfbac4e9bd\"/>\n",
    "\n",
    "## maxBins\n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.DecisionTreeRegressor.html?highlight=decisiontreeregressor#pyspark.ml.regression.DecisionTreeRegressor.maxBins\" target=\"_blank\">maxBins</a> は、どのようなパラメータでしょうか？ **`maxBins`** パラメータを説明するために、分散決定木の実装の1つであるPLANETの実装を見てみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df5f16d3-81b0-4689-84de-edbfeb5ea9e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"1f9c229e-6f8c-4174-9927-c284e64e5753\"/>\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/DistDecisionTrees.png\" height=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a3546c8-cbfb-4d50-97fe-5f746ac13d6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"3b7e60c3-22de-4794-9cd4-6713255b79a4\"/>\n",
    "\n",
    "Sparkでは、データは行で分割されます。そのため、分割を行う必要がある場合、各Workerは分割点ごとに各特徴の要約統計量を計算する必要があります。そして、分割するためにこれらの統計情報を（tree reduceによって）集約する必要があります。 \n",
    "\n",
    "考えてみてください。Worker1が値 **`32`** を持っているが、他のどのWorkerもその値を持っていなかったとしたらどうなるでしょうか。どれだけ良い分割になるのかどうやって分かりますか。そこで、Sparkには連続変数を離散化してバケットにするためのmaxBinsパラメータを使います。しかし、バケット数は最も基数の多いカテゴリ型変数と同じ大きさでなければなりません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d6e1c62-21ef-49db-908c-09489e897090",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"0552ed6a-120f-4e49-ae3a-5f92bd9f863d\"/>\n",
    "\n",
    "では、maxBinsを **`40`** に増やしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97cec708-1a1a-4c78-aa42-4495553f2757",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dt.setMaxBins(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "493f3ec1-4faf-4744-bd05-e3e1bce39bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"92252524-e388-439b-a92b-958cc332a861\"/>\n",
    "\n",
    "Pipelineをfitしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebd9eea-9251-45c9-8796-75b4616e14fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e6b580-86f9-46de-aa4d-5042e7523570",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"2426e78b-9bd2-4b7d-a65b-52054906e438\"/>\n",
    "\n",
    "## 特徴量の重要度 (Feature Importance)\n",
    "\n",
    "フィットした決定木モデルを取得し、特徴量の重要度を見ましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa8f96ea-2309-432a-9dcf-08ce62d1b23a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dt_model = pipeline_model.stages[-1]\n",
    "display(dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b56e2f3-c04d-4365-b615-5158c5c0112e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44b2ca0-1d41-4ff7-8761-e2cd2850f769",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"823c20ff-f20b-4853-beb0-4b324debb2e6\"/>\n",
    "\n",
    "## 特徴量の重要度の解釈 (Interpreting Feature Importance)\n",
    "\n",
    "うーん。feature 4, feature 11のような表記は分かり難いです。特徴量の重要度スコアは小さい値なので、Pandasを使って元の列名を復元できるようにしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51160ad6-45d7-4a6f-b5b6-5f547955fe3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features_df = pd.DataFrame(list(zip(vec_assembler.getInputCols(), dt_model.featureImportances)), columns=[\"feature\", \"importance\"])\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d66df854-2390-4372-b33e-a736c26611e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"1fe0f603-add5-4904-964b-7288ae98b2e8\"/>\n",
    "\n",
    "## なぜわずかな特徴量だけがnon-zeroなのか？ (Why so few features are non-zero?)\n",
    "\n",
    "SparkMLの場合、デフォルトの **`maxDepth`** は5なので、検討できる特徴量の数が限られています（同じ特徴量を異なる分割点で何度も分割することもあります）。\n",
    "\n",
    "Databricksのwidgetを使って、top-K個の特徴量を取得してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c555cf3-0ac9-4dcd-a4cf-b1381209ce3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"top_k\", \"5\")\n",
    "top_k = int(dbutils.widgets.get(\"top_k\"))\n",
    "\n",
    "top_features = features_df.sort_values([\"importance\"], ascending=False)[:top_k][\"feature\"].values\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8b1be9e-3e5b-4721-a4aa-28b53d73a94e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"d9525bf7-b871-45c8-b0f9-dca5fd7ae825\"/>\n",
    "\n",
    "## スケール不変性(Scale Invariant)\n",
    "\n",
    "決定木の場合、特徴量のスケールの大きさは問題にならない。例えば、分割点が100であっても、0.33に正規化されていても、データの1/3を分割する。重要なのは、その分割点から左右にいくつのデータが落ちるかだけで、分割点の絶対値ではありません。\n",
    "\n",
    "これは線形回帰には当てはまらず、Sparkのデフォルトでは最初に正規化することになっています。考えてみてください。靴のサイズをアメリカ式とヨーロッパ式で測ると、同じ足のサイズに合わせた靴であっても、その値は大きく異なります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5590ab3d-ec5e-4ae3-9c34-cc3c156b112a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"bad0dd6d-05ba-484b-90d6-cfe16a1bc11e\"/>\n",
    "\n",
    "## テストセットへのモデルの適用 (Apply model to test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0077b39-fb94-4452-9d7c-0c18842853a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred_df = pipeline_model.transform(test_df)\n",
    "\n",
    "display(pred_df.select(\"features\", \"price\", \"prediction\").orderBy(\"price\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90be273b-7d95-4149-95b5-570f7d21803c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"094553a3-10c0-4e08-9a58-f94430b4a512\"/>\n",
    "\n",
    "## 落とし穴 (Pitfall)\n",
    "\n",
    "Airbnbで巨大な物件を借りたらどうなるでしょう？20ベッドルームと20バスルームでした。このとき決定木は何を予測するのか？\n",
    "\n",
    "決定木は、学習に用いた値より大きな値を予測することができないです。トレーニングセットの最大値は1万ドルだったので、それ以上の値を予測することはできません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f892cab4-e9e0-4417-8255-79c2c52d7f56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "\n",
    "rmse = regression_evaluator.evaluate(pred_df)\n",
    "r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67b4c097-3263-4a87-9d81-06da9fd036e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"033a9c19-0f9d-4c33-aa5e-f58665637448\"/>\n",
    "\n",
    "## Uh oh!\n",
    "\n",
    "このモデルは線形回帰モデルよりもずっと悪いし、平均値を予測するよりも悪いです。\n",
    "\n",
    "次のノートブックでは、ハイパーパラメータのチューニングとアンサンブルモデルを使って、単一の決定木の性能を向上させる方法を説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb53525-c369-4e8c-b319-1907a3e53ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 06 - Decision Trees",
   "widgets": {
    "top_k": {
     "currentValue": "5",
     "nuid": "c2388835-96ce-4f2f-a19f-0f22f0ff3265",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "5",
      "label": null,
      "name": "top_k",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
