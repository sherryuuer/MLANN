{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0a7e113-5591-4aa5-bb1e-60a997525e5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a834c644-9864-486e-aebc-a92b402922ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"1fa7a9c8-3dad-454e-b7ac-555020a4bda8\"/>\n",
    "\n",
    "# Hyperopt\n",
    "\n",
    "Hyperoptは、「実数値、離散、条件付き次元を含む、厄介な探索空間上でのシリアルおよびパラレル最適化」のためのPythonライブラリです。\n",
    "\n",
    "機械学習ワークフローにおいて、hyperoptは、他のライブラリで利用可能なものより高度な最適化戦略を用いてハイパーパラメータ最適化プロセスを分散/並列化するために使用することができます。\n",
    "\n",
    "Apache Sparkでhyperoptをスケールさせるには、2つの方法があります。\n",
    "* シングルマシンのhyperoptで、分散学習アルゴリズム（MLlibなど）を使う \n",
    "* 分散hyperoptで、SparkTrialsクラスと一緒にシングルマシンの学習アルゴリズム（scikit-learnなど）を使う。 \n",
    "\n",
    "このレッスンでは、シングルマシンのhyperoptでMLlibを使用しますが、ラボでは、分散hyperoptでシングルノードモデルのハイパーパラメータチューニングを使用する方法を紹介します。 \n",
    "\n",
    "残念ながら現時点では、hyperoptを使用して分散型の学習アルゴリズムとともにハイパーパラメータ最適化を分散させることはできません。しかし、Spark MLを使ってより高度なハイパーパラメータ探索アルゴリズム（ランダム探索、TPEなど）を使用する利点があります。\n",
    "\n",
    "\n",
    "リソース\n",
    "\n",
    "0. <a href=\"http://hyperopt.github.io/hyperopt/scaleout/spark/\" target=\"_blank\">Documentation</a>\n",
    "0. <a href=\"https://docs.databricks.com/applications/machine-learning/automl/hyperopt/index.html\" target=\"_blank\">Hyperopt on Databricks</a>\n",
    "0. <a href=\"https://databricks.com/blog/2019/06/07/hyperparameter-tuning-with-mlflow-apache-spark-mllib-and-hyperopt.html\" target=\"_blank\">Hyperparameter Tuning with MLflow, Apache Spark MLlib and Hyperopt</a>\n",
    "0. <a href=\"https://databricks.com/blog/2021/04/15/how-not-to-tune-your-model-with-hyperopt.html\" target=\"_blank\">How (Not) to Tune Your Model With Hyperopt</a>\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)このレッスンで次を行います : <br>\n",
    " - TPEを使用してMLlibモデルの最適なパラメータを見つけるためにhyperoptを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839d2aab-c678-4f43-ba88-244e8246bccb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87507971-7dc6-4384-b296-97bf59e55af8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"2340cdf4-9753-41b4-a613-043b90f0f472\"/>\n",
    "\n",
    "まずはSF Airbnb Datasetをロードしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18475a04-a055-42f9-8f53-aaae283b12cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "train_df, val_df, test_df = airbnb_df.randomSplit([.6, .2, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73284187-dad2-47c5-9d15-896b565f5b17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"37bbd5bd-f330-4d02-8af6-1b185612cdf8\"/>\n",
    "\n",
    "その後、ランダムフォレストパイプラインと回帰のevaluatorを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97586c80-c391-487b-afd0-4e1f7ade3319",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)\n",
    "pipeline = Pipeline(stages=[string_indexer, vec_assembler, rf])\n",
    "regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "589c673c-2aa7-4a22-922b-4012da7a0f77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"e4627900-f2a5-4f65-881e-1374187dd4f9\"/>\n",
    "\n",
    "次に、ワークフローのhyperopt部分を作ります。\n",
    "\n",
    "まず、**目的関数**を定義します。目的関数は、主に2つの要件を持っています: \n",
    "\n",
    "1. **入力** **`params`** は、モデルの学習に使用するハイパーパラメータの値を含みます。\n",
    "2. **出力** は、最適化するための損失(loss)メトリックを含みます。\n",
    "\n",
    "ここでは **`max_depth`** と **`num_trees`** を指定し、損失指標としてRMSEを返すようにしています。\n",
    "\n",
    "指定したハイパーパラメータ値を使用するように、 **`RandomForestRegressor`** のパイプラインを再構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1691187-f3a4-440e-8ab0-8b7bf7650f6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective_function(params):    \n",
    "    # set the hyperparameters that we want to tune\n",
    "    max_depth = params[\"max_depth\"]\n",
    "    num_trees = params[\"num_trees\"]\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        estimator = pipeline.copy({rf.maxDepth: max_depth, rf.numTrees: num_trees})\n",
    "        model = estimator.fit(train_df)\n",
    "\n",
    "        preds = model.transform(val_df)\n",
    "        rmse = regression_evaluator.evaluate(preds)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "698ca584-edb8-4af6-8efb-10a6a920bdf1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"d4f9dd2b-060b-4eef-8164-442b2be242f4\"/>\n",
    "\n",
    "次に、ハイパーパラメータの探索空間を定義します。 \n",
    "\n",
    "これはグリッドサーチ処理におけるパラメータグリッドと同様です。ただし、テストする個々の具体的な値ではなく、値の範囲を指定します。実際の値を選択するのは、hyperoptの最適化アルゴリズムに任されています。\n",
    "\n",
    "検索空間を定義するのに役立つヒントについては <a href=\"https://github.com/hyperopt/hyperopt/wiki/FMin\" target=\"_blank\">ドキュメント</a> を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c28e692-0558-4ad8-a828-cb57c985d3f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "search_space = {\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 5, 1),\n",
    "    \"num_trees\": hp.quniform(\"num_trees\", 10, 100, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aea5f9d-b28f-4cbe-8e0b-9884f6bfe67e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"27891521-e481-4734-b21c-b2c5fe1f01fe\"/>\n",
    "\n",
    "**`fmin()`** が、 **`目的関数`** に使用する新しいハイパーパラメータの構成を生成します。 以下では最大4つのモデルを評価します(変数で指定)。その際に、前のモデルから得た情報を使って、次に試すべきハイパーパラメータを効果的に決定することができます。 \n",
    "\n",
    "Hyperoptでは、ランダムサーチまたはTree of Parzen Estimators（TPE）を用いて、ハイパーパラメータのチューニングを並行して行うことができます。以下のセルで、 **`tpe`** をインポートしていることに注意してください。.<a href=\"http://hyperopt.github.io/hyperopt/scaleout/spark/\" target=\"_blank\">ドキュメント</a> によると、TPEは以下のような適応的なアルゴリズムです。 \n",
    "\n",
    "> ハイパーパラメータ空間を繰り返し探索します。テストされる新しいハイパーパラメータ設定は、過去の結果に基づいて選択される。 \n",
    "\n",
    "このため **`tpe.suggest`** はベイジアンの探索方法です。\n",
    "\n",
    "MLflowはHyperoptと統合されているため、ハイパーパラメータのチューニングの一環として、学習させたすべてのモデルの結果とその結果を追跡することができます。このノートブックのMLflowの実験を追跡することができますが、このノートブック以外の実験を指定することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3224365-bf23-4aa2-b09e-40bfae7cc562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "mlflow.pyspark.ml.autolog(log_models=False)\n",
    "\n",
    "num_evals = 4\n",
    "trials = Trials()\n",
    "best_hyperparam = fmin(fn=objective_function, \n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest, \n",
    "                       max_evals=num_evals,\n",
    "                       trials=trials,\n",
    "                       rstate=np.random.default_rng(42))\n",
    "\n",
    "# Retrain model on train & validation dataset and evaluate on test dataset\n",
    "with mlflow.start_run():\n",
    "    best_max_depth = best_hyperparam[\"max_depth\"]\n",
    "    best_num_trees = best_hyperparam[\"num_trees\"]\n",
    "    estimator = pipeline.copy({rf.maxDepth: best_max_depth, rf.numTrees: best_num_trees})\n",
    "    combined_df = train_df.union(val_df) # Combine train & validation together\n",
    "\n",
    "    pipeline_model = estimator.fit(combined_df)\n",
    "    pred_df = pipeline_model.transform(test_df)\n",
    "    rmse = regression_evaluator.evaluate(pred_df)\n",
    "\n",
    "    # Log param and metrics for the final model\n",
    "    mlflow.log_param(\"maxDepth\", best_max_depth)\n",
    "    mlflow.log_param(\"numTrees\", best_num_trees)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.spark.log_model(pipeline_model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b52b9099-5d5f-4420-b2d7-68ae1f659438",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 08 - Hyperopt",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
