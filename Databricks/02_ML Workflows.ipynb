{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8e35ce-e501-4c7c-b151-071caaef9119",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Section 2: ML Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c657db7-9199-420d-a53e-8627a08834eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c44283d-4bab-4d47-b9f3-e2b41a54bf27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06.csv\"\n",
    "df = spark.read.csv(file_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "from pyspark.sql.functions import col, translate\n",
    "fixed_price_df = df.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08702c47-2188-49ac-9b51-69cea68ade41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49cbfa4-8870-41e2-af3f-99f2ffcdefbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute summary statistics on a Spark DataFrame using .summary()\n",
    "display(df.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e00e7fb-cd84-463d-b792-eb910905e77f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute summary statistics on a Spark DataFrame using dbutils data summaries.\n",
    "dbutils.data.summarize(fixed_price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbf1a43-06ba-44bf-8e57-9ffdccb0cff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove outliers from a Spark DataFrame that are beyond or less than a designated threshold.\n",
    "threshhold = 0\n",
    "display(fixed_price_df.filter(col(\"price\") > threshhold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d411c406-422f-4905-a62d-8175bd823527",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6449c476-d117-486a-ac89-37d4be7fc7c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify why it is important to add indicator variables for missing values that have been imputed or replaced.\n",
    "\n",
    "# If you do ANY imputation techniques for categorical/numerical features, you MUST include an additional field specifying that field was imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8217bc09-5262-4990-be50-d8e68f325b39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe when replacing missing values with the mode value is an appropriate way to handle missing values.\n",
    "\n",
    "# カテゴリ変数の場合\n",
    "\n",
    "# mean: 平均\n",
    "# median: 中央値\n",
    "# mode: 最頻値\n",
    "# constant: 定数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89da5a61-e0cd-4677-9203-ed61b99212d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare and contrast imputing missing values with the mean value or median value.\n",
    "\n",
    "display(fixed_price_df.select('price'))\n",
    "\n",
    "# 歪んでいる場合、外れ値がある場合に有効"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd077b68-0f11-41f4-99aa-f116f3d1810e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Impute missing values with the mean or median value.\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "integer_columns = [x.name for x in fixed_price_df.schema.fields if x.dataType == IntegerType()]\n",
    "doubles_df = fixed_price_df\n",
    "for c in integer_columns:\n",
    "    doubles_df = doubles_df.withColumn(c, col(c).cast(\"double\"))\n",
    "columns = \"\\n - \".join(integer_columns)\n",
    "\n",
    "impute_cols = [\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"beds\", \n",
    "    \"review_scores_rating\",\n",
    "    \"review_scores_accuracy\",\n",
    "    \"review_scores_cleanliness\",\n",
    "    \"review_scores_checkin\",\n",
    "    \"review_scores_communication\",\n",
    "    \"review_scores_location\",\n",
    "    \"review_scores_value\"\n",
    "]\n",
    "\n",
    "for c in impute_cols:\n",
    "    doubles_df = doubles_df.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))\n",
    "imputer = Imputer(strategy=\"median\", inputCols=impute_cols, outputCols=impute_cols) #estimator\n",
    "\n",
    "imputer_model = imputer.fit(doubles_df) #transformer\n",
    "imputed_df = imputer_model.transform(doubles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c17d94-3f8a-405f-b2d4-c696f33b1421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe the process of one-hot encoding categorical features.\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "df = spark.read.format(\"delta\").load(file_path)\n",
    "train_df, test_df = df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembler_inputs = ohe_output_cols + numeric_cols\n",
    "\n",
    "# step1: string indexer\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "# step2: ohe\n",
    "ohe_encoder = OneHotEncoder(inputCols=index_output_cols, outputCols=ohe_output_cols)\n",
    "\n",
    "# step3: vector assembler\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "# step4: transform one by one or wrap estimators in pipeline and fit\n",
    "stages = [string_indexer, ohe_encoder, vec_assembler]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78c1976d-5152-4480-a997-e2a4e8ca3bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe why one-hot encoding categorical features can be inefficient for tree-based models.\n",
    "\n",
    "# 基数の多い(high cardinality)カテゴリ変数にOHEを適用すると、ツリーベースの手法では非効率になることがあります。アルゴリズムにより、連続変数がダミー変数よりも重要視されるようになるため、特徴量の重要度の順序が不明瞭になり、パフォーマンスが低下する可能性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272f2847-6577-4a3a-a85f-37bdb848876e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b5a77e-3ace-4630-8073-d97e746c5b3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform random search as a method for tuning hyperparameters.\n",
    "\n",
    "# randome search\n",
    "# 指定したパラメータ範囲の組み合わせ(e.g. maxDepth:[2, 5, 10 ], numTrees:[5, 10])を指定した探索回数分ランダムに探索し、最も精度(評価指標)が高い組み合わせを採択する方法。\n",
    "# 違いはGridSearchCVではなく、RandomizedSearchCVを使うこと\n",
    "\n",
    "from sklearn import svm, datasets, linear_model\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "logistic = linear_model.LogisticRegression(solver='saga', tol=1e-2, max_iter=200, random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
    "clfrand = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "searchrand = clfrand.fit(iris.data, iris.target)\n",
    "\n",
    "\n",
    "# grid search\n",
    "# 指定したパラメータ範囲の組み合わせ(e.g. maxDepth:[2, 5, 10], numTrees:[5, 10])を網羅的に探索し、最も精度(評価指標)が高い組み合わせを採択する方法。\n",
    "iris = datasets.load_iris()\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "clfgs = GridSearchCV(svc, parameters)\n",
    "searchgs = clfgs.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ffd8ee2-66a8-4cfa-b240-59cc4b9866ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe the basics of Bayesian methods for tuning hyperparameters.\n",
    "\n",
    "# ベイズの定理を利用して、以前までに得た組み合わせの結果からある値が小さくなるような組み合わせを探索候補にして、最も精度(評価指標)が高い組み合わせを採択する方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e424c67-304d-4204-a137-c96800a56334",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe why parallelizing sequential/iterative models can be difficult.\n",
    "\n",
    "# Gradient boostingのアルゴリズムはiterative、弱学習器(小さいモデル)を作るときに以前のモデルの誤差を使用する\n",
    "# ので、処理の分散をさせるとノード間で誤差のやり取りが発生するので、難しい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19603dae-7bbd-4b11-ba14-dac69fc89b0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Understand the balance between compute resources and parallelization.\n",
    "\n",
    "# parallelismを設定する際、処理の速度と適応性(≒精度)がトレードオフになる点注意\n",
    "\n",
    "# https://hyperopt.github.io/hyperopt/scaleout/spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fe48fb-0684-466e-bf12-d68507d0bf8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parallelize the tuning of hyperparameters using Hyperopt and SparkTrials.\n",
    "\n",
    "# single-machine hyperopt with a distributed training algorithm (e.g. MLlib)\n",
    "# SparkMLのモデルでhyperoptを使う場合は以下の通り\n",
    "num_evals = 4\n",
    "trials = Trials()\n",
    "best_hyperparam = fmin(fn=objective_function, \n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest, \n",
    "                       max_evals=num_evals,\n",
    "                       trials=trials,\n",
    "                       rstate=np.random.default_rng(42))\n",
    "\n",
    "# distributed hyperopt with single-machine training algorithms (e.g. scikit-learn) with the SparkTrials class.\n",
    "# sklearnのモデルでhyperoptを使う場合は以下の通り\n",
    "num_evals = 4\n",
    "spark_trials = SparkTrials(parallelism=2)\n",
    "best_hyperparam = fmin(fn=objective_function, \n",
    "                       space=search_space,\n",
    "                       algo=tpe.suggest, \n",
    "                       trials=spark_trials,\n",
    "                       max_evals=num_evals,\n",
    "                       rstate=np.random.default_rng(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c0eddf-1f31-432c-b056-b768fa6a9209",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify the usage of SparkTrials as the tool that enables parallelization for tuning single-node models.\n",
    "\n",
    "# parallelism The maximum number of trials to evaluate concurrently. Greater parallelism allows scale-out testing of more hyperparameter settings. Defaults to Spark SparkContext.defaultParallelism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897b2b98-6f6e-45b1-b0ae-38dd04c118e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Evaluation and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b67911f-2871-4596-be11-4173780a147a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe cross-validation and the benefits of downsides of using cross-validation over a train-validation split.\n",
    "\n",
    "# n-fold クロスバリデーションでは、n-1/nのデータで学習し、残りの1/nのデータ（ホールドアウトセット）で評価します。このプロセスをn回繰り返し、各foldが検証セットとして利用されます。そして、n回の結果の平均を取ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef1d8c3-b6a8-4372-8520-f3168a37dcdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform cross-validation as a part of model fitting.\n",
    "\n",
    "# cvにpipelineを含める場合\n",
    "# pros: データ漏洩の可能性が低い\n",
    "# cons: string indexerのようなestimator/transformerがある場合、foldのdatasetに対して毎回変換をかけることになる\n",
    "stages = [string_indexer, vec_assembler, rf]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=param_grid, \n",
    "                    numFolds=3, seed=42)\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# pipelineにcvを含める場合\n",
    "# pros: 変換後にfoldのdatasetに分割するため、処理速度向上が見込める\n",
    "# cons: データ漏洩の可能性がある\n",
    "cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=param_grid, \n",
    "                    numFolds=3, seed=42)\n",
    "stages_with_cv = [string_indexer, vec_assembler, cv]\n",
    "pipeline = Pipeline(stages=stages_with_cv)\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b91966-fe51-4e03-b3c3-c13818cf8e72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify the number of models being trained in conjunction with a grid-search and cross-validation process.\n",
    "\n",
    "# パラメータの組み合わせ×foldの数\n",
    "# 以下の場合だと、(2*2) * 3 = 12回\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(rf.maxDepth, [2, 5])\n",
    "              .addGrid(rf.numTrees, [5, 10])\n",
    "              .build())\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=param_grid, \n",
    "                    numFolds=3, seed=42)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00a3450-2a43-4b18-8be1-d65cce0c5903",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe Recall and F1 as evaluation metrics.\n",
    "\n",
    "# PPT参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b17975ba-f6cf-43a1-91bc-6de11c7dffbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify the need to exponentiate the RMSE when the log of the label variable is used.\n",
    "\n",
    "# 目的変数の分布が歪んでいるときに、logをとって正規分布に近づけることでモデルの精度が向上する場合がある\n",
    "# RMSEはrootをとって単位を合わせるので、正しくRMSEを解釈するために、logではなく、実数に戻す必要あり。そのときにexponentiateする\n",
    "\n",
    "log_train_df = train_df.withColumn(\"log_price\", log(col(\"price\"))) #学習データ\n",
    "log_test_df = test_df.withColumn(\"log_price\", log(col(\"price\"))) #テストデータ\n",
    "\n",
    "r_formula = RFormula(formula=\"log_price ~ . - price\", featuresCol=\"features\", labelCol=\"log_price\", handleInvalid=\"skip\") \n",
    "\n",
    "lr.setLabelCol(\"log_price\").setPredictionCol(\"log_pred\")\n",
    "pipeline = Pipeline(stages=[r_formula, lr])\n",
    "pipeline_model = pipeline.fit(log_train_df)\n",
    "pred_df = pipeline_model.transform(log_test_df)\n",
    "\n",
    "#exponentiateしない場合\n",
    "exp_df_noexp = pred_df.withColumn(\"prediction\", col(\"log_pred\"))\n",
    "\n",
    "regression_evaluator_noexp = RegressionEvaluator(labelCol=\"log_price\", predictionCol=\"prediction\")\n",
    "rmse_noexp = regression_evaluator.setMetricName(\"rmse\").evaluate(exp_df_noexp)\n",
    "print(f\"RMSE is {rmse_noexp}\")\n",
    "\n",
    "#exponentiateする場合\n",
    "exp_df = pred_df.withColumn(\"prediction\", exp(col(\"log_pred\")))\n",
    "\n",
    "rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(exp_df)\n",
    "print(f\"RMSE is {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_ML Workflows",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
