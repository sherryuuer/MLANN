{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c630bc2-1f4a-4f3a-95e8-3a9c3d84c3ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Section 4: Scaling ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70cf5609-6e99-4967-b19a-47b7abdf1c1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bf66a89-93ff-4327-8473-cee102bcdf24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Model Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f7cd62f-ad3c-4c57-8113-5eed8e3dd1e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearRegression in module pyspark.ml.regression:\n\nclass LinearRegression(_JavaRegressor, _LinearRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n |  LinearRegression(*, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)\n |  \n |  Linear regression.\n |  \n |  The learning objective is to minimize the specified loss function, with regularization.\n |  This supports two kinds of loss:\n |  \n |  * squaredError (a.k.a squared loss)\n |  * huber (a hybrid of squared error for relatively small errors and absolute error for     relatively large ones, and we estimate the scale parameter from training data)\n |  \n |  This supports multiple types of regularization:\n |  \n |  * none (a.k.a. ordinary least squares)\n |  * L2 (ridge regression)\n |  * L1 (Lasso)\n |  * L2 + L1 (elastic net)\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  Notes\n |  -----\n |  Fitting with huber loss only supports none and L2 regularization.\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.ml.linalg import Vectors\n |  >>> df = spark.createDataFrame([\n |  ...     (1.0, 2.0, Vectors.dense(1.0)),\n |  ...     (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n |  >>> lr = LinearRegression(regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n |  >>> lr.setMaxIter(5)\n |  LinearRegression...\n |  >>> lr.getMaxIter()\n |  5\n |  >>> lr.setRegParam(0.1)\n |  LinearRegression...\n |  >>> lr.getRegParam()\n |  0.1\n |  >>> lr.setRegParam(0.0)\n |  LinearRegression...\n |  >>> model = lr.fit(df)\n |  >>> model.setFeaturesCol(\"features\")\n |  LinearRegressionModel...\n |  >>> model.setPredictionCol(\"newPrediction\")\n |  LinearRegressionModel...\n |  >>> model.getMaxIter()\n |  5\n |  >>> model.getMaxBlockSizeInMB()\n |  0.0\n |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n |  >>> abs(model.predict(test0.head().features) - (-1.0)) < 0.001\n |  True\n |  >>> abs(model.transform(test0).head().newPrediction - (-1.0)) < 0.001\n |  True\n |  >>> abs(model.coefficients[0] - 1.0) < 0.001\n |  True\n |  >>> abs(model.intercept - 0.0) < 0.001\n |  True\n |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n |  >>> abs(model.transform(test1).head().newPrediction - 1.0) < 0.001\n |  True\n |  >>> lr.setParams(featuresCol=\"vector\")\n |  LinearRegression...\n |  >>> lr_path = temp_path + \"/lr\"\n |  >>> lr.save(lr_path)\n |  >>> lr2 = LinearRegression.load(lr_path)\n |  >>> lr2.getMaxIter()\n |  5\n |  >>> model_path = temp_path + \"/lr_model\"\n |  >>> model.save(model_path)\n |  >>> model2 = LinearRegressionModel.load(model_path)\n |  >>> model.coefficients[0] == model2.coefficients[0]\n |  True\n |  >>> model.intercept == model2.intercept\n |  True\n |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n |  True\n |  >>> model.numFeatures\n |  1\n |  >>> model.write().format(\"pmml\").save(model_path + \"_2\")\n |  \n |  Method resolution order:\n |      LinearRegression\n |      _JavaRegressor\n |      Regressor\n |      pyspark.ml.wrapper.JavaPredictor\n |      pyspark.ml.base.Predictor\n |      pyspark.ml.wrapper.JavaEstimator\n |      pyspark.ml.wrapper.JavaParams\n |      pyspark.ml.wrapper.JavaWrapper\n |      pyspark.ml.base.Estimator\n |      _LinearRegressionParams\n |      pyspark.ml.base._PredictorParams\n |      pyspark.ml.param.shared.HasLabelCol\n |      pyspark.ml.param.shared.HasFeaturesCol\n |      pyspark.ml.param.shared.HasPredictionCol\n |      pyspark.ml.param.shared.HasRegParam\n |      pyspark.ml.param.shared.HasElasticNetParam\n |      pyspark.ml.param.shared.HasMaxIter\n |      pyspark.ml.param.shared.HasTol\n |      pyspark.ml.param.shared.HasFitIntercept\n |      pyspark.ml.param.shared.HasStandardization\n |      pyspark.ml.param.shared.HasWeightCol\n |      pyspark.ml.param.shared.HasSolver\n |      pyspark.ml.param.shared.HasAggregationDepth\n |      pyspark.ml.param.shared.HasLoss\n |      pyspark.ml.param.shared.HasMaxBlockSizeInMB\n |      pyspark.ml.param.Params\n |      pyspark.ml.util.Identifiable\n |      pyspark.ml.util.JavaMLWritable\n |      pyspark.ml.util.MLWritable\n |      pyspark.ml.util.JavaMLReadable\n |      pyspark.ml.util.MLReadable\n |      typing.Generic\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)\n |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                  standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                  loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n |  \n |  setAggregationDepth(self, value: int) -> 'LinearRegression'\n |      Sets the value of :py:attr:`aggregationDepth`.\n |  \n |  setElasticNetParam(self, value: float) -> 'LinearRegression'\n |      Sets the value of :py:attr:`elasticNetParam`.\n |  \n |  setEpsilon(self, value: float) -> 'LinearRegression'\n |      Sets the value of :py:attr:`epsilon`.\n |      \n |      .. versionadded:: 2.3.0\n |  \n |  setFitIntercept(self, value: bool) -> 'LinearRegression'\n |      Sets the value of :py:attr:`fitIntercept`.\n |  \n |  setLoss(self, value: str) -> 'LinearRegression'\n |      Sets the value of :py:attr:`loss`.\n |  \n |  setMaxBlockSizeInMB(self, value: float) -> 'LinearRegression'\n |      Sets the value of :py:attr:`maxBlockSizeInMB`.\n |      \n |      .. versionadded:: 3.1.0\n |  \n |  setMaxIter(self, value: int) -> 'LinearRegression'\n |      Sets the value of :py:attr:`maxIter`.\n |  \n |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0) -> 'LinearRegression'\n |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                   standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                   loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n |      Sets params for linear regression.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setRegParam(self, value: float) -> 'LinearRegression'\n |      Sets the value of :py:attr:`regParam`.\n |  \n |  setSolver(self, value: str) -> 'LinearRegression'\n |      Sets the value of :py:attr:`solver`.\n |  \n |  setStandardization(self, value: bool) -> 'LinearRegression'\n |      Sets the value of :py:attr:`standardization`.\n |  \n |  setTol(self, value: float) -> 'LinearRegression'\n |      Sets the value of :py:attr:`tol`.\n |  \n |  setWeightCol(self, value: str) -> 'LinearRegression'\n |      Sets the value of :py:attr:`weightCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n |  \n |  __orig_bases__ = (pyspark.ml.regression._JavaRegressor[ForwardRef('Lin...\n |  \n |  __parameters__ = ()\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Predictor:\n |  \n |  setFeaturesCol(self: ~P, value: str) -> ~P\n |      Sets the value of :py:attr:`featuresCol`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  setLabelCol(self: ~P, value: str) -> ~P\n |      Sets the value of :py:attr:`labelCol`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  setPredictionCol(self: ~P, value: str) -> ~P\n |      Sets the value of :py:attr:`predictionCol`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n |  \n |  clear(self, param: pyspark.ml.param.Param) -> None\n |      Clears a param from the param map if it has been explicitly set.\n |  \n |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n |      Creates a copy of this instance with the same uid and some\n |      extra params. This implementation first calls Params.copy and\n |      then make a copy of the companion Java pipeline component with\n |      extra params. So both the Python wrapper and the Java pipeline\n |      component get copied.\n |      \n |      Parameters\n |      ----------\n |      extra : dict, optional\n |          Extra parameters to copy to the new instance\n |      \n |      Returns\n |      -------\n |      :py:class:`JavaParams`\n |          Copy of this instance\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __del__(self) -> None\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Estimator:\n |  \n |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n |      Fits a model to the input dataset with optional parameters.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      dataset : :py:class:`pyspark.sql.DataFrame`\n |          input dataset.\n |      params : dict or list or tuple, optional\n |          an optional param map that overrides embedded params. If a list/tuple of\n |          param maps is given, this calls fit on each param map and returns a list of\n |          models.\n |      \n |      Returns\n |      -------\n |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n |          fitted model(s)\n |  \n |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n |      Fits a model to the input dataset for each param map in `paramMaps`.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Parameters\n |      ----------\n |      dataset : :py:class:`pyspark.sql.DataFrame`\n |          input dataset.\n |      paramMaps : :py:class:`collections.abc.Sequence`\n |          A Sequence of param maps.\n |      \n |      Returns\n |      -------\n |      :py:class:`_FitMultipleIterator`\n |          A thread safe iterable which contains one model for each param map. Each\n |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n |          using `paramMaps[index]`. `index` values may not be sequential.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from _LinearRegressionParams:\n |  \n |  getEpsilon(self) -> float\n |      Gets the value of epsilon or its default value.\n |      \n |      .. versionadded:: 2.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from _LinearRegressionParams:\n |  \n |  epsilon = Param(parent='undefined', name='epsilon', doc='T...s. Must b...\n |  \n |  loss = Param(parent='undefined', name='loss', doc='The ...imized. Supp...\n |  \n |  solver = Param(parent='undefined', name='solver', doc='Th...ation. Sup...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n |  \n |  getLabelCol(self) -> str\n |      Gets the value of labelCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n |  \n |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n |  \n |  getFeaturesCol(self) -> str\n |      Gets the value of featuresCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n |  \n |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  getPredictionCol(self) -> str\n |      Gets the value of predictionCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n |  \n |  getRegParam(self) -> float\n |      Gets the value of regParam or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n |  \n |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasElasticNetParam:\n |  \n |  getElasticNetParam(self) -> float\n |      Gets the value of elasticNetParam or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasElasticNetParam:\n |  \n |  elasticNetParam = Param(parent='undefined', name='elasticNetParam'...L...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n |  \n |  getMaxIter(self) -> int\n |      Gets the value of maxIter or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n |  \n |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasTol:\n |  \n |  getTol(self) -> float\n |      Gets the value of tol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n |  \n |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n |  \n |  getFitIntercept(self) -> bool\n |      Gets the value of fitIntercept or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n |  \n |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasStandardization:\n |  \n |  getStandardization(self) -> bool\n |      Gets the value of standardization or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasStandardization:\n |  \n |  standardization = Param(parent='undefined', name='standardization'...t...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n |  \n |  getWeightCol(self) -> str\n |      Gets the value of weightCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n |  \n |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasSolver:\n |  \n |  getSolver(self) -> str\n |      Gets the value of solver or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasAggregationDepth:\n |  \n |  getAggregationDepth(self) -> int\n |      Gets the value of aggregationDepth or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasAggregationDepth:\n |  \n |  aggregationDepth = Param(parent='undefined', name='aggregationDepth', ...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasLoss:\n |  \n |  getLoss(self) -> str\n |      Gets the value of loss or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n |  \n |  getMaxBlockSizeInMB(self) -> float\n |      Gets the value of maxBlockSizeInMB or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n |  \n |  maxBlockSizeInMB = Param(parent='undefined', name='maxBlockSizeInMB......\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.Params:\n |  \n |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n |      Explains a single param and returns its name, doc, and optional\n |      default value and user-supplied value in a string.\n |  \n |  explainParams(self) -> str\n |      Returns the documentation of all params with their optionally\n |      default values and user-supplied values.\n |  \n |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n |      Extracts the embedded default param values and user-supplied\n |      values, and then merges them with extra values from input into\n |      a flat param map, where the latter value is used if there exist\n |      conflicts, i.e., with ordering: default param values <\n |      user-supplied values < extra.\n |      \n |      Parameters\n |      ----------\n |      extra : dict, optional\n |          extra param values\n |      \n |      Returns\n |      -------\n |      dict\n |          merged param map\n |  \n |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n |      Gets the value of a param in the user-supplied param map or its\n |      default value. Raises an error if neither is set.\n |  \n |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n |      Gets a param by its name.\n |  \n |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n |      Checks whether a param has a default value.\n |  \n |  hasParam(self, paramName: str) -> bool\n |      Tests whether this instance contains a param with a given\n |      (string) name.\n |  \n |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n |      Checks whether a param is explicitly set by user or has\n |      a default value.\n |  \n |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n |      Checks whether a param is explicitly set by user.\n |  \n |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n |      Sets a parameter in the embedded param map.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from pyspark.ml.param.Params:\n |  \n |  params\n |      Returns all params ordered by name. The default implementation\n |      uses :py:func:`dir` to get all attributes of type\n |      :py:class:`Param`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.Identifiable:\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n |  \n |  write(self) -> pyspark.ml.util.JavaMLWriter\n |      Returns an MLWriter instance for this ML instance.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.MLWritable:\n |  \n |  save(self, path: str) -> None\n |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n |  \n |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n |      Returns an MLReader instance for this class.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.MLReadable:\n |  \n |  load(path: str) -> ~RL from abc.ABCMeta\n |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from typing.Generic:\n |  \n |  __class_getitem__(params) from abc.ABCMeta\n |  \n |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n |      This method is called when a class is subclassed.\n |      \n |      The default implementation does nothing. It may be\n |      overridden to extend subclasses.\n\n"
     ]
    }
   ],
   "source": [
    "# Describe how Spark scales linear regression.\n",
    "\n",
    "# 標準化がデフォルト\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "help(LinearRegression)\n",
    "# https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eacbf1f-3e96-4059-8541-741388d1a314",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeRegressor in module pyspark.ml.regression:\n\nclass DecisionTreeRegressor(_JavaRegressor, _DecisionTreeRegressorParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n |  DecisionTreeRegressor(*, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, impurity: str = 'variance', seed: Optional[int] = None, varianceCol: Optional[str] = None, weightCol: Optional[str] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0)\n |  \n |  `Decision tree <http://en.wikipedia.org/wiki/Decision_tree_learning>`_\n |  learning algorithm for regression.\n |  It supports both continuous and categorical features.\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.ml.linalg import Vectors\n |  >>> df = spark.createDataFrame([\n |  ...     (1.0, Vectors.dense(1.0)),\n |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n |  >>> dt = DecisionTreeRegressor(maxDepth=2)\n |  >>> dt.setVarianceCol(\"variance\")\n |  DecisionTreeRegressor...\n |  >>> model = dt.fit(df)\n |  >>> model.getVarianceCol()\n |  'variance'\n |  >>> model.setLeafCol(\"leafId\")\n |  DecisionTreeRegressionModel...\n |  >>> model.depth\n |  1\n |  >>> model.numNodes\n |  3\n |  >>> model.featureImportances\n |  SparseVector(1, {0: 1.0})\n |  >>> model.numFeatures\n |  1\n |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n |  >>> model.predict(test0.head().features)\n |  0.0\n |  >>> result = model.transform(test0).head()\n |  >>> result.prediction\n |  0.0\n |  >>> model.predictLeaf(test0.head().features)\n |  0.0\n |  >>> result.leafId\n |  0.0\n |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n |  >>> model.transform(test1).head().prediction\n |  1.0\n |  >>> dtr_path = temp_path + \"/dtr\"\n |  >>> dt.save(dtr_path)\n |  >>> dt2 = DecisionTreeRegressor.load(dtr_path)\n |  >>> dt2.getMaxDepth()\n |  2\n |  >>> model_path = temp_path + \"/dtr_model\"\n |  >>> model.save(model_path)\n |  >>> model2 = DecisionTreeRegressionModel.load(model_path)\n |  >>> model.numNodes == model2.numNodes\n |  True\n |  >>> model.depth == model2.depth\n |  True\n |  >>> model.transform(test1).head().variance\n |  0.0\n |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n |  True\n |  >>> df3 = spark.createDataFrame([\n |  ...     (1.0, 0.2, Vectors.dense(1.0)),\n |  ...     (1.0, 0.8, Vectors.dense(1.0)),\n |  ...     (0.0, 1.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n |  >>> dt3 = DecisionTreeRegressor(maxDepth=2, weightCol=\"weight\", varianceCol=\"variance\")\n |  >>> model3 = dt3.fit(df3)\n |  >>> print(model3.toDebugString)\n |  DecisionTreeRegressionModel...depth=1, numNodes=3...\n |  \n |  Method resolution order:\n |      DecisionTreeRegressor\n |      _JavaRegressor\n |      Regressor\n |      pyspark.ml.wrapper.JavaPredictor\n |      pyspark.ml.base.Predictor\n |      pyspark.ml.wrapper.JavaEstimator\n |      pyspark.ml.wrapper.JavaParams\n |      pyspark.ml.wrapper.JavaWrapper\n |      pyspark.ml.base.Estimator\n |      pyspark.ml.base._PredictorParams\n |      pyspark.ml.param.shared.HasLabelCol\n |      pyspark.ml.param.shared.HasFeaturesCol\n |      pyspark.ml.param.shared.HasPredictionCol\n |      _DecisionTreeRegressorParams\n |      pyspark.ml.tree._DecisionTreeParams\n |      pyspark.ml.param.shared.HasCheckpointInterval\n |      pyspark.ml.param.shared.HasSeed\n |      pyspark.ml.param.shared.HasWeightCol\n |      pyspark.ml.tree._TreeRegressorParams\n |      pyspark.ml.tree._HasVarianceImpurity\n |      pyspark.ml.param.shared.HasVarianceCol\n |      pyspark.ml.param.Params\n |      pyspark.ml.util.Identifiable\n |      pyspark.ml.util.JavaMLWritable\n |      pyspark.ml.util.MLWritable\n |      pyspark.ml.util.JavaMLReadable\n |      pyspark.ml.util.MLReadable\n |      typing.Generic\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, impurity: str = 'variance', seed: Optional[int] = None, varianceCol: Optional[str] = None, weightCol: Optional[str] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0)\n |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10,                  impurity=\"variance\", seed=None, varianceCol=None, weightCol=None,                  leafCol=\"\", minWeightFractionPerNode=0.0)\n |  \n |  setCacheNodeIds(self, value: bool) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`cacheNodeIds`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setCheckpointInterval(self, value: int) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`checkpointInterval`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setImpurity(self, value: str) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`impurity`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setMaxBins(self, value: int) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`maxBins`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setMaxDepth(self, value: int) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`maxDepth`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setMaxMemoryInMB(self, value: int) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`maxMemoryInMB`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setMinInfoGain(self, value: float) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`minInfoGain`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setMinInstancesPerNode(self, value: int) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`minInstancesPerNode`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setMinWeightFractionPerNode(self, value: float) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`minWeightFractionPerNode`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxDepth: int = 5, maxBins: int = 32, minInstancesPerNode: int = 1, minInfoGain: float = 0.0, maxMemoryInMB: int = 256, cacheNodeIds: bool = False, checkpointInterval: int = 10, impurity: str = 'variance', seed: Optional[int] = None, varianceCol: Optional[str] = None, weightCol: Optional[str] = None, leafCol: str = '', minWeightFractionPerNode: float = 0.0) -> 'DecisionTreeRegressor'\n |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                   maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10,                   impurity=\"variance\", seed=None, varianceCol=None, weightCol=None,                   leafCol=\"\", minWeightFractionPerNode=0.0)\n |      Sets params for the DecisionTreeRegressor.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setSeed(self, value: int) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`seed`.\n |  \n |  setVarianceCol(self, value: str) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`varianceCol`.\n |      \n |      .. versionadded:: 2.0.0\n |  \n |  setWeightCol(self, value: str) -> 'DecisionTreeRegressor'\n |      Sets the value of :py:attr:`weightCol`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n |  \n |  __orig_bases__ = (pyspark.ml.regression._JavaRegressor[ForwardRef('Dec...\n |  \n |  __parameters__ = ()\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Predictor:\n |  \n |  setFeaturesCol(self: ~P, value: str) -> ~P\n |      Sets the value of :py:attr:`featuresCol`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  setLabelCol(self: ~P, value: str) -> ~P\n |      Sets the value of :py:attr:`labelCol`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  setPredictionCol(self: ~P, value: str) -> ~P\n |      Sets the value of :py:attr:`predictionCol`.\n |      \n |      .. versionadded:: 3.0.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n |  \n |  clear(self, param: pyspark.ml.param.Param) -> None\n |      Clears a param from the param map if it has been explicitly set.\n |  \n |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n |      Creates a copy of this instance with the same uid and some\n |      extra params. This implementation first calls Params.copy and\n |      then make a copy of the companion Java pipeline component with\n |      extra params. So both the Python wrapper and the Java pipeline\n |      component get copied.\n |      \n |      Parameters\n |      ----------\n |      extra : dict, optional\n |          Extra parameters to copy to the new instance\n |      \n |      Returns\n |      -------\n |      :py:class:`JavaParams`\n |          Copy of this instance\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __del__(self) -> None\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Estimator:\n |  \n |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n |      Fits a model to the input dataset with optional parameters.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      dataset : :py:class:`pyspark.sql.DataFrame`\n |          input dataset.\n |      params : dict or list or tuple, optional\n |          an optional param map that overrides embedded params. If a list/tuple of\n |          param maps is given, this calls fit on each param map and returns a list of\n |          models.\n |      \n |      Returns\n |      -------\n |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n |          fitted model(s)\n |  \n |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n |      Fits a model to the input dataset for each param map in `paramMaps`.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Parameters\n |      ----------\n |      dataset : :py:class:`pyspark.sql.DataFrame`\n |          input dataset.\n |      paramMaps : :py:class:`collections.abc.Sequence`\n |          A Sequence of param maps.\n |      \n |      Returns\n |      -------\n |      :py:class:`_FitMultipleIterator`\n |          A thread safe iterable which contains one model for each param map. Each\n |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n |          using `paramMaps[index]`. `index` values may not be sequential.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n |  \n |  getLabelCol(self) -> str\n |      Gets the value of labelCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n |  \n |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n |  \n |  getFeaturesCol(self) -> str\n |      Gets the value of featuresCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n |  \n |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  getPredictionCol(self) -> str\n |      Gets the value of predictionCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.tree._DecisionTreeParams:\n |  \n |  getCacheNodeIds(self) -> bool\n |      Gets the value of cacheNodeIds or its default value.\n |  \n |  getLeafCol(self) -> str\n |      Gets the value of leafCol or its default value.\n |  \n |  getMaxBins(self) -> int\n |      Gets the value of maxBins or its default value.\n |  \n |  getMaxDepth(self) -> int\n |      Gets the value of maxDepth or its default value.\n |  \n |  getMaxMemoryInMB(self) -> int\n |      Gets the value of maxMemoryInMB or its default value.\n |  \n |  getMinInfoGain(self) -> float\n |      Gets the value of minInfoGain or its default value.\n |  \n |  getMinInstancesPerNode(self) -> int\n |      Gets the value of minInstancesPerNode or its default value.\n |  \n |  getMinWeightFractionPerNode(self) -> float\n |      Gets the value of minWeightFractionPerNode or its default value.\n |  \n |  setLeafCol(self: 'P', value: str) -> 'P'\n |      Sets the value of :py:attr:`leafCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.tree._DecisionTreeParams:\n |  \n |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n |  \n |  leafCol = Param(parent='undefined', name='leafCol', doc='L...ndex of e...\n |  \n |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n |  \n |  maxDepth = Param(parent='undefined', name='maxDepth', doc='... node + ...\n |  \n |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n |  \n |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n |  \n |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n |  \n |  minWeightFractionPerNode = Param(parent='undefined', name='minWeightFr...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n |  \n |  getCheckpointInterval(self) -> int\n |      Gets the value of checkpointInterval or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n |  \n |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n |  \n |  getSeed(self) -> int\n |      Gets the value of seed or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n |  \n |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n |  \n |  getWeightCol(self) -> str\n |      Gets the value of weightCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n |  \n |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.tree._HasVarianceImpurity:\n |  \n |  getImpurity(self) -> str\n |      Gets the value of impurity or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.tree._HasVarianceImpurity:\n |  \n |  impurity = Param(parent='undefined', name='impurity', doc='...(case-in...\n |  \n |  supportedImpurities = ['variance']\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasVarianceCol:\n |  \n |  getVarianceCol(self) -> str\n |      Gets the value of varianceCol or its default value.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasVarianceCol:\n |  \n |  varianceCol = Param(parent='undefined', name='varianceCol', do...e for...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.Params:\n |  \n |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n |      Explains a single param and returns its name, doc, and optional\n |      default value and user-supplied value in a string.\n |  \n |  explainParams(self) -> str\n |      Returns the documentation of all params with their optionally\n |      default values and user-supplied values.\n |  \n |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n |      Extracts the embedded default param values and user-supplied\n |      values, and then merges them with extra values from input into\n |      a flat param map, where the latter value is used if there exist\n |      conflicts, i.e., with ordering: default param values <\n |      user-supplied values < extra.\n |      \n |      Parameters\n |      ----------\n |      extra : dict, optional\n |          extra param values\n |      \n |      Returns\n |      -------\n |      dict\n |          merged param map\n |  \n |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n |      Gets the value of a param in the user-supplied param map or its\n |      default value. Raises an error if neither is set.\n |  \n |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n |      Gets a param by its name.\n |  \n |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n |      Checks whether a param has a default value.\n |  \n |  hasParam(self, paramName: str) -> bool\n |      Tests whether this instance contains a param with a given\n |      (string) name.\n |  \n |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n |      Checks whether a param is explicitly set by user or has\n |      a default value.\n |  \n |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n |      Checks whether a param is explicitly set by user.\n |  \n |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n |      Sets a parameter in the embedded param map.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from pyspark.ml.param.Params:\n |  \n |  params\n |      Returns all params ordered by name. The default implementation\n |      uses :py:func:`dir` to get all attributes of type\n |      :py:class:`Param`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.Identifiable:\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n |  \n |  write(self) -> pyspark.ml.util.JavaMLWriter\n |      Returns an MLWriter instance for this ML instance.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.MLWritable:\n |  \n |  save(self, path: str) -> None\n |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n |  \n |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n |      Returns an MLReader instance for this class.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.MLReadable:\n |  \n |  load(path: str) -> ~RL from abc.ABCMeta\n |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from typing.Generic:\n |  \n |  __class_getitem__(params) from abc.ABCMeta\n |  \n |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n |      This method is called when a class is subclassed.\n |      \n |      The default implementation does nothing. It may be\n |      overridden to extend subclasses.\n\n"
     ]
    }
   ],
   "source": [
    "# Describe how Spark scales decision trees.\n",
    "\n",
    "# 閾値でデータを分割するため、スケールは不要\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "help(DecisionTreeRegressor)\n",
    "# https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22491bcd-51db-4bf2-9b04-0c9e4d89b24d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ensembling Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d51e58e-aed9-4a96-9fc3-cbaeaa42b557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Describe the basic concepts of ensemble learning.\n",
    "\n",
    "# 複数のアルゴリズムの集合を使用することでバイアスとバリアンスのバランスのとれた学習結果を得る手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c4ad108-cf46-4a75-822e-55d24dfc4424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare and contrast bagging, boosting, and stacking\n",
    "\n",
    "# bagging: ブートストラップ(ランダム復元抽出), 並列で弱学習器を学習⇒分散に適している\n",
    "# boosting: イテレートしながら弱学習器を作成する(過去の学習の誤差を修正しながら精度向上を図る)⇒分散処理が難しい"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_Scaling ML Models",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
