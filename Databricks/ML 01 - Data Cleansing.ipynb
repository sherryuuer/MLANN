{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307b31cd-4c80-4878-924e-8a46fffe8917",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "568ee6e6-2b1d-44ae-8622-24a51686134b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"8c6d3ef3-e44b-4292-a0d3-1aaba0198525\"/>\n",
    "\n",
    "# データクレンジング (Data Cleansing)\n",
    "\n",
    "今回はSparkを使って、<a href=\"http://insideairbnb.com/get-the-data.html\" target=\"_blank\">Inside Airbnb</a>のSF Airbnb賃貸データセットの探索的データ解析とクレンジングを行います。\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/301/sf.jpg\" style=\"height: 200px; margin: 10px; border: 1px solid #ddd; padding: 10px\"/>\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)このレッスンで次を行います: <br>\n",
    " - 欠損値の補完\n",
    " - 外れ値の特定と除外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9e3ce4-2c43-48d7-84a6-9eb6727d9095",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8df5d9e-b063-411c-9722-220c389d913f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"969507ea-bffc-4255-9a99-2306a594625f\"/>\n",
    "\n",
    "Airbnbのデータセットをロードしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d175b6-fd9a-4603-80e9-af53beb4cda9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06.csv\"\n",
    "\n",
    "raw_df = spark.read.csv(file_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e64172d-2364-403b-ad36-b969fc9ca4ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74e1c2d2-040c-4353-8f01-8edfc4f08c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"94856418-c319-4915-a73e-5728fcd44101\"/>\n",
    "\n",
    "シンプルにするため、特定のカラムだけを残すようにします。特徴量の選択については後述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be52ef36-336d-4404-8550-56105a041dee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    \"host_is_superhost\",\n",
    "    \"cancellation_policy\",\n",
    "    \"instant_bookable\",\n",
    "    \"host_total_listings_count\",\n",
    "    \"neighbourhood_cleansed\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"property_type\",\n",
    "    \"room_type\",\n",
    "    \"accommodates\",\n",
    "    \"bathrooms\",\n",
    "    \"bedrooms\",\n",
    "    \"beds\",\n",
    "    \"bed_type\",\n",
    "    \"minimum_nights\",\n",
    "    \"number_of_reviews\",\n",
    "    \"review_scores_rating\",\n",
    "    \"review_scores_accuracy\",\n",
    "    \"review_scores_cleanliness\",\n",
    "    \"review_scores_checkin\",\n",
    "    \"review_scores_communication\",\n",
    "    \"review_scores_location\",\n",
    "    \"review_scores_value\",\n",
    "    \"price\"\n",
    "]\n",
    "\n",
    "base_df = raw_df.select(columns_to_keep)\n",
    "base_df.cache().count()\n",
    "display(base_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e978781a-a4aa-4765-af18-134b2c4497f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"a12c5a59-ad1c-4542-8695-d822ec10c4ca\"/>\n",
    "\n",
    "### データ型の修正 (Fixing Data Types)\n",
    "\n",
    "上のスキーマを見てください。 **`賃貸価格(price)`** フィールドが文字列として扱われたことに気がつくでしょう。このタスクでは 数値（double 型）フィールドである必要があります。 \n",
    "\n",
    "データ型を修正しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82780384-1e83-45de-bdaa-5646daacb0e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, translate\n",
    "\n",
    "fixed_price_df = base_df.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\"))\n",
    "\n",
    "display(fixed_price_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbfb8450-853c-47d4-8b48-44a9059e4cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"4ad08138-4563-4a93-b038-801832c9bc73\"/>\n",
    "\n",
    "### 要約統計 (Summary statistics)\n",
    "\n",
    "要約統計を出力する2つのオプションがあります。\n",
    "* **`describe`** : count, mean, stddev, min, max \n",
    "* **`summary`** : 上記のdescribeの項目 + 四分位範囲(IQR : interquartile range)\n",
    "\n",
    "**質問** : 平均値よりも IQR/中央値を使うべき時は？その逆は？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2746e5c-d901-4768-83d0-09924dbe0ddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fixed_price_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d79988-b088-4bd1-9007-b61f3949ae46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fixed_price_df.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73386033-2068-422e-8c78-2b49ede5d64c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"bd55efda-86d0-4584-a6fc-ef4f221b2872\"/>\n",
    "\n",
    "### Dbutils によるデータ要約 (Dnutils Data Summary)\n",
    "\n",
    " **`dbutils.data.summarize`** を使用して、より詳細な要約統計とデータプロットを見ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84ed4f2-0d58-424b-8f53-258c1ed4a431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.data.summarize(fixed_price_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4e6b24f-d761-4c35-8306-9df8235e8d0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"e9860f92-2fbe-4d23-b728-678a7bb4734e\"/>\n",
    "\n",
    "### 極端な値の除外 (Geeting rid of exterme values)\n",
    "\n",
    " **`price`** 列について *最小値* と *最大値* を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e487b8da-ac76-4754-9ff8-c6d5835af76e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fixed_price_df.select(\"price\").describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c9363e-6dbe-4df0-a857-c7021cfae664",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"4a8fe21b-1dac-4edf-a0a3-204f170b05c9\"/>\n",
    "\n",
    "超高額な物件もありますが、そのデータをどうするかはSME（Subject Matter Experts）次第です。しかし「無料」のAirbnbはフィルタリングすることは可能でしょう。\n",
    "\n",
    "まず、*price* がゼロの物件がいくつあるか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b10e56-7e89-402c-94a3-5a43173cd0d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fixed_price_df.filter(col(\"price\") == 0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59828db4-e48d-416a-99b3-178a1371ec6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"bf195d9b-ea4d-4a3e-8b61-372be8eec327\"/>\n",
    "\n",
    "*price* が0より大きい行だけを残すようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19b03e5-bbe6-41e4-8582-aee0f1cfce6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pos_prices_df = fixed_price_df.filter(col(\"price\") > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5674bd6-4215-4119-bdf3-80cda0621d25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"dc8600db-ebd1-4110-bfb1-ce555bc95245\"/>\n",
    "\n",
    "*minimum\\_nights* カラムの *最小値* と *最大値* を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d2a548-33e5-4535-b8ea-0f5486176d46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pos_prices_df.select(\"minimum_nights\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98f8b0e9-00af-4a92-bee4-0dc34178db16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pos_prices_df\n",
    "        .groupBy(\"minimum_nights\").count()\n",
    "        .orderBy(col(\"count\").desc(), col(\"minimum_nights\"))\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9055b7e-7ebc-4df8-a614-9eb4150aca18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"5aa4dfa8-d9a1-42e2-9060-a5dcc3513a0d\"/>\n",
    "\n",
    "minimum\\_nights の上限は1年が妥当と思われます。*minimum\\_nights* が365より大きいレコードをフィルタリングしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa8349b-de1c-4bab-bdb0-0089e9bbc9fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "min_nights_df = pos_prices_df.filter(col(\"minimum_nights\") <= 365)\n",
    "\n",
    "display(min_nights_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3049fd9d-ca71-4ebd-a91a-09768e8691f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"25a35390-d716-43ad-8f51-7e7690e1c913\"/>\n",
    "\n",
    "### 欠損値の取り扱い (Handling Null Values)\n",
    "\n",
    "欠損値を扱うには、さまざまな方法があります。時には、欠損であることが実際に予測するための重要な指標となることもあります（例えば、フォームの特定の部分を記入しなければ、それが承認される確率は低下します）。\n",
    "\n",
    "欠損を処理するいくつかの方法: \n",
    "* 欠損を含むレコードはすべて削除する。 \n",
    "* 数値型の場合:\n",
    "    * 平均値/中央値/ゼロ/その他で置き換える。\n",
    "* カテゴリカル変数の場合: \n",
    "    * 最頻値に置き換える。\n",
    "    * 欠損を表す表現を用意する\n",
    "* 欠損値補完のために設計された ALS (Alternating Least Squares) のようなテクニックを使用する。\n",
    "  \n",
    "**カテゴリ／数値特徴量に対して補完(Imputation)を行う場合、このフィールドが補完されたことがわかるように新フィールドを追加しなければならない（MUST）。**\n",
    "\n",
    "SparkMLのImputer（後述）は、カテゴリ特徴量はサポートしていません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f07f32-cad3-4014-b263-f4378972c354",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"83e56fca-ce6d-4e3c-8042-0c1c7b9eaa5a\"/>\n",
    "\n",
    "### 補完：Double型へのキャスト (Impute: Cast to Double)\n",
    "\n",
    "SparkMLの <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html?highlight=imputer#pyspark.ml.feature.Imputer\" target=\"_blank\">Imputer</a> は、すべてのフィールドがdouble型であることを要求しています。すべての整数フィールドをdoubleにキャストしてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3f255e-7650-473f-9213-8bdef1af74ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "integer_columns = [x.name for x in min_nights_df.schema.fields if x.dataType == IntegerType()]\n",
    "doubles_df = min_nights_df\n",
    "\n",
    "for c in integer_columns:\n",
    "    doubles_df = doubles_df.withColumn(c, col(c).cast(\"double\"))\n",
    "\n",
    "columns = \"\\n - \".join(integer_columns)\n",
    "print(f\"Columns converted from Integer to Double:\\n - {columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "806795b6-a1bb-4f3d-8656-781d7c7ca984",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"69b58107-82ad-4cec-8984-028a5df1b69e\"/>\n",
    "\n",
    "代入の前に、NULL値の存在を示すダミー列を追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e8af02-a4b0-46a7-bcac-388a86741e26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "impute_cols = [\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"beds\", \n",
    "    \"review_scores_rating\",\n",
    "    \"review_scores_accuracy\",\n",
    "    \"review_scores_cleanliness\",\n",
    "    \"review_scores_checkin\",\n",
    "    \"review_scores_communication\",\n",
    "    \"review_scores_location\",\n",
    "    \"review_scores_value\"\n",
    "]\n",
    "\n",
    "for c in impute_cols:\n",
    "    doubles_df = doubles_df.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674e3efa-d21d-402a-aab4-2906a55f1312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(doubles_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31f2c791-e06e-4370-b0bb-bd26acb27da1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"c88f432d-1252-4acc-8c91-4834c00da789\"/>\n",
    "\n",
    "### 変換器と推定器 (Transformers and Estimators)\n",
    "\n",
    "Spark MLは、機械学習アルゴリズムのAPIを標準化し、複数のアルゴリズムを1つのパイプライン（ワークフロー）にまとめることを容易にしている。Spark ML APIで導入された2つの重要な概念について説明します: **`変換器(transformer)`** と **`推定器(estimator)`** .\n",
    "\n",
    "**変換器(transformer)**: DataFrameを別のDataFrameに変換します。DataFrameを入力として受け取り、1つまたは複数の列が追加された新しいDataFrameを返します。Transformerはデータからパラメータを学習せず、単純にルールベースの変換を適用します。Transformerは **`.transform()`** メソッドを持ちます。\n",
    "\n",
    "**推定器(estimator)**: DataFrameが持つデータにフィットして、Transformerを生成することができるアルゴリズムです。例えば、学習アルゴリズムはDataFrameから学習し、モデルを生成するestimatorです。Estimatorは **`.fit()`** メソッドを持っており、DataFrameからパラメータを学習（または「フィット」）します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443e948f-e355-4b5b-b872-0450d55b9420",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"median\", inputCols=impute_cols, outputCols=impute_cols)\n",
    "\n",
    "imputer_model = imputer.fit(doubles_df)\n",
    "imputed_df = imputer_model.transform(doubles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2e35e24-ede8-472d-9e12-0229d4d40845",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"4df06e83-27e6-4cc6-b66d-883317b2a7eb\"/>\n",
    "\n",
    "これでデータはきれいになりました。このDataFrameをDeltaに保存して、モデル作りを始めましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0861f1-a4ce-468e-991c-80060619735e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imputed_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{DA.paths.working_dir}/imputed_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2160e36c-b6c1-4daf-864d-c15131d9bad4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 01 - Data Cleansing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
