{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72533e5c-84ec-4223-9c7a-3a0398ba1ac0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73c852b3-daa2-4864-b821-a427cd99fb3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"2ab084da-06ed-457d-834a-1d19353e5c59\"/>\n",
    "\n",
    "# ランダムフォレストとハイパーパラメータチューニング(Random Forests and Hyperparameter Tuning)\n",
    "\n",
    "では、グリッドサーチとクロスバリデーションを使って最適なハイパーパラメータを見つけるためのランダムフォレストをチューニングする方法を見ていきましょう。Databricks Runtime for MLを使用すると、MLflowは自動的にSparkML cross-validatorで実験(Experiment)をログに記録します!\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)このレッスンで次を行います:<br>\n",
    " - グリッドサーチを用いたハイパーパラメータのチューニング\n",
    " - SparkMLパイプラインの最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7c075a-5b79-4e5c-96cc-2a4116131812",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0336c696-193e-4c5f-8680-758f9e81c310",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06-clean.delta/\"\n",
    "airbnb_df = spark.read.format(\"delta\").load(file_path)\n",
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols, outputCols=index_output_cols, handleInvalid=\"skip\")\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40)\n",
    "stages = [string_indexer, vec_assembler, rf]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00b0b177-751b-44fe-b91d-46cfa20af447",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"4561938e-90b5-413c-9e25-ef15ba40e99c\"/>\n",
    "\n",
    "## ParamGrid\n",
    "\n",
    "まず、ランダムフォレストで調整できる様々なハイパーパラメータを見てみましょう。\n",
    "\n",
    "**ポップクイズ：**パラメータとハイパーパラメータの違いは？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f331d3-e492-406f-b1f6-cb14e6f855f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca3d9d5-9d0b-4ff4-8739-2d5ceb1db333",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"819de6f9-75d2-45df-beb1-6b59ecd2cfd2\"/>\n",
    "\n",
    "チューニングできるハイパーパラメーターはたくさんあり、手作業で設定するには時間がかかるでしょう。\n",
    "\n",
    "手作業（アドホック）ではなく、Sparkの <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html?highlight=paramgridbuilder#pyspark.ml.tuning.ParamGridBuilder\" target=\"_blank\">ParamGridBuilder</a> を使って、よりシステマチックに最適なハイパーパラメータを求めましょう。\n",
    "\n",
    "テストするハイパーパラメータのグリッドを定義してみましょう。\n",
    "* **`maxDepth`** : 各決定木の最大の深さ (次の値を使用 :  **`2, 5`** )\n",
    "* **`木の本数`** : 学習する決定木の本数 (次の値を使用 :  **`5, 10`**)\n",
    "\n",
    "**`addGrid()`** は、パラメータの名前 (例. **`rf.maxDepth`**) と、取り得る値のリスト (例えば **`[2, 5]`** )を受け付けます."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad20a72-07be-4863-a1ed-98918718a1c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(rf.maxDepth, [2, 5])\n",
    "              .addGrid(rf.numTrees, [5, 10])\n",
    "              .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2d64207-1608-4144-8090-011de9cb4b8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"9f043287-11b8-482d-8501-2f7d8b1458ea\"/>\n",
    "\n",
    "## クロスバリデーション (Cross Validation)\n",
    "\n",
    "3-fold クロスバリデーションを用いて最適なハイパーパラメータを特定することにします。\n",
    "\n",
    "![crossValidation](https://files.training.databricks.com/images/301/CrossValidation.png)\n",
    "\n",
    "3-fold クロスバリデーションでは、2/3のデータで学習し、残りの1/3のデータ（ホールドアウトセット）で評価します。このプロセスを3回繰り返し、各foldが検証セットとして利用されます。そして、3回の結果の平均を取ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647204de-0da1-4c6b-b930-f813cd9ad692",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"ec0440ab-071d-4201-be86-5eeedaf80a4f\"/>\n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator#pyspark.ml.tuning.CrossValidator\" target=\"_blank\">CrossValidator</a> に、 **`estimator`** (pipeline)、 **`evaluator`** 、 **`estimatorParamMaps`** を渡すことで、以下を設定します： \n",
    "* どのモデルを使うか\n",
    "* どのようにモデルを評価するか\n",
    "* モデルに対してどのハイパーパラメータを設定するか\n",
    "\n",
    "また、データをいくつのfoldに分割するか（3つ）、また、再現性を確保するためseed（42）も設定もできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57771e32-f2c4-48e8-8f05-50a52a1a02d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=param_grid, \n",
    "                    numFolds=3, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c7eda45-1f01-4101-a66c-33434b5c4035",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"673c9261-a861-4ace-b008-c04565230a8e\"/>\n",
    "\n",
    "**質問** :今、いくつのモデルをトレーニングしているでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21874948-5385-46be-aade-eb7aa5d66356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d965e81d-04a0-4c04-9aa8-5e81019ab6bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"c9bc1596-7b0f-4595-942c-109cfca51698\"/>\n",
    "\n",
    "## 並列化パラメータ (Parallelism Parameter)\n",
    "\n",
    "うーん、実行に時間がかかりましたね。それは、モデルが並列ではなく、逐次的に学習されていたためです。\n",
    "\n",
    "Spark 2.3では、 <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator#pyspark.ml.tuning.CrossValidator.parallelism\" target=\"_blank\">Parallelism（並列化）</a>というパラメータが導入されました。（ドキュメント引用: **`並列アルゴリズムを実行する際に使用するスレッド数 (>= 1)`** 。）\n",
    "\n",
    "この値を4にして、より速くトレーニングできるかどうか見てみましょう。Sparkの <a href=\"https://spark.apache.org/docs/latest/ml-tuning.html\" target=\"_blank\">ドキュメント</a> では、2～10の間の値を推奨しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0d81d4c-fe2a-4472-abdb-12fdceafe7f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_model = cv.setParallelism(4).fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "625bf07a-8a9d-44ca-8a50-08b3dbd2c258",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"2d00b40f-c5e7-4089-890b-a50ccced34c6\"/>\n",
    "\n",
    "**質問**: うーん、まだ実行に時間がかかりましたね。cross validatorの中にパイプラインを入れるべきか、パイプラインの中にcross validatorを入れるべきか、どちらでしょうか？\n",
    "\n",
    "パイプラインにestimatorやtransformerがあるかによります。\n",
    "\n",
    "パイプラインの中にcross validatorを入れるほう：パイプラインにStringIndexer（estimator）などがある場合、パイプライン全体をクロスバリデータにすると、毎回リフィットする必要があります（上が例）。ホールドアウトセットからトレーニングセットへのデータ漏洩の懸念がある場合、この方法が最も安全です。cross validatorはまずデータを分割し、次にパイプラインをフィットします。\n",
    "\n",
    "cross validatorの中にパイプラインを入れるほう：下の例のようにパイプラインの末尾にcross validatorを配置すると、(cross validatorがデータを分割する前にestimatorがfitをすると)ホールドアウトセットからトレーニングセットに情報が漏れる可能性があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d68bd9e-f3dc-41fc-b1ef-6c39689d29b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=param_grid, \n",
    "                    numFolds=3, parallelism=4, seed=42)\n",
    "\n",
    "stages_with_cv = [string_indexer, vec_assembler, cv]\n",
    "pipeline = Pipeline(stages=stages_with_cv)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6083fe98-c57e-4a1b-96eb-6ec19a140977",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"dede990c-2551-4c07-8aad-d697ae827e71\"/>\n",
    "\n",
    "最適なハイパーパラメータ構成におけるモデルを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf3d542-3a32-4e97-91e9-9a65029e486f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list(zip(cv_model.getEstimatorParamMaps(), cv_model.avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f8e3fe-3bba-4294-b62f-e920110d240a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred_df = pipeline_model.transform(test_df)\n",
    "\n",
    "rmse = evaluator.evaluate(pred_df)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "print(f\"RMSE is {rmse}\")\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fa2239-575a-4c74-86e5-2779c152c570",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<i18n value=\"8f80daf2-8f0b-4cab-a8e6-4060c78d94b0\"/>\n",
    "\n",
    "改良されてますね! 決定木の性能を上回っているようですね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5ab79d4-b21e-4cc2-a7b1-2e935eb7d8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML 07 - Random Forests and Hyperparameter Tuning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
